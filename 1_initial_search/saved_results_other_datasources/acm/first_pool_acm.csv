Item type;Authors;Title;Journal;Publication year;Volume;Issue;Pages;Publisher;Address;Book title;Proceedings title;Conference location;Date published;ISBN;ISSN;URLs;DOI;Abstract;Keywords;Notes;Series
Conference Paper;Farkas KI,Flinn J,Back G,Grunwald D,Anderson JM;Quantifying the Energy Consumption of a Pocket Computer and a Java Virtual Machine;;2000;;;252–263;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 2000 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems;Santa Clara, California, USA;2000;9,78158E+12;;"https://doi.org/10.1145/339331.339421;http://dx.doi.org/10.1145/339331.339421";10.1145/339331.339421;In this paper, we examine the energy consumption of a state-of-the-art pocket computer. Using a data acquisition system, we measure the energy consumption of the Itsy Pocket Computer, developed by Compaq Computer Corporation's Palo Alto Research Labs. We begin by showing that the energy usage characteristics of the Itsy differ markedly from that of a notebook computer. Then, since we expect that flexible software environments will become increasingly prevalent on pocket computers, we consider applications running in a Java environment. In particular, we explain some of the Java design tradeoffs applicable to pocket computers, and quantify their energy costs. For the design options we considered and the three workloads we studied, we find a maximum change in energy use of 25%.;;;SIGMETRICS '00
Journal Article;Farkas KI,Flinn J,Back G,Grunwald D,Anderson JM;Quantifying the Energy Consumption of a Pocket Computer and a Java Virtual Machine;SIGMETRICS Perform. Eval. Rev.;2000;28;1;252–263;Association for Computing Machinery;New York, NY, USA;;;;2000-06;;0163-5999;"https://doi.org/10.1145/345063.339421;http://dx.doi.org/10.1145/345063.339421";10.1145/345063.339421;In this paper, we examine the energy consumption of a state-of-the-art pocket computer. Using a data acquisition system, we measure the energy consumption of the Itsy Pocket Computer, developed by Compaq Computer Corporation's Palo Alto Research Labs. We begin by showing that the energy usage characteristics of the Itsy differ markedly from that of a notebook computer. Then, since we expect that flexible software environments will become increasingly prevalent on pocket computers, we consider applications running in a Java environment. In particular, we explain some of the Java design tradeoffs applicable to pocket computers, and quantify their energy costs. For the design options we considered and the three workloads we studied, we find a maximum change in energy use of 25%.;;;
Conference Paper;Gao Y,Yan D,Liu N,Luo S,Zhou Z,Wang Y,Chen Y,Gao B;A High-Frequency Monitoring Method for Urban Carbon Emissions Based on Vertical Federated Deep Learning and Multi-Source Heterogeneous Data Fusion Methods;;2023;;;232–237;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 5th International Conference on Information Technologies and Electrical Engineering;Changsha, China;2023;9,78145E+12;;"https://doi.org/10.1145/3582935.3582972;http://dx.doi.org/10.1145/3582935.3582972";10.1145/3582935.3582972;Carbon emissions are the main culprit of global warming. Accurate carbon emission forecasting helps government departments formulate effective carbon emission reduction policies and helps the carbon emission market develop orderly. Implementing an accurate and effective carbon emission monitoring model requires the collaboration of many parties because carbon emission-related data involves many sectors and industries. However, for the relevant characteristics of carbon emission monitoring, due to the different collection and storage standards of various departments, poor maintenance environment, lack of data, data loss, and abnormal severe, resulting in high frequency and high precision carbon emission monitoring. As privacy protection and data security issues are gradually taken seriously by government departments and related enterprises, the inability or unwillingness to share carbon emission-related data among enterprises or even among various departments within enterprises has created an increasingly severe data silo phenomenon. In addition, how effectively breaking the data barriers between various sectors is an urgent problem in grasping carbon emission change changes accurately. Therefore, this paper proposes a carbon emission monitoring model for key urban sectors based on vertical federated deep learning and multi-source heterogeneous data fusion and sharing. The experimental results show that the model accurately predicts carbon emission change trends in various application scenarios under the data availability and invisibility of each participant.;Carbon Emissions Monitoring, Vertical Federated Deep Learning, Privacy and security protection, Multi-source heterogeneous data fusion;;ICITEE '22
Conference Paper;Jun L,Jie Z,DingHong P;Cloud Computing Virtual Machine Migration Energy Measuring Research;;2020;;;;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 3rd International Conference on Vision, Image and Signal Processing;Vancouver, BC, Canada;2020;9,78145E+12;;"https://doi.org/10.1145/3387168.3387192;http://dx.doi.org/10.1145/3387168.3387192";10.1145/3387168.3387192;This paper research virtual machine migration energy measuring on IPv4/IPv6 network based on cloud computing infrastructure platform, it conducts a research on the energy measuring in IPv4/IPv6 cloud computing platform, and presents a dynamic energy measuring mathematical model based on analyzing CPU energy consumption changes brought by random assignment works. The research determines mathematical model parameter values and it completes the IPv4/IPv6 cloud computing platform virtual machine migration experimentally. This paper achieves the energy consumption of IPv4/IPv6 transition prior-period, mid-period and last-period cloud computing platform virtual machine migration, the conclusions in line with the cloud energy measurement needs, it builds the theoretical foundation for cloud computing platform energy consumption optimize.;Virtual Machine Migration, Energy Measuring, Cloud Computing, Migration Energy Consumption;;ICVISP 2019
Conference Paper;Pathania P,Mehra R,Sharma VS,Kaulgud V,Podder S,Burden AP;ESAVE: Estimating Server and Virtual Machine Energy;;2023;;;;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering;Rochester, MI, USA;2023;9,78145E+12;;"https://doi.org/10.1145/3551349.3561170;http://dx.doi.org/10.1145/3551349.3561170";10.1145/3551349.3561170;Sustainable software engineering has received a lot of attention in recent times, as we witness an ever-growing slice of energy use, for example, at data centers, as software systems utilize the underlying infrastructure. Characterizing servers for their energy use accurately without being intrusive, is therefore important to make sustainable software deployment choices. In this paper, we introduce ESAVE which is a machine learning-based approach that leverages a small set of hardware attributes to characterize a server or virtual machine’s energy usage across different levels of utilization. This is based upon an extensive exploration of multiple ML approaches, with a focus on a minimal set of required attributes, while showcasing good accuracy. Early validations show that ESAVE has only around 12% average prediction error, despite being non-intrusive.;;;ASE '22
Conference Paper;Huynh HT,Quan HD;Energy Expenditure Estimation Based on Artificial Intelligence and Microservice Architecture;;2020;;;159–163;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 4th International Conference on Machine Learning and Soft Computing;Haiphong City, Viet Nam;2020;9,78145E+12;;"https://doi.org/10.1145/3380688.3380715;http://dx.doi.org/10.1145/3380688.3380715";10.1145/3380688.3380715;"Nutritional status plays an important role in not only pregnancy outcomes but also neonatal health. One of efficient techniques to control the nutritional status is to estimate the energy expenditure. There are some approaches for estimating energy expenditure. However, they have limitations including high cost, relative complexity, trained personnel requirements, or locality. This study investigates in a system for data collection and analysis (IoH-Internet of Health) developing based on microservice architecture, and its application for energy expenditure estimation. The proposed system has a good ability to scale and integrate with other systems; the energy expenditure estimation is performed by using artificial intelligence. The experimental results have shown the promising results of the proposed system.";IoH system, data collection, expenditure energy estimation, healthcare, visualization;;ICMLSC '20
Journal Article;Alavani G,Desai J,Saha S,Sarkar S;Program Analysis and Machine Learning Based Approach to Predict Power Consumption of CUDA Kernel;ACM Trans. Model. Perform. Eval. Comput. Syst.;2023;;;;Association for Computing Machinery;New York, NY, USA;;;;2023-06;;2376-3639;"https://doi.org/10.1145/3603533;http://dx.doi.org/10.1145/3603533";10.1145/3603533;General Purpose Graphics Processing Unit (GPGPU) has secured a prominent position in the High-Performance Computing (HPC) world due to its performance gain and programmability. Understanding the relationship between GPU power consumption and program features can aid developers in building energy-efficient sustainable applications. In this work, we propose a static analysis based power model built using machine learning techniques. We have investigated six machine learning models across three NVIDIA GPU architectures: Kepler, Maxwell, and Volta with Random Forest, Extra Trees, Gradient Boosting, CatBoost, and XGBoost, reporting favorable results. We observed that the XGBoost technique based prediction model is the most efficient technique with an R-square value of 0.9646 on Volta Architecture. The dataset used for these techniques includes kernels from different benchmarks suits, sizes, nature (e.g., compute-bound, memory-bound), and complexity (e.g., control divergence, memory access patterns). Experimental results suggest that the proposed solution can help developers precisely predict GPU applications power consumption using program analysis across GPU architectures. Developers can use this approach to refactor their code to build energy-efficient GPU applications.;"GPGPU Computing ; XGBoost ; CatBoost ; CUDA; Static Analysis ; Sustainable Computing";Just Accepted;
Conference Paper;Colmant M,Kurpicz M,Felber P,Huertas L,Rouvoy R,Sobe A;Process-Level Power Estimation in VM-Based Systems;;2015;;;;Association for Computing Machinery;New York, NY, USA;;Proceedings of the Tenth European Conference on Computer Systems;Bordeaux, France;2015;9,78145E+12;;"https://doi.org/10.1145/2741948.2741971;http://dx.doi.org/10.1145/2741948.2741971";10.1145/2741948.2741971;Power estimation of software processes provides critical indicators to drive scheduling or power capping heuristics. State-of-the-art solutions can perform coarse-grained power estimation in virtualized environments, typically treating virtual machines (VMs) as a black box. Yet, VM-based systems are nowadays commonly used to host multiple applications for cost savings and better use of energy by sharing common resources and assets.In this paper, we propose a fine-grained monitoring middleware providing real-time and accurate power estimation of software processes running at any level of virtualization in a system. In particular, our solution automatically learns an application-agnostic power model, which can be used to estimate the power consumption of applications.Our middleware implementation, named BitWatts, builds on a distributed actor implementation to collect process usage and infer fine-grained power consumption without imposing any hardware investment (e.g., power meters). BitWatts instances use high-throughput communication channels to spread the power consumption across the VM levels and between machines. Our experiments, based on CPU- and memory-intensive benchmarks running on different hardware setups, demonstrate that BitWatts scales both in number of monitored processes and virtualization levels. This non-invasive monitoring solution therefore paves the way for scalable energy accounting that takes into account the dynamic nature of virtualized environments.;;;EuroSys '15
Conference Paper;Hinz M,Miers CC,Pillon MA,Koslovski GP;A Cost Model for IaaS Clouds Based on Virtual Machine Energy Consumption;;2016;;;136–143;Brazilian Computer Society;Porto Alegre, BRA;;Proceedings of the XII Brazilian Symposium on Information Systems on Brazilian Symposium on Information Systems: Information Systems in the Cloud Computing Era - Volume 1;Florianopolis, Santa Catarina, Brazil;2016;9,78858E+12;;;;Reduction in data center energy consumption is a constant motivation for IaaS providers. Among all components, CPU appears as a main energy consumer. Although there is a strong relationship between CPU load and its energy consumption, pricing models of popular IaaS providers do not consider this information as a primary and variable element. This paper quantifies the relationship by identifying the individual consumption of virtual CPUs, which form the basis for an allocation cost model. The proposed model, termed Virtual Power, is faced with Amazon EC2 pricing model pointing a cost reduction for IaaS provider and a proportional sharing between users.;IaaS, Cost, Virtual Machine, Model;;SBSI 2016
Conference Paper;Lu Y,Wei D;Chaos Prediction of Power Systems by Using Deep Learning;;2022;;;11–17;Association for Computing Machinery;New York, NY, USA;;2022 14th International Conference on Machine Learning and Computing (ICMLC);Guangzhou, China;2022;9,78145E+12;;"https://doi.org/10.1145/3529836.3529843;http://dx.doi.org/10.1145/3529836.3529843";10.1145/3529836.3529843;Ensuring the stability of power systems is an important issue that should be considered in order to ensure the social and economic development of a country. Therefore, predicting the chaotic behavior of power systems in order to develop protection measures and keep power systems stable is vital. In this paper, a deep learning algorithm was proposed to predict the chaotic behavior of power systems by using deep long short-term memory (DLSTM) networks, which have two forms: deep long short-term memory with static scenario (DLSTM-s) and deep long-term memory with dynamic scenario (DLSTM-d). The genetic algorithm was used to optimize the hyperparameters of the networks. Then, taking interconnected power systems as an example, the effectiveness of the proposed DLSTM network was verified via numerical simulation. Finally, the experimental results of the DLSTM network were compared with those of the echo state network, multi-recurrent neural network, deep gated recurrent unit, and long short-term memory. Experimental results illustrated that a trained DLSTM network can predict the chaotic behavior of power systems by using the time series data of a single state variable. Moreover, the DLSTM-s network proposed in this paper can achieve competitive prediction performance compared with other baseline methods.;Chaos prediction, Deep learning, Deep long short-term memory, Power system;;ICMLC 2022
Journal Article;Dong X,Chen Y,Chen J,Wang Y,Li J,Ni T,Shi Z,Yin X,Zhuo C;Worst-Case Power Integrity Prediction Using Convolutional Neural Network;ACM Trans. Des. Autom. Electron. Syst.;2023;28;4;;Association for Computing Machinery;New York, NY, USA;;;;2023-05;;1084-4309;"https://doi.org/10.1145/3564932;http://dx.doi.org/10.1145/3564932";10.1145/3564932;Power integrity analysis is an essential step in power distribution network (PDN) sign-off to ensure the performance and reliability of chips. However, with the growing PDN size and increasing scenarios to be validated, it becomes very time- and resource-consuming to conduct full-stack PDN simulation to check the power integrity for different test vectors. Recently, various works have proposed machine learning–based methods for PDN power integrity prediction, many of which still suffer from large training overhead, inefficiency, or non-scalability. Thus, this article proposed an efficient and scalable framework for the worst-case power integrity prediction, which can handle general tasks including dynamic noise prediction and bump current prediction. The framework first reduces the spatial and temporal redundancy in the PDN and input current vector and then employs efficient feature extraction as well as a novel convolutional neural network architecture to predict the worst-case power integrity. Experimental results show that the proposed framework consistently outperforms the commercial tool and the state-of-the-art machine learning method with only 0.63–1.02% mean relative error and 25–69× speedup for noise prediction and 0.22–1.06% mean relative error and 24–64× speedup for bump current prediction.;convolutional neural network, Power distribution network, sign-off, power integrity, dynamic noise validation, bump current prediction;;
Journal Article;Henderson P,Hu J,Romoff J,Brunskill E,Jurafsky D,Pineau J;Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning;J. Mach. Learn. Res.;2020;21;1;;JMLR.org;;;;;2020-01;;1532-4435;;;Accurate reporting of energy and carbon usage is essential for understanding the potential climate impacts of machine learning research. We introduce a framework that makes this easier by providing a simple interface for tracking realtime energy consumption and carbon emissions, as well as generating standardized online appendices. Utilizing this framework, we create a leaderboard for energy efficient reinforcement learning algorithms to incentivize responsible research in this area as an example for other areas of machine learning. Finally, based on case studies using our framework, we propose strategies for mitigation of carbon emissions and reduction of energy consumption. By making accounting easier, we hope to further the sustainable development of machine learning experiments and spur more research into energy efficient algorithms.;climate change, deep learning, green computing, reinforcement learning, energy efficiency;;
Conference Paper;Dutta B,Adhinarayanan V,Feng WC;GPU Power Prediction via Ensemble Machine Learning for DVFS Space Exploration;;2018;;;240–243;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 15th ACM International Conference on Computing Frontiers;Ischia, Italy;2018;9,78145E+12;;"https://doi.org/10.1145/3203217.3203273;http://dx.doi.org/10.1145/3203217.3203273";10.1145/3203217.3203273;A software-based approach to achieve high performance within a power budget often involves dynamic voltage and frequency scaling (DVFS). Thus, accurately predicting the power consumption of an application at different DVFS levels (or more generally, different processor configurations) is paramount for the energy-efficient functioning of a high-performance computing (HPC) system. The increasing prevalence of graphics processing units (GPUs) in HPC systems presents new challenges in power management, and machine learning presents an unique way to improve the software-based power management of these systems. As such, we explore the problem of GPU power prediction at different DVFS states via machine learning. Specifically, we propose a new ensemble technique that incorporates three machine-learning techniques --- sequential minimal optimization regression, simple linear regression, and decision tree --- to reduce the mean absolute error (MAE) to 3.5%.;;;CF '18
Conference Paper;Jeyasurya B;Power System Voltage Instability Monitoring with Artificial Neural Networks;;1994;;;57–62;Gordon and Breach Science Publishers, Inc.;USA;;Proceedings of the 7th International Conference on Industrial and Engineering Applications of Artificial Intelligence and Expert Systems;Austin, Texas, USA;1994;9,78288E+12;;;;;;;IEA/AIE '94
Conference Paper;Zhang B,Han J,Ren Y,Wen H,Song Z,Gan Q,Wei R,Dang X,Zhou B;An Online Power System Dynamics Prediction Based on Deep Neural Network;;2018;;;39–43;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 2018 International Conference on Electronics and Electrical Engineering Technology;Tianjin, China;2018;9,78145E+12;;"https://doi.org/10.1145/3277453.3277464;http://dx.doi.org/10.1145/3277453.3277464";10.1145/3277453.3277464;Building Power analysis has drawn more and more attention in recent years. In this paper, we present a system for online power prediction for the public building. It is based on a 4-layers Deep Neural Network that use architectural metrics of the physical machines collected dynamically by our system to predict the physical machine power consumption. A real implementation of our system shows that the prediction accuracy could reach 76.50%.;Deep computer architecture, Energy management, Deep neural networks;;EEET '18
Conference Paper;Ciftcioglu E,Ricos M;Efficient Power Adaptation against Deep Learning Based Predictive Adversaries;;2019;;;37–42;Association for Computing Machinery;New York, NY, USA;;Proceedings of the ACM Workshop on Wireless Security and Machine Learning;Miami, FL, USA;2019;9,78145E+12;;"https://doi.org/10.1145/3324921.3328787;http://dx.doi.org/10.1145/3324921.3328787";10.1145/3324921.3328787;Wireless communication networks are subject to various types of adversarial attacks, which might be passive in the form of eavesdropping, or active in the form of jamming. For the former category, even if the traffic is encrypted, an adversary performing analysis on observed traffic signatures may lead to leakage of the so called contextual information regarding the traffic. New advances in the field of machine learning also result in significantly more complex adversarial units, which may deduce different forms and uses of such contextual information. In this work, we are interested in power adaptation against an intelligent adversary which utilizes deep learning and attempts to perform predictions and time forecasting on the observed traffic traces to estimate the imminent traffic intensities. Based on its traffic predictions, the adversary might possibly activate its jamming mode and utilize its limited power more efficiently to inflict maximal damage. As a method of mitigation, the transmitter may want to increase transmitter power if it expects a higher probability of jamming, and it has a significant amount of upcoming data to transmit. We leverage Lyapunov optimization and virtual queues to meet a certain level of data transmission reliability while also minimizing power consumption.;power control, wireless security, recurrent neural networks;;WiseML 2019
Conference Paper;Shanti N,Assi A,Shakhshir H,Salman A;Machine Learning-Powered Mobile App for Predicting Used Car Prices;;2022;;;52–60;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 2021 3rd International Conference on Big-Data Service and Intelligent Computation;Xiamen, China;2022;9,78145E+12;;"https://doi.org/10.1145/3502300.3502307;http://dx.doi.org/10.1145/3502300.3502307";10.1145/3502300.3502307;Buying and selling used cars is a common practice in all countries. When looking to sell a car, the seller decides the price of their car after monitoring the prices of similar cars in the advertisements. When someone is looking to buy a car, they watch advertisements for similar cars to get an idea of the price of the car they want to purchase. Despite the availability of blue books that provide an estimate of automotive pricing, real market prices vary depending on demand and supply. In this paper, we applied cutting-edge machine learning techniques to automate this process. The training data set is up-to-date and it was collected from an active commercial website. We created semi-automated rule-based scripts to clean and prepare the data for machine learning. Several machine learning algorithms were explored to generate an approximate value for the car pricing, including Artificial Neural Network, Support Vector Machine, K-Nearest Neighbors, Random Forest, and Gradient Boosted Decision Tree. To determine the features that most affect the price, extensive data analysis and cleaning were undertaken. Our findings indicate that the testing accuracy is 90%. Finally, a mobile application was created that provides an estimated price of a given car's properties, guiding a user in determining the price of their car. The complete code and the data used to obtain these results can be accessed on GitHub at [1] [19].;mobile application,  Car price Prediction, Regression, Machine learning;;BDSIC '21
Conference Paper;Zhang K;Application of Data Mining Based on Machine Learning in Automobile Power Prediction;;2022;;;1678–1681;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 2021 5th International Conference on Electronic Information Technology and Computer Engineering;Xiamen, China;2022;9,78145E+12;;"https://doi.org/10.1145/3501409.3501532;http://dx.doi.org/10.1145/3501409.3501532";10.1145/3501409.3501532;Automobile power prediction is the key to the automobile industry. Only by reasonably predicting the automobile power can we produce cars that meet the needs of consumers. Compared with traditional methods, machine learning model improves the accuracy of classification in power. Machine learning models include BP neural network, random forest and KNN algorithm. In order to select the optimal vehicle power prediction model, this paper compares the advantages and disadvantages of these three machine learning models through design experiments.;KNN algorithm, BP neural network, machine learning, random forest, automobile power prediction;;EITCE '21
Conference Paper;Rugwiro U,Chunhua G;Customization of Virtual Machine Allocation Policy Using K-Means Clustering Algorithm to Minimize Power Consumption in Data Centers;;2017;;;;Association for Computing Machinery;New York, NY, USA;;Proceedings of the Second International Conference on Internet of Things, Data and Cloud Computing;Cambridge, United Kingdom;2017;9,78145E+12;;"https://doi.org/10.1145/3018896.3018947;http://dx.doi.org/10.1145/3018896.3018947";10.1145/3018896.3018947;Cloud Computing provides rapid provision of computing resources like processing power, memory, network resources, storage, etc. Running computing resources for longer time, leads energy consumption, increase the emission of Carbon Dioxide (CO2) and increase the expenditure cost for the resources usage. Hence there is a necessity to minimize the execution time to reduce energy consumption in the cloud environment. One of the existing approaches to reducing energy consumption is based on Migration and Placement Policy for Virtual Machine, but still improving placement technique we can further minimize power consumption. In our proposed architecture for cloud resource allocation based on Clustering method, we do map a group of tasks to virtual machines. For clustering, we work on task usage of CPU, memory, and bandwidth. This proposed clustering technique further decreases energy consumption by efficient resource allocation.;cloud computing, K-means clustering, virtual machine allocation, energy efficiency;;ICC '17
Conference Paper;Burger M,Boer CA,Straub E,Saanen YA;Predictive Maintenance Powered by Machine Learning and Simulation;;2023;;;2807–2818;IEEE Press;Singapore, Singapore;;Proceedings of the Winter Simulation Conference;;2023;;;;;To optimize the balance between costs and reliability of cranes, it is important to perform maintenance when the risk of failures becomes high while possibly delaying planned maintenance when the crane shows no signs of possible problems. To accomplish this, we investigate the possibility of applying predictive maintenance for container-handling cranes. The application of predictive maintenance requires historical data collection and preprocessing of equipment sensor and maintenance data. To get a feeling of the possibilities and limitations of predictive maintenance for container-handling cranes, before investing time and money to collect operational data, we have used simulations to generate synthetic data for a few components of the cranes. Using the simulated crane data, a prediction model was trained to predict upcoming component failures. The results show that using simulation we can identify the possibilities and limitations of machine learning for predicting failures of components of the crane.;;;WSC '22
Book Chapter;Bengio S,Deng L,Morency LP,Schuller B;Perspectives on Predictive Power of Multimodal Deep Learning: Surprises and Future Directions;;2018;;;455–472;Association for Computing Machinery and Morgan & Claypool;;The Handbook of Multimodal-Multisensor Interfaces: Signal Processing, Architectures, and Detection of Emotion and Cognition - Volume 2;;;2018;9,78197E+12;;"https://doi.org/10.1145/3107990.3108006;http://dx.doi.org/10.1145/3107990.3108006";10.1145/3107990.3108006;;;;
Conference Paper;Chuah JY;Machine Learning GPU Power Measurement on Chameleon Cloud;;2017;;;181;Association for Computing Machinery;New York, NY, USA;;Proceedings of The10th International Conference on Utility and Cloud Computing;Austin, Texas, USA;2017;9,78145E+12;;"https://doi.org/10.1145/3147213.3149450;http://dx.doi.org/10.1145/3147213.3149450";10.1145/3147213.3149450;"Machine Learning (ML) is becoming critical for many industrial and scientific endeavors, and has a growing presence in High Performance Computing (HPC) environments. Neural network training requires long execution times for large data sets, and libraries like TensorFlow implement GPU acceleration to reduce the total runtime for each calculation. This tutorial demonstrates how to 1) use Chameleon Cloud to perform comparative studies of ML training performance across different hardware configurations; and 2) run and monitor power utilization of TensorFlow on NVIDIA GPUs.";power, gpu, machine learning, acm proceedings;;UCC '17
Conference Paper;Karami E,Rafi M,Ridah A;Output PV Power Prediction Using an Artificial Neural Network in Casablanca, Morocco;;2019;;;;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 4th International Conference on Smart City Applications;Casablanca, Morocco;2019;9,78145E+12;;"https://doi.org/10.1145/3368756.3369084;http://dx.doi.org/10.1145/3368756.3369084";10.1145/3368756.3369084;Optimal use of renewable energy requires its characterization and prediction to size detectors or estimate the potential of power plants [20-21]. In terms of prediction, electricity suppliers are interested in different horizons to manage power plants and predict their production [1-2]. This paper proposes a model for predicting the output power in photovoltaic (PV) panels installed on the rooftop of the Ben m'sik faculty at Hassan II University, Casablanca, Morocco, and this model is based on a multilayer perceptron (MLP) model. In this work, different combinations of weather variables were used to develop this model and for validate the proposed model results different practical measurement methods are used, such as mean square error (MSE), mean absolute error (MAE), correlation (R) and coefficient of determination (R2). The determination coefficient of the proposed model is 0.98501 with an RMSE value of 30.663. The proposed model was tested on new data, the results showed that the model works with a good preferment and that the prediction quality depends on the time of year with a determination coefficient of 0.9972, 0.9856, 0.9487 and 0.9942 for summer, autumn, winter and spring respectively.;artificial neural network, PV system, output PV power, meteorological parameters, prediction;;SCA '19
Conference Paper;Dong X,Chen Y,Yin X,Zhuo C;Worst-Case Dynamic Power Distribution Network Noise Prediction Using Convolutional Neural Network;;2022;;;1225–1230;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 59th ACM/IEEE Design Automation Conference;San Francisco, California;2022;9,78145E+12;;"https://doi.org/10.1145/3489517.3530600;http://dx.doi.org/10.1145/3489517.3530600";10.1145/3489517.3530600;Worst-case dynamic PDN noise analysis is an essential step in PDN sign-off to ensure the performance and reliability of chips. However, with the growing PDN size and increasing scenarios to be validated, it becomes very time- and resource-consuming to conduct full-stack PDN simulation to check the worst-case noise for different test vectors. Recently, various works have proposed machine learning based methods for supply noise prediction, many of which still suffer from large training overhead, inefficiency, or non-scalability. Thus, this paper proposed an efficient and scalable framework for the worst-case dynamic PDN noise prediction. The framework first reduces the spatial and temporal redundancy in the PDN and input current vector, and then employs efficient feature extraction as well as a novel convolutional neural network architecture to predict the worst-case dynamic PDN noise. Experimental results show that the proposed framework consistently outperforms the commercial tool and the state-of-the-art machine learning method with only 0.63--1.02% mean relative error and 25--69× speedup.;;;DAC '22
Conference Paper;Li R,Li,Zhang S;A Deep Learning Prediction Process Based on Low-Power Heterogeneous Multi Core Architecture;;2018;;;220–224;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 2nd International Conference on Advances in Image Processing;Chengdu, China;2018;9,78145E+12;;"https://doi.org/10.1145/3239576.3239609;http://dx.doi.org/10.1145/3239576.3239609";10.1145/3239576.3239609;With the rapid development of machine learning both in theory and practice in the past decade. And recently, it is widely used in applications and cloud services. As the emerging field of machine learning, deep learning shows excellent ability in solving complex learning problems. In this paper, we designed a deep learning prediction process based on low-power heterogeneous multi core architecture. Firstly, the fundamental principle of image recognition method based on deep learning reviewed as the basis of the research. Secondly, a set of key algorithm design to parallel access and process image for object detection based on Parallella multi core platform was proposed to improve the detection speed and the computational resource efficiency on single node. Thirdly, Rockchip RK3288 SoC with 4 Arm Cortex-A17 cores hardware platform, Xilinx Zynq and Adapteva Epiphany combined heterogeneous multi core hardware platform was introduced. Some key designs based on Parallella board's architecture to achieve image recognition was proposed to improve the recognition speed and the computational resource efficiency. Finally, The experimental results that based on Parallella board indicate that the proposed image recognition system can achieve nearly 14.8 times speedup than dual-core Arm which was integrated in Parallella board with similar accuracy and achieve 8.6 times speedup than RK3288 board which has the newest series of high-performance Arm core CPU as the control included 4 Arm Cortex-A17 cores.;Epiphany, the deep learning prediction process, multi core, Arm Cortex core, Accelerator;;ICAIP '18
Conference Paper;Li Y,Hu H,Wen Y,Zhang J;Learning-Based Power Prediction for Data Centre Operations via Deep Neural Networks;;2016;;;;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 5th International Workshop on Energy Efficient Data Centres;Waterloo, Ontario, Canada;2016;9,78145E+12;;"https://doi.org/10.1145/2940679.2940685;http://dx.doi.org/10.1145/2940679.2940685";10.1145/2940679.2940685;"Modelling and analyzing power consumption for data centres can diagnose potential energy-hungry components and applications, and facilitate in-time control, benefiting the energy efficiency of data centers. However, solutions to this problem, including static power models and canonical prediction models, either aim to build a static relationship between power consumption and hardware/application configurations without considering the dynamic fluctuation of power; or simply treat it as time series, ignoring the inherit power data characteristics. To tackle these issues, in this paper, we present a systematic power prediction framework based on extensive power dynamic profiling and deep learning models. In particular, we first analyse different power series samples to illustrate their noise patterns; accordingly we propose a power data de-noising method, which lowers noise interference to the modelling. With the pretreated data, we propose two deep learning based prediction models, including a fine-grained model and a coarse-grained model, which are suitable for different time scales. In the fine-grained prediction model, a recursive autoencoder (AE) is employed for short-duration prediction; in the coarse-grained model, an AE is used to encode massive fine-grained historical data as a further data pretreatment for long-duration prediction. Experimental results show that our proposed models outperform canonical prediction methods with higher accuracy, up to 79% error reduction for certain cases.";power modelling, data centre, deep learning, power prediction;;E2DC '16
Journal Article;Lu YC,Nath S,Pentapati S,Lim SK;ECO-GNN: Signoff Power Prediction Using Graph Neural Networks with Subgraph Approximation;ACM Trans. Des. Autom. Electron. Syst.;2023;28;4;;Association for Computing Machinery;New York, NY, USA;;;;2023-05;;1084-4309;"https://doi.org/10.1145/3569942;http://dx.doi.org/10.1145/3569942";10.1145/3569942;Modern electronic design automation flows depend on both implementation and signoff tools to perform timing-constrained power optimization through Engineering Change Orders (ECOs), which involve gate sizing and threshold-voltage (Vth)-assignment of standard cells. However, the signoff ECO optimization is highly time-consuming, and the power improvement is hard to predict in advance. Ever since the industrial benchmarks released by the ISPD-2012 gate-sizing contest, active research has been conducted extensively to improve the optimization process. Nonetheless, previous works were mostly based on heuristics or analytical methods whose timing models were oversimplified and lacked of formal validations from commercial signoff tools. In this article, we propose ECO-graph neural networks (GNN), a transferable graph-learning-based framework, which harnesses GNNs to perform commercial-quality signoff power optimization through discrete (Vth-assignment. One of the highlights of our framework is that it generates tool-accurate optimization results instantly on unseen netlists that are not utilized in the training process. Furthermore, we propose a subgraph approximation technique to improve training and inferencing time of the proposed GNN model. We show that design instances with non-overlapping subgraphs can be optimized in parallel so as to improve the inference time of the learning-based model. Finally, we implement a GNN-based explanation method to interpret the optimization results achieved by our framework. Experimental results on 14 industrial designs, including a RISC-V-based multi-core system and the renowned ISPD-2012 benchmarks, demonstrate that our framework achieves up to 14× runtime improvement with similar signoff power optimization quality compared with Synopsys PrimeTime, an industry-leading signoff tool.;power prediction, Graph Neural Networks (GNNs), Engineering Change Order (ECO);;
Conference Paper;Hönig T,Herzog B,Schröder-Preikschat W;Energy-Demand Estimation of Embedded Devices Using Deep Artificial Neural Networks;;2019;;;617–624;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing;Limassol, Cyprus;2019;9,78145E+12;;"https://doi.org/10.1145/3297280.3297338;http://dx.doi.org/10.1145/3297280.3297338";10.1145/3297280.3297338;The need for high performance in embedded devices grows at a breathtaking pace. Embedded processors that satisfy the hunger for superlative processing power share a common issue: the increasing performance leads to growing energy demands during operation. As energy remains a limited resource to embedded devices, it is critical to optimise software components for low power. Low-power software needs energy models which, however, are increasingly difficult to create as to the complexity of today's devices.In this paper we present a black-box approach to construct precise energy models for complex hardware devices. We apply machine-learning techniques in combination with fully automatic energy measurements and evaluate our approach with an ARM Cortex platform. We show that our system estimates the energy demand of program code with a mean percentage error of 1.8% compared to the results of energy measurements.;machine learning, embedded systems, energy demand analysis;;SAC '19
Conference Paper;Gong H;Uninterruptible Power Supply State of Charge Estimation Based on BP Neural Network;;2021;;;240–243;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 2020 8th International Conference on Information Technology: IoT and Smart City;Xi'an, China;2021;9,78145E+12;;"https://doi.org/10.1145/3446999.3447639;http://dx.doi.org/10.1145/3446999.3447639";10.1145/3446999.3447639;Objetive: Display console is the software and hardware platform of warship and submarine control system. It's very important to strengthen the performance and improve the life of the Uninterruptible power supply (UPS) in display console . The residual capacity of UPS is a nonlinear function of voltage, discharge current, temperature and other variables. At present, there are some problems such as large measurement error and poor state prediction, what influence the battery power management system's management effectiveness. Therefore, this paper studies the estimation method of battery residual capacity based on BP Algorithm according to the principle of neural network, so as to improve the estimation accuracy of UPS residual capacity. Method: Firstly, we establish the BP neural network model consists of three layers, secondly gather UPS experimental data set, then build, train and test BP network model with LM algorithm by Matlab program language, finally gather the state of charge(SOC) training results. Result: Experimental results indicate that the method in this paper can estimate UPS SOC accurately and efficiently. Conclusion: It can satisfy the system requirements of uninterruptible power supply SOC estimation.;state of charge, BP neural network, MATLAB, uninterruptible power supply;;ICIT '20
Conference Paper;Jia Z,Lyu X,Zhang W,Martin RP,Howard RE,Zhang Y;Continuous Low-Power Ammonia Monitoring Using Long Short-Term Memory Neural Networks;;2018;;;224–236;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 16th ACM Conference on Embedded Networked Sensor Systems;Shenzhen, China;2018;9,78145E+12;;"https://doi.org/10.1145/3274783.3274836;http://dx.doi.org/10.1145/3274783.3274836";10.1145/3274783.3274836;Accurate and continuous ammonia monitoring is important for laboratory animal studies and many other applications. Existing solutions are often expensive, inaccurate, or unsuitable for long-term monitoring. In this work, we propose a new ammonia monitoring approach that is low-power, automatic, accurate, and wireless.Our system uses metal oxide sensors which change their electrical resistance due to an induced reduction reaction with ammonia at high temperatures. Traditional methods infer the ammonia level by measuring the sensor's electrical resistance after it reaches equilibrium. Such a system consumes a significant amount of energy because reaching equilibrium requires heating the sensor for minutes. Our proposed approach does not wait for equilibrium, but tries to predict the resistance at equilibrium using the sensor's initial resistance response curve in a very short heating pulse (as short as 200ms). The prediction model is built on long short-term memory (LSTM) neural networks.We built 38 prototype sensors and a home-grown gas flow system. In a 3-month in-lab testing period, we conducted extensive experiments and collected 13,770 measurements. Our model accurately predicts the equilibrium state resistance value, with an average error rate of 0.12%. The final average estimation error for the ammonia concentration level is 9.38ppm. Given the ultra low power consumption and accurate measurements, we have partnered with cage vendors and deployed our system at two animal research facilities (NIH and Cornell University) for month-long medical trials.;Low-power, Neural Networks, Ammonia Sensor, Long Short-term Memory, Transient Response;;SenSys '18
Journal Article;Zhang Q,Nicolson A,Wang M,Paliwal KK,Wang C;DeepMMSE: A Deep Learning Approach to MMSE-Based Noise Power Spectral Density Estimation;IEEE/ACM Trans. Audio, Speech and Lang. Proc.;2020;28;;1404–1415;IEEE Press;;;;;2020-05;;2329-9290;"https://doi.org/10.1109/TASLP.2020.2987441;http://dx.doi.org/10.1109/TASLP.2020.2987441";10.1109/TASLP.2020.2987441;An accurate noise power spectral density (PSD) tracker is an indispensable component of a single-channel speech enhancement system. Bayesian-motivated minimum mean-square error (MMSE)-based noise PSD estimators have been the most prominent in recent time. However, they lack the ability to track highly non-stationary noise sources due to current methods of a priori signal-to-noise (SNR) estimation. This is caused by the underlying assumption that the noise signal changes at a slower rate than the speech signal. As a result, MMSE-based noise PSD trackers exhibit a large tracking delay and produce noise PSD estimates that require bias compensation. Motivated by this, we propose an MMSE-based noise PSD tracker that employs a temporal convolutional network (TCN) a priori SNR estimator. The proposed noise PSD tracker, called DeepMMSE makes no assumptions about the characteristics of the noise or the speech, exhibits no tracking delay, and produces an accurate estimate that requires no bias correction. Our extensive experimental investigation shows that the proposed DeepMMSE method outperforms state-of-the-art noise PSD trackers and demonstrates the ability to track abrupt changes in the noise level. Furthermore, when employed in a speech enhancement framework, the proposed DeepMMSE method is able to outperform state-of-the-art noise PSD trackers, as well as multiple deep learning approaches to speech enhancement. Availability: DeepMMSE is available at: https://github.com/anicolson/DeepXi.;;;
Conference Paper;Al Qathrady M,Helmy A;Improving BLE Distance Estimation and Classification Using TX Power and Machine Learning: A Comparative Analysis;;2017;;;79–83;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 20th ACM International Conference on Modelling, Analysis and Simulation of Wireless and Mobile Systems;Miami, Florida, USA;2017;9,78145E+12;;"https://doi.org/10.1145/3127540.3127577;http://dx.doi.org/10.1145/3127540.3127577";10.1145/3127540.3127577;Distance estimation and proximity classification techniques are essential for numerous IoT applications and in providing efficient services in smart cities. Bluetooth Low Energy (BLE) is designed for IoT devices, and its received signal strength indicator (RSSI) has been used in distance and proximity estimation, though they are noisy and unreliable. In this study, we leverage the BLE TX power level in BLE models.We adopt a comparative analysis framework that utilizes our extensive data library of measurements. It considers commonly used state-of-the-art model, in addition to our data-driven proposed approach. The RSSI and TX power are integrated into several parametric models such as log shadowing and Android Beacon library models, and machine learning models such as linear regression, decision trees, random forests and neural networks. Specific mobile apps are developed for the study experiment. We have collected more than 1.8 millions of BLE records between encounters with various distances that range from 0.5 to 22 meters in an indoor environment. Interestingly, considering TX power when estimating the distance reduced the mean errors by up to 46% in parametric models and by up to 35% in machine learning models. Also, the proximity classification accuracy increased by up to 103% and 70% in parametric and machine learning models, respectively. This work is one of the first studies (if not the first) that analyze in depth the TX power variations in improving the distance estimation and classification.;ble, distance estimation, iot, proximity classification;;MSWiM '17
Conference Paper;Bergen A,Desmarais R,Ganti S,Stege U;Towards Software-Adaptive Green Computing Based on Server Power Consumption;;2014;;;9–16;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 3rd International Workshop on Green and Sustainable Software;Hyderabad, India;2014;9,78145E+12;;"https://doi.org/10.1145/2593743.2593745;http://dx.doi.org/10.1145/2593743.2593745";10.1145/2593743.2593745;With the proliferation of virtualization and cloud comput- ing, optimizing the power usage effectiveness of enterprise data centers has become a laudable goal and a critical re- quirement in IT operations all over the world. While a sig- nificant body of research exists to measure, monitor, and control the greenness level of hardware components, signif- icant research efforts are needed to relate hardware energy consumption to energy consumption due to program exe- cution. In this paper we report on our investigations to characterize power consumption profiles for different types of compute and memory intensive software applications. In particular, we focus on studying the effects of CPU loads on the power consumption of compute servers by monitoring rack power consumption in a data center. We conducted a series of experiments with a variety of processes of differ- ent complexity to understand and characterize the effect on power consumption. Combining processes of varying com- plexity with varying resource allocations produces different energy consumption levels. The challenge is to optimize pro- cess orchestration based on a power consumption framework to accrue energy savings. Our ultimate goal is to develop smart adaptive green computing techniques, such as adap- tive job scheduling and resource provisioning, to reduce over- all power consumption in data centers or clouds.;cloud computing, data centers, Power consumption measurments, power consumption framework, green-aware, power application pro- files, self-adaptive green computing;;GREENS 2014
Conference Paper;Shoukourian H,Wilde T,Auweter A,Bode A,Tafani D;Predicting Energy Consumption Relevant Indicators of Strong Scaling HPC Applications for Different Compute Resource Configurations;;2015;;;115–126;Society for Computer Simulation International;San Diego, CA, USA;;Proceedings of the Symposium on High Performance Computing;Alexandria, Virginia;2015;9,78151E+12;;;;"Finding the best energy-performance tradeoffs for High Performance Computing (HPC) applications is a major challenge for many modern supercomputing centers. With the increased focus on data center energy efficiency and the emergence of possible data center power constraints, making the right decision at a given time is becoming more important. A real-world situation like ""can a given 1000 compute node application be executed at a maximum of 2.7 GHz CPU frequency without going over the energy provider defined power band, or the available monthly energy limit?"" is just one example of the types of decisions HPC data centers will face. The previously developed Adaptive Energy and Power Consumption Prediction (AEPCP) model answers this question for the case of a fixed CPU frequency. This paper will extend the AEPCP process to enable the development of analytical models for estimating application execution time, power, and energy consumptions as functions of the number of compute nodes and maximum operating CPU frequency. Based on these analytical models, an adaptive model (Lightweight Adaptive Consumption Prediction (LACP)) is presented that implements the extended prediction process. This information allows for improved estimation of potential energy-performance costs and tradeoffs of applications and thus identifies the optimal resource configuration for specific data center boundary conditions.";LACP, HPC, power and energy capping, consumption modeling and prediction, compute node number and CPU frequency;;HPC '15
Journal Article;Wu TH,Lin PC,Chou HH,Shen MR,Hsieh SY;Pathogenicity Prediction of Single Amino Acid Variants With Machine Learning Model Based on Protein Structural Energies;IEEE/ACM Trans. Comput. Biol. Bioinformatics;2022;20;1;606–615;IEEE Computer Society Press;Washington, DC, USA;;;;2022-12;;1545-5963;"https://doi.org/10.1109/TCBB.2021.3139048;http://dx.doi.org/10.1109/TCBB.2021.3139048";10.1109/TCBB.2021.3139048;The most popular tools for predicting pathogenicity of single amino acid variants (SAVs) were developed based on sequence-based techniques. SAVs may change protein structure and function. In the context of van der Waals force and disulfide bridge calculations, no method directly predicts the impact of mutations on the energies of the protein structure. Here, we combined machine learning methods and energy scores of protein structures calculated by Rosetta Energy Function 2015 to predict SAV pathogenicity. The accuracy level of our model (0.76) is higher than that of six prediction tools. Further analyses revealed that the differential reference energies, attractive energies, and solvation of polar atoms between wildtype and mutant side-chains played essential roles in distinguishing benign from pathogenic variants. These features indicated the physicochemical properties of amino acids, which were observed in 3D structures instead of sequences. We added 16 features to Rhapsody (the prediction tool we used for our data set) and consequently improved its performance. The results indicated that these energy scores were more appropriate and more detailed representations of the pathogenicity of SAVs.;;;
Conference Paper;Liu N,Ding C,Wang Y,Hu J;Neural Network-Based Prediction Algorithms for In-Door Multi-Source Energy Harvesting System for Non-Volatile Processors;;2016;;;275–280;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 26th Edition on Great Lakes Symposium on VLSI;Boston, Massachusetts, USA;2016;9,78145E+12;;"https://doi.org/10.1145/2902961.2903037;http://dx.doi.org/10.1145/2902961.2903037";10.1145/2902961.2903037;Due to size, longevity, safety, and recharging concerns, energy harvesting is becoming a better choice for many wearable embedded systems than batteries. However, harvested energy is intrinsically unstable. In order to overcome this drawback, non-volatile processors (NVPs) have been proposed to bridge intermittent program execution. However, even with NVPs, frequent power interruptions will severely degrade system performance. Hence, in this paper we adopt a multi-source in-door energy harvesting architecture to compensate the shortcoming of single energy source. We further investigate power harvesting prediction techniques, which are critical for NVP systems since they can coordinate with task scheduler in the NVP system to compensate the intermittent ambient energy harvesting. We investigate prediction methods both for single energy harvesting source and for multiple energy harvesting sources, the total output power of which is more stable compared with the single source case. A comprehensive evaluation framework has been developed using actually measured harvesting traces on the proposed neural network-based power harvesting prediction methods. It turns out that the most favorable prediction methods are directly predicting the total output power of DC-DC converters (connecting between energy sources and NVP), or predicting the total input power of DC-DC converters first and then inferring the total output power using a learned mapping function, for multi-source power harvesting predictions.;energy harvesting, neural network, non-volatile processors, multiple energy source;;GLSVLSI '16
Conference Paper;Basmadjian R,Ali N,Niedermeier F,de Meer H,Giuliani G;A Methodology to Predict the Power Consumption of Servers in Data Centres;;2011;;;1–10;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 2nd International Conference on Energy-Efficient Computing and Networking;New York, New York, USA;2011;9,78145E+12;;"https://doi.org/10.1145/2318716.2318718;http://dx.doi.org/10.1145/2318716.2318718";10.1145/2318716.2318718;Until recently, there have been relatively few studies exploring the power consumption of ICT resources in data centres. In this paper, we propose a methodology to capture the behaviour of most relevant energy-related ICT resources in data centres and present a generic model for them. This is achieved by decomposing the design process into four modelling phases. Furthermore, unlike the state-of-the-art approaches, we provide detailed power consumption models at server and storage levels. We evaluate our model for different types of servers and show that it suffers from an error rate of 2% in the best case, and less than 10% in the worst case.;IT resources, power consumption, modelling, data centre;;e-Energy '11
Conference Paper;von Kistowski J,Grohmann J,Schmitt N,Kounev S;Predicting Server Power Consumption from Standard Rating Results;;2019;;;301–312;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 2019 ACM/SPEC International Conference on Performance Engineering;Mumbai, India;2019;9,78145E+12;;"https://doi.org/10.1145/3297663.3310298;http://dx.doi.org/10.1145/3297663.3310298";10.1145/3297663.3310298;Data center providers and server operators try to reduce the power consumption of their servers. Finding an energy efficient server for a specific target application is a first step in this regard. Estimating the power consumption of an application on an unavailable server is difficult, as nameplate power values are generally overestimations. Offline power models are able to predict the consumption accurately, but are usually intended for system design, requiring very specific and detailed knowledge about the system under consideration.In this paper, we introduce an offline power prediction method that uses the results of standard power rating tools. The method predicts the power consumption of a specific application for multiple load levels on a target server that is otherwise unavailable for testing. We evaluate our approach by predicting the power consumption of three applications on different physical servers. Our method is able to achieve an average prediction error of 9.49% for three workloads running on real-world, physical servers.;interpolation, power, regression, energy efficiency, prediction, performance, benchmarking, load level;;ICPE '19
Conference Paper;Rivero L,Diniz J,Silva G,Borralho G,Braz Junior G,Paiva A,Alves E,Oliveira M;Deployment of a Machine Learning System for Predicting Lawsuits Against Power Companies: Lessons Learned from an Agile Testing Experience for Improving Software Quality;;2021;;;;Association for Computing Machinery;New York, NY, USA;;Proceedings of the XIX Brazilian Symposium on Software Quality;São Luís, Brazil;2021;9,78145E+12;;"https://doi.org/10.1145/3439961.3439991;http://dx.doi.org/10.1145/3439961.3439991";10.1145/3439961.3439991;"The advances in Machine Learning (ML) require software organizations to evolve their development processes in order to improve the quality of ML systems. Within the software development process, the testing stage of an ML system is more critical, considering that it is necessary to add data validation, trained model quality evaluation, and model validation to traditional unit, integration tests and system tests. In this paper, we focus on reporting the lessons learned of using model testing and exploratory testing within the context of the agile development process of an ML system that predicts lawsuits proneness in energy supply companies. Through the development of the project, the SCRUM agile methodology was applied and activities related to the development of the ML model and the development of the end-user application were defined. After the testing process of the ML model, we managed to achieve 93.89 accuracy; 95.58 specificity; 88.84 sensitivity; and 87.09 precision. Furthermore, we focused on the quality of use of the application embedding the ML model, by carrying out exploratory testing. As a result, through several iterations, different types of defects were identified and corrected. Our lessons learned support software engineers willing to develop ML systems that consider both the ML model and the end-user application.";and Tools, Methods, Verification, Software Processes, and Testing, Validation;;SBQS '20
Conference Paper;Palma I,Ladeira M,Reis AC;Machine Learning Predictive Model for the Passive Transparency at the Brazilian Ministry of Mines and Energy;;2021;;;76–81;Association for Computing Machinery;New York, NY, USA;;DG.O2021: The 22nd Annual International Conference on Digital Government Research;Omaha, NE, USA;2021;9,78145E+12;;"https://doi.org/10.1145/3463677.3463715;http://dx.doi.org/10.1145/3463677.3463715";10.1145/3463677.3463715;This paper presents a case study based on the CRISP-DM Model and the use of Text Mining tools and techniques to automate the Passive Transparency process at the Brazilian Ministry of Mines and Energy. Thus, a Machine Learning Model is proposed to predict the class of the technical unit responsible for the data/information requested by citizens. Through the application of the algorithm LDA and TF-IDF it was possible to map the topics of the most relevant subjects for society. The stability of the model was tested from the comparative analysis between 5 known classification algorithms (Random Forest, Multinomial NB, Linear SVC, Logistic Regression, XGBoost and Gradient Boosting). XGBoost presented better performance and precision in multiclass learning outcomes.;Passive Transparency, Predictive Analisys and XGBoost., Topic Modeling, Multicriteria Decision Making, Machine Learning Algorithms;;DG.O'21
Conference Paper;Dariol Q,Le Nours S,Helms D,Stemmer R,Pillement S,Grüttner K;Fast Yet Accurate Timing and Power Prediction of Artificial Neural Networks Deployed on Clock-Gated Multi-Core Platforms;;2023;;;79–86;Association for Computing Machinery;New York, NY, USA;;Proceedings of the DroneSE and RAPIDO: System Engineering for Constrained Embedded Systems;Toulouse, France;2023;;;"https://doi.org/10.1145/3579170.3579263;http://dx.doi.org/10.1145/3579170.3579263";10.1145/3579170.3579263;When deploying Artificial Neural Networks (ANNs) onto multi-core embedded platforms, an intensive evaluation flow is necessary to find implementations that optimize resource usage, timing and power. ANNs require indeed significant amounts of computational and memory resources to execute, while embedded execution platforms offer limited resources with strict power budget. Concurrent accesses from processors to shared resources on multi-core platforms can lead to bottlenecks with impact on performance and power. Existing approaches show limitations to deliver fast yet accurate evaluation ahead of ANN deployment on the targeted hardware. In this paper, we present a modeling flow for timing and power prediction in early design stage of fully-connected ANNs on multi-core platforms. Our flow offers fast yet accurate predictions with consideration of shared communication resources and scalability in regards of the number of cores used. The flow is evaluated on real measurements for 42 mappings of 3 fully-connected ANNs executed on a clock-gated multi-core platform featuring two different communication modes: polling or interrupt-based. Our modeling flow predicts timing with accuracy and power with accuracy on the tested mappings for an average simulation time of 0.23 s for 100 iterations. We then illustrate the application of our approach for efficient design space exploration of ANN implementations.;System Level Simulation, Power Model, Multi-Core, Artificial Neural Networks;;RAPIDO '23
Journal Article;Dey S,Nandi S,Trivedi G;Machine Learning Approach for Fast Electromigration Aware Aging Prediction in Incremental Design of Large Scale On-Chip Power Grid Network;ACM Trans. Des. Autom. Electron. Syst.;2020;25;5;;Association for Computing Machinery;New York, NY, USA;;;;2020-07;;1084-4309;"https://doi.org/10.1145/3399677;http://dx.doi.org/10.1145/3399677";10.1145/3399677;With the advancement of technology nodes, Electromigration (EM) signoff has become increasingly difficult, which requires a considerable amount of time for an incremental change in the power grid (PG) network design in a chip. The traditional Black’s empirical equation and Blech’s criterion are still used for EM assessment, which is a time-consuming process. In this article, for the first time, we propose a machine learning (ML) approach to obtain the EM-aware aging prediction of the PG network. We use neural network--based regression as our core ML technique to instantly predict the lifetime of a perturbed PG network. The performance and accuracy of the proposed model using neural network are compared with the well-known standard regression models. We also propose a new failure criterion based on which the EM-aging prediction is done. Potential EM-affected metal segments of the PG network is detected by using a logistic-regression--based classification ML technique. Experiments on different standard PG benchmarks show a significant speedup for our ML model compared to the state-of-the-art models. The predicted value of MTTF for different PG benchmarks using our approach is also better than some of the state-of-the-art MTTF prediction models and comparable to the other accurate models.;Electromigration, machine learning, regression, power grid network, reliability, MTTF, neural network;;
Conference Paper;Park JY,Kim ST,Lee JK,Ham JW,Oh KY;Automatic Inspection Drone with Deep Learning-Based Auto-Tracking Camera Gimbal to Detect Defects in Power Lines;;2020;;;;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 3rd International Conference on Vision, Image and Signal Processing;Vancouver, BC, Canada;2020;9,78145E+12;;"https://doi.org/10.1145/3387168.3387176;http://dx.doi.org/10.1145/3387168.3387176";10.1145/3387168.3387176;The traditional drone inspection performed by human operators is unsuited for the purpose of inspecting power transmission lines, because steel towers and their spans are too high and wide to be inspected with a 250 m line of sight. For this reason, the KEPCO Research Institute developed a new inspection drone system that can automatically fly a predetermined flight path based on the GPS coordinates of steel towers, filming a video of power transmission lines with a high definition camera and a thermal imaging camera. In this system, a camera gimbal with the cameras was still controlled by a human operator from a long distance away. When the drone approached close to a steel tower, however, the camera gimbal was often unable to be controlled and real-time video transmission for the gimbal operator was sometimes interrupted due to radio-frequency interference from steel structure and energized power conductors. To solve such a control problem in the field, we also developed an auto-tracking camera gimbal that can automatically track and photograph power facilities on the basis of Deep Learning. With the automatic gimbal, the entire inspection process can be fully automated. The effectiveness of the developed overall system was confirmed through field tests.;Deep Learning, Power Line Inspection, Gimbal, Automatic Drone, Auto-tracking, Camera;;ICVISP 2019
Conference Paper;Chiroma H,Gital AY,Abubakar A,Usman MJ,Waziri U;Optimization of Neural Network through Genetic Algorithm Searches for the Prediction of International Crude Oil Price Based on Energy Products Prices;;2014;;;;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 11th ACM Conference on Computing Frontiers;Cagliari, Italy;2014;9,78145E+12;;"https://doi.org/10.1145/2597917.2597956;http://dx.doi.org/10.1145/2597917.2597956";10.1145/2597917.2597956;This study investigated the prediction of crude oil price based on energy product prices using genetically optimized Neural Network (GANN). It was found from experimental evidence that the international crude oil price can be predicted based on energy product prices. The comparison of the prediction performance accuracy of the propose GANN with Support Vector Machine (SVM), Vector Autoregression (VAR), and Feed Forward NN (FFNN) suggested that the propose GANN was more accurate than the SVM, VAR, and FFNN in the prediction accuracy and time computational complexity. The propose GANN was able to improve the performance accuracy of the comparison algorithms. Our approach can easily be modified for the prediction of similar commodities.;genetic algorithm, neural network, crude oil price;;CF '14
Conference Paper;Rubin F,Souza P,Ferreto T;Reducing Power Consumption during Server Maintenance on Edge Computing Infrastructures;;2023;;;691–698;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing;Tallinn, Estonia;2023;9,78145E+12;;"https://doi.org/10.1145/3555776.3577739;http://dx.doi.org/10.1145/3555776.3577739";10.1145/3555776.3577739;Edge servers must routinely undergo maintenance to ensure the environment's performance and security. During maintenance, applications hosted by outdated servers must be relocated to alternative servers to avoid downtime. In distributed edges with servers spread across large regions, ensuring that applications are not migrated to servers too far away from their users to avoid high latency hardens the maintenance planning. In addition, the limited power supply of edge sites restricts the list of suitable alternative hosts for the applications even further. Past work has focused on optimizing maintenance or increasing the power efficiency of edge computing infrastructures. However, no work addresses both objectives together. This paper presents Emma, a maintenance strategy that reduces power consumption during edge server maintenance without excessively extending maintenance time or increasing application latency. Experiments show that Emma can minimize power consumption during maintenance by up to 26.48% compared to strategies from the literature.;edge computing, update, maintenance, infrastructure, power consumption;;SAC '23
Journal Article;Lewis AW,Tzeng NF,Ghosh S;Runtime Energy Consumption Estimation for Server Workloads Based on Chaotic Time-Series Approximation;ACM Trans. Archit. Code Optim.;2012;9;3;;Association for Computing Machinery;New York, NY, USA;;;;2012-10;;1544-3566;"https://doi.org/10.1145/2355585.2355588;http://dx.doi.org/10.1145/2355585.2355588";10.1145/2355585.2355588;This article proposes a runtime model that relates server energy consumption to its overall thermal envelope, using hardware performance counters and experimental measurements. While previous studies have attempted system-wide modeling of server power consumption through subsystem models, our approach is different in that it links system energy input to subsystem energy consumption based on a small set of tightly correlated parameters. The proposed model takes into account processor power, bus activities, and system ambient temperature for real-time prediction on the power consumption of long running jobs. Using the HyperTransport and QuickPath Link structures as case studies and through electrical measurements on example server subsystems, we develop a chaotic time-series approximation for runtime power consumption, arriving at the Chaotic Attractor Predictor (CAP). With polynomial time complexity, CAP exhibits high prediction accuracy, having the prediction errors within 1.6% (or 3.3%) for servers based on the HyperTransport bus (or the QuickPath Links), as verified by a set of common processor benchmarks. Our CAP is a superior predictive mechanism over existing linear auto-regressive methods, which require expensive and complex corrective steps to address the nonlinear and chaotic aspects of the underlying physical system.;chaotic time series, Analysis of variance, thermal envelope, time series approximation, QuickPath links, performance counters, HyperTransport buses, energy consumption;;
Conference Paper;Zhang S,Zhao S,Yuan M,Zeng J,Yao J,Lyu MR,King I;Traffic Prediction Based Power Saving in Cellular Networks: A Machine Learning Method;;2017;;;;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 25th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems;Redondo Beach, CA, USA;2017;9,78145E+12;;"https://doi.org/10.1145/3139958.3140053;http://dx.doi.org/10.1145/3139958.3140053";10.1145/3139958.3140053;In smart cities, green cellular networks play a crucial role to support wireless access for numerous devices anywhere and anytime with efficiency and sustainability. Because base stations (BSes) consume more than 70% of overall cellular network infrastructure energy, saving the power consumption of BSes is the key task to build a green cellular network. Except for low power design of the BS hardware and software, the traffic-driven BS sleeping operation is an economical way to improve existing cellular networks, which can reduce the BS power consumption at low traffic load. However, prior BS sleeping strategies establish on the static temporal characteristics of traffic load, which ignore the fact that network traffic is influenced by many factors such as time, human mobility, holiday, weather, etc. Hence, prior traffic estimation is coarse, and the BS sleeping strategies cannot apply to the changing network traffic. In this paper, we exploit a machine learning method to estimate the BS traffic and propose a BS sleeping strategy based on predicted traffic for power saving in the cellular network. We analyze network traffic in multi-views: temporal influence, spatial influence, and event influence. Then, we propose a multi-view ensemble learning model to predict network traffic load, which learns the traffic in multi-views and combine the results with ensemble. Furthermore, we formulate a BS sleeping strategy based on the predicted traffic load. Finally, we evaluate our traffic prediction algorithm on real cellular network data. The evaluation shows that our traffic prediction algorithm improves about 40% than state-of-the-art machine learning methods. Also, we evaluate the proposed BS sleeping strategy, which yields about 10% more energy savings and less device damage than the competitors in the simulated environment.;Smart City, Green Cellular Network, Network Traffic Prediction, Multi-view Learning, Spatio-Temporal Data Analysis;;SIGSPATIAL '17
Conference Paper;Chang J,Meza J,Ranganathan P,Shah A,Shih R,Bash C;Totally Green: Evaluating and Designing Servers for Lifecycle Environmental Impact;;2012;;;25–36;Association for Computing Machinery;New York, NY, USA;;Proceedings of the Seventeenth International Conference on Architectural Support for Programming Languages and Operating Systems;London, England, UK;2012;9,78145E+12;;"https://doi.org/10.1145/2150976.2150980;http://dx.doi.org/10.1145/2150976.2150980";10.1145/2150976.2150980;The environmental impact of servers and datacenters is an important future challenge. System architects have traditionally focused on operational energy as a proxy for designing green servers, but this ignores important environmental implications from server production (materials, manufacturing, etc.). In contrast, this paper argues for a lifecycle focus on the environmental impact of future server designs, to include both operation and production. We present a new methodology to quantify the total environmental impact of system design decisions. Our approach uses the thermodynamic metric of exergy consumption, adapted and validated for use by system architects. Using this methodology, we evaluate the lifecycle impact of several example system designs with environment-friendly optimizations. Our results show that environmental impact from production can be important (around 20% on current servers and growing) and system design choices can reduce this component (by 30--40%). Our results also highlight several, sometimes unexpected, cross-interactions between the environmental impact of production and operation that further motivate a total lifecycle emphasis for future green server designs.;dematerialization, green computing, lifecycle impact, server architecture, environmental sustainability, datacenter design, exergy, disaggregation;;ASPLOS XVII
Journal Article;Chang J,Meza J,Ranganathan P,Shah A,Shih R,Bash C;Totally Green: Evaluating and Designing Servers for Lifecycle Environmental Impact;SIGARCH Comput. Archit. News;2012;40;1;25–36;Association for Computing Machinery;New York, NY, USA;;;;2012-03;;0163-5964;"https://doi.org/10.1145/2189750.2150980;http://dx.doi.org/10.1145/2189750.2150980";10.1145/2189750.2150980;The environmental impact of servers and datacenters is an important future challenge. System architects have traditionally focused on operational energy as a proxy for designing green servers, but this ignores important environmental implications from server production (materials, manufacturing, etc.). In contrast, this paper argues for a lifecycle focus on the environmental impact of future server designs, to include both operation and production. We present a new methodology to quantify the total environmental impact of system design decisions. Our approach uses the thermodynamic metric of exergy consumption, adapted and validated for use by system architects. Using this methodology, we evaluate the lifecycle impact of several example system designs with environment-friendly optimizations. Our results show that environmental impact from production can be important (around 20% on current servers and growing) and system design choices can reduce this component (by 30--40%). Our results also highlight several, sometimes unexpected, cross-interactions between the environmental impact of production and operation that further motivate a total lifecycle emphasis for future green server designs.;exergy, dematerialization, green computing, disaggregation, lifecycle impact, environmental sustainability, datacenter design, server architecture;;
Journal Article;Chang J,Meza J,Ranganathan P,Shah A,Shih R,Bash C;Totally Green: Evaluating and Designing Servers for Lifecycle Environmental Impact;SIGPLAN Not.;2012;47;4;25–36;Association for Computing Machinery;New York, NY, USA;;;;2012-03;;0362-1340;"https://doi.org/10.1145/2248487.2150980;http://dx.doi.org/10.1145/2248487.2150980";10.1145/2248487.2150980;The environmental impact of servers and datacenters is an important future challenge. System architects have traditionally focused on operational energy as a proxy for designing green servers, but this ignores important environmental implications from server production (materials, manufacturing, etc.). In contrast, this paper argues for a lifecycle focus on the environmental impact of future server designs, to include both operation and production. We present a new methodology to quantify the total environmental impact of system design decisions. Our approach uses the thermodynamic metric of exergy consumption, adapted and validated for use by system architects. Using this methodology, we evaluate the lifecycle impact of several example system designs with environment-friendly optimizations. Our results show that environmental impact from production can be important (around 20% on current servers and growing) and system design choices can reduce this component (by 30--40%). Our results also highlight several, sometimes unexpected, cross-interactions between the environmental impact of production and operation that further motivate a total lifecycle emphasis for future green server designs.;dematerialization, green computing, datacenter design, lifecycle impact, exergy, environmental sustainability, disaggregation, server architecture;;
Journal Article;Qiu K,Jao N,Zhou K,Liu Y,Sampson J,Kandemir MT,Narayanan V;MaxTracker: Continuously Tracking the Maximum Computation Progress for Energy Harvesting ReRAM-Based CNN Accelerators;ACM Trans. Embed. Comput. Syst.;2021;20;5s;;Association for Computing Machinery;New York, NY, USA;;;;2021-09;;1539-9087;"https://doi.org/10.1145/3477009;http://dx.doi.org/10.1145/3477009";10.1145/3477009;There is an ongoing trend to increasingly offload inference tasks, such as CNNs, to edge devices in many IoT scenarios. As energy harvesting is an attractive IoT power source, recent ReRAM-based CNN accelerators have been designed for operation on harvested energy. When addressing the instability problems of harvested energy, prior optimization techniques often assume that the load is fixed, overlooking the close interactions among input power, computational load, and circuit efficiency, or adapt the dynamic load to match the just-in-time incoming power under a simple harvesting architecture with no intermediate energy storage.Targeting a more efficient harvesting architecture equipped with both energy storage and energy delivery modules, this paper is the first effort to target whole system, end-to-end efficiency for an energy harvesting ReRAM-based accelerator. First, we model the relationships among ReRAM load power, DC-DC converter efficiency, and power failure overhead. Then, a maximum computation progress tracking scheme (MaxTracker) is proposed to achieve a joint optimization of the whole system by tuning the load power of the ReRAM-based accelerator. Specifically, MaxTracker accommodates both continuous and intermittent computing schemes and provides dynamic ReRAM load according to harvesting scenarios.We evaluate MaxTracker over four input power scenarios, and the experimental results show average speedups of 38.4%/40.3% (up to 51.3%/84.4%), over a full activation scheme (with energy storage) and order-of-magnitude speedups over the recently proposed (energy storage-less) ResiRCA technique. Furthermore, we also explore MaxTracker in combination with the Capybara reconfigurable capacitor approach to offer more flexible tuners and thus further boost the system performance.;maximum computation progress, CNN, DC-DC efficiency, Energy harvesting, computing schemes, ReRAM crossbar;;
Conference Paper;OuldOuali L,Sabouret N,Rich C;I've Got the Power's Value! A Computational Model to Evaluate the Interlocutor's Behaviors in Collaborative Negotiation: Socially Interactive Agents Track;;2018;;;2245–2246;International Foundation for Autonomous Agents and Multiagent Systems;Richland, SC;;Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems;Stockholm, Sweden;2018;;;;;We present in this paper a simulation-oriented theory of mind model for interpreting behaviors of power during a collaborative negotiation. This model relies on a model of negotiation that allows an agent to express behaviors of power through its strategy of negotiation. Based on the simulation theory, we adapted the decision model of the agent to reason about its interlocutor's behavior. A preliminary evaluation in the context of agent-agent interaction shows that the system correctly predicts the interlocutor's power.;reasoning about other, theory of mind, dominance;;AAMAS '18
Conference Paper;Bui DM,Huh EN,Lee S;Optimizing Power Consumption in Cloud Computing Based on Optimization and Predictive Analysis;;2018;;;;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 12th International Conference on Ubiquitous Information Management and Communication;Langkawi, Malaysia;2018;9,78145E+12;;"https://doi.org/10.1145/3164541.3164608;http://dx.doi.org/10.1145/3164541.3164608";10.1145/3164541.3164608;Due to the budget and the environmental issues, achieving energy efficiency gradually receives a lot of attentions these days. In our previous research, a prediction technique has been developed to improve the monitoring statistics. In this research, by adopting the predictive monitoring information, our new proposal can perform the optimization to solve the energy issue of cloud computing. Actually, the optimization technique, which is convex optimization, is coupled with the proposed prediction method to produce a near-optimal set of hosting physical machines. After that, a corresponding migrating instruction can be created eventually. Based on this instruction, the cloud orchestrator can suitably relocate virtual machines to a designed subset of infrastructure. Subsequently, the idle physical servers can be turned off in an appropriate manner to save the power as well as maintain the system performance. For the purpose of evaluation, an experiment is conducted based on 29-day period of Google traces. By utilizing this evaluation, the proposed approach shows the potential to significantly reduce the power consumption without affecting the quality of services.;Energy Efficiency, Cloud Computing, Predictive Analysis, Convex Optimization, IaaS;;IMCOM '18
Conference Paper;Mair J,Huang Z,Eyers D,Chen Y;Quantifying the Energy Efficiency Challenges of Achieving Exascale Computing;;2015;;;943–950;IEEE Press;Shenzhen, China;;Proceedings of the 15th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing;;2015;9,78148E+12;;"https://doi.org/10.1109/CCGrid.2015.130;http://dx.doi.org/10.1109/CCGrid.2015.130";10.1109/CCGrid.2015.130;Power and performance are two potentially opposing objectives in the design of a supercomputer, where increases in performance often come at the cost of increased power consumption and vice versa. The task of simultaneously maximising both objectives is becoming an increasingly prominent challenge in the development of future exascale supercomputers. To gain some perspective on the scale of the challenge, we analyse the power and performance trends for the Top500 and Green500 supercomputer lists. We then present the Pa PW metric, which we use to evaluate the scalability of power efficiency, projecting the development of an exascale system. From this analysis, we found that when both power and performance are considered, the projected date of achieving an exascale system falls far beyond the current target of 2020.;;;CCGRID '15
Conference Paper;Belesaca JD,Avila-Campos P,Vazquez-Rodas A;Artificial Neural Network Performance Evaluation for a Hybrid Power Domain Orthogonal / Non-Orthogonal Multiple Access (OMA / NOMA) System;;2020;;;73–80;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 17th ACM Symposium on Performance Evaluation of Wireless Ad Hoc, Sensor, & Ubiquitous Networks;Alicante, Spain;2020;9,78145E+12;;"https://doi.org/10.1145/3416011.3424760;http://dx.doi.org/10.1145/3416011.3424760";10.1145/3416011.3424760;Next-generation wireless technologies face considerable challenges in terms of providing the required latency and connectivity for new heterogeneous mobile networks. Driven by these problems, this study focuses on increasing user connectivity together with system throughput. For doing so, we propose and evaluate a hybrid machine learning-driven orthogonal/non-orthogonal multiple access (OMA/NOMA) system. In this work, we use an artificial neural network (ANN) to assign an OMA or NOMA access method to each user equipment (UE). As part of this research we also evaluate the accuracy and training time of the three most relevant learning algorithms of ANN (L-M, BFGS, and OSS). The main objective is to increase the sum-rate of the mobile network in the introduced beamforming and mmWave channel environment.Simulation results show up to a $20%$ sum-rate average performance increase of the system using the ANN management in contrast to a random non-ANN managed system. The Leveberg-Marquard (L-M) training algorithm is the best overall algorithm for this proposed application as presents the highest accuracy of around $77%$ despite 37 minutes of training and lower accuracy of $73%$ with approximately 28 seconds of training time.;artificial neural networks ANN, MM-wave channel, BFGS, sum-rate, analog beamforming, OSS, L-M, OMA, NOMA;;PE-WASUN '20
Conference Paper;Colmant M,Kurpicz M,Felber P,Huertas L,Rouvoy R,Sobe A;BitWatts: A Process-Level Power Monitoring Middleware;;2014;;;41–42;Association for Computing Machinery;New York, NY, USA;;Proceedings of the Posters and Demos Session of the 15th International Middleware Conference;Bordeaux, France;2014;9,78145E+12;;"https://doi.org/10.1145/2678508.2678529;http://dx.doi.org/10.1145/2678508.2678529";10.1145/2678508.2678529;Power estimation of software processes provides critical indicators to drive scheduling or power capping heuristics. State-of-the-art power estimation solutions only provide coarse-grained support for power estimation. In this paper, we therefore propose a middleware for assembling and configuring software-defined power meters. Software-defined power meters provide real-time and accurate power estimation of software processes. In particular, our solution automatically learns an application-agnostic power model, which can be used to estimate the power consumption of applications.Our approach, named BitWatts, builds on a distributed actor middleware to collect process usage and infer fine-grained power consumption without imposing any hardware investment (e.g., power meters).;CPU power model, middleware toolkit, power monitoring;;Middleware Posters and Demos '14
Conference Paper;Kurihara S,Fukuda S,Hamanaka S,Oguchi M,Yamaguchi S;Application Power Consumption Estimation Considering Software Dependency in Android;;2017;;;;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 11th International Conference on Ubiquitous Information Management and Communication;Beppu, Japan;2017;9,78145E+12;;"https://doi.org/10.1145/3022227.3022312;http://dx.doi.org/10.1145/3022227.3022312";10.1145/3022227.3022312;Android operating system has become one of the most popular smartphone platforms. One report stated that the most important issue of smartphones was its power consumption. Android has a function with which an application can be invoked in screen-off state without user's operation. Some applications frequently work in screen-off state, heavily and consume battery. For saving power consumption in the state, accurate estimation of power consumption of each application is important. However, estimating power consumption cannot be easily achieved because of its dependency on device. That is, application's power consumption varies on installed application and type of hardware module in the device, which can be called software and hardware dependency, respectively. In this paper, we discuss estimation of power consumption in screen-off state considering software dependency. First, we explain software dependency of power consumption. Second, we propose a method, which takes account of software dependency, for estimating power consumption due to GPS. The proposed method monitors GPS utilization individually. Third, we evaluate our method with a benchmark and practical applications using GPS. We then demonstrate that our method can estimate power consumption of each application and suitably predict consumption after uninstalling an application without uninstallation.;battery-draining application, Android, smartphone, GPS;;IMCOM '17
Conference Paper;Chamberlain R,Hemmeter E,Morley R,White J;Modeling the Power Consumption of Audio Signal Processing Computations Using Customized Numerical Representations;;2003;;;249;IEEE Computer Society;USA;;Proceedings of the 36th Annual Symposium on Simulation;;2003;9,78077E+12;;;;This paper explores the impact that numericalrepresentation has on the power consumption of audiosignal processing applications. The motivation is digitalhearing aids, for which minimizing the powerconsumption is a critical design goal. We investigate twoaspects of this problem. First, we evaluate the validity ofusing signal transition counts to model actual powerconsumption within this problem domain, and second, wecompare the relative power consumption of multiply-accumulateoperations for several customized numericalrepresentations.;audio signal processing, power consumption, numerical representation;;ANSS '03
Conference Paper;McGough AS,Forshaw M;Evaluation of Energy Consumption of Replicated Tasks in a Volunteer Computing Environment;;2018;;;85–90;Association for Computing Machinery;New York, NY, USA;;Companion of the 2018 ACM/SPEC International Conference on Performance Engineering;Berlin, Germany;2018;9,78145E+12;;"https://doi.org/10.1145/3185768.3186313;http://dx.doi.org/10.1145/3185768.3186313";10.1145/3185768.3186313;High Throughput Computing allows workloads of many thousands of tasks to be performed efficiently over many distributed resources and frees the user from the laborious process of managing task deployment, execution and result collection. However, in many cases the High Throughput Computing system is comprised from volunteer computational resources where tasks may be evicted by the owner of the resource. This has two main disadvantages. First, tasks may take longer to run as they may require multiple deployments before finally obtaining enough time on a resource to complete. Second, the wasted computation time will lead to wasted energy. We may be able to reduce the effect of the first disadvantage here by submitting multiple replicas of the task and take the results from the first one to complete. This, though, could lead to a significant increase in energy consumption. Thus we desire to only ever submit the minimum number of replicas required to run the task in the allocated time whilst simultaneously minimising energy. In this work we evaluate the use of fixed replica counts and Reinforcement Learning on the proportion of task which fail to finish in a given time-frame and the energy consumed by the system.;trace-driven, machine learning, energy, simulation;;ICPE '18
Conference Paper;Somavat P,Jadhav S,Namboodiri V;Accounting for the Energy Consumption of Personal Computing Including Portable Devices;;2010;;;141–149;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 1st International Conference on Energy-Efficient Computing and Networking;Passau, Germany;2010;9,78145E+12;;"https://doi.org/10.1145/1791314.1791337;http://dx.doi.org/10.1145/1791314.1791337";10.1145/1791314.1791337;In light of the increased awareness of global energy consumption, questions are also being asked about the contribution of computing equipment. Though studies have documented the share of energy consumption due to these equipment over the years, these have rarely characterized the increasing share contributed by the rapidly growing portable computing segment. The portable computing device segment is widely predicted to be a dominant mode of computing and communication, and accounting for its energy consumption is necessary for energy-efficient computing in the future. This work takes a fresh and updated look at the energy consumption due to computing devices in perspective of global consumption, and pays special attention to the contribution of portable computing devices. We further quantify the impact of energy consumed by the computing sector on the environment, as well as on the electricity cost to an average residential consumer. Finally, based on the results of the study, recommendations targeted at the computer networking community are made.;electricity, environment, portable devices, networking, computing, energy;;e-Energy '10
Journal Article;Miyoshi T,Sugino N;Fine-Grain Compensation Method with Consideration of Trade-Offs between Computation and Data Transfer for Power Consumption;SIGARCH Comput. Archit. News;2007;35;5;39–44;Association for Computing Machinery;New York, NY, USA;;;;2007-12;;0163-5964;"https://doi.org/10.1145/1360464.1360475;http://dx.doi.org/10.1145/1360464.1360475";10.1145/1360464.1360475;Fine-grain parallelizing method with consideration of the number of data transfers for low power consumption is proposed. In the proposed method, power consumption by data transfers between processor elements in a multiprocessor is focused on, and the number of data transfers is reduced.In this paper, a measure based on the relationship between variables in a given program is defined to evaluate the number of data transfers, firstly. And then a proposed compensation method by use of the evaluation of power consumption based on the measure is explained. Finally, the result of applying proposed compensation method implemented on COINS framework to several example programs is shown.;;;
Conference Paper;v. Kistowski J,Block H,Beckett J,Lange KD,Arnold JA,Kounev S;Analysis of the Influences on Server Power Consumption and Energy Efficiency for CPU-Intensive Workloads;;2015;;;223–234;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 6th ACM/SPEC International Conference on Performance Engineering;Austin, Texas, USA;2015;9,78145E+12;;"https://doi.org/10.1145/2668930.2688057;http://dx.doi.org/10.1145/2668930.2688057";10.1145/2668930.2688057;Energy efficiency of servers has become a significant research topic over the last years, as server energy consumption varies depending on multiple factors, such as server utilization and workload type. Server energy analysis and estimation must take all relevant factors into account to ensure reliable estimates and conclusions. Thorough system analysis requires benchmarks capable of testing different system resources at different load levels using multiple workload types. Server energy estimation approaches, on the other hand, require knowledge about the interactions of these factors for the creation of accurate power models. Common approaches to energy-aware workload classification categorize workloads depending on the resource types used by the different workloads. However, they rarely take into account differences in workloads targeting the same resources. Industrial energy-efficiency benchmarks typically do not evaluate the system's energy consumption at different resource load levels, and they only provide data for system analysis at maximum system load.In this paper, we benchmark multiple server configurations using the CPU worklets included in SPEC's Server Efficiency Rating Tool (SERT). We evaluate the impact of load levels and different CPU workloads on power consumption and energy efficiency. We analyze how functions approximating the measured power consumption differ over multiple server configurations and architectures. We show that workloads targeting the same resource can differ significantly in their power draw and energy efficiency. The power consumption of a given workload type varies depending on utilization, hardware and software configuration. The power consumption of CPU-intensive workloads does not scale uniformly with increased load, nor do hardware or software configuration changes affect it in a uniform manner.;power, sert, workload characterization, energy efficiency, metrics, spec, utilization;;ICPE '15
Conference Paper;Arjona Aroca J,Chatzipapas A,Fernández Anta A,Mancuso V;A Measurement-Based Analysis of the Energy Consumption of Data Center Servers;;2014;;;63–74;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 5th International Conference on Future Energy Systems;Cambridge, United Kingdom;2014;9,78145E+12;;"https://doi.org/10.1145/2602044.2602061;http://dx.doi.org/10.1145/2602044.2602061";10.1145/2602044.2602061;Energy consumption is a growing issue in data centers, impacting their economic viability and their public image. In this work we empirically characterize the power and energy consumed by different types of servers. In particular, in order to understand the behavior of their energy and power consumption, we perform measurements in different servers. In each of them, we exhaustively measure the power consumed by the CPU, the disk, and the network interface under different configurations, identifying the optimal operational levels. One interesting conclusion of our study is that the curve that defines the minimal CPU power as a function of the load is neither linear nor purely convex as has been previously assumed. Moreover, we find that the efficiency of the various server components can be maximized by tuning the CPU frequency and the number of active cores as a function of the system and network load, while the block size of I/O operations should be always maximized by applications. We also show how to estimate the energy consumed by an application as a function of some simple parameters, like the CPU load, and the disk and network activity. We validate the proposed approach by accurately estimating the energy of a map-reduce computation in a Hadoop platform.;power and energy consumption, network, disk I/O, measurements, DVFs, CPU;;e-Energy '14
Conference Paper;Shoaib M,Jha N,Verma N;A Low-Energy Computation Platform for Data-Driven Biomedical Monitoring Algorithms;;2011;;;591–596;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 48th Design Automation Conference;San Diego, California;2011;9,78145E+12;;"https://doi.org/10.1145/2024724.2024861;http://dx.doi.org/10.1145/2024724.2024861";10.1145/2024724.2024861;A key challenge in closed-loop chronic biomedical systems is the ability to detect complex physiological states from patient signals within a constrained power budget. Data-driven machine-learning techniques are major enablers for the modeling and interpretation of such states. Their computational energy, however, scales with the complexity of the required models. In this paper, we propose a low-energy, biomedical computation platform optimized through the use of an accelerator for data-driven classification. The accelerator retains selective flexibility through hardware reconfiguration and exploits voltage scaling and parallelism to operate at a sub-threshold minimum-energy point. Using cardiac arrhythmia detection algorithms with patient data from the MIT-BIH database, classification is achieved in 2.96 ?J (at Vdd = 0.4 V), over four orders of magnitude smaller than that on a low-power general-purpose processor. The energy of feature extraction is 148 ?J while retaining flexibility for a range of possible biomarkers.;electrocardiograph (ECG), support vector machine (SVM);;DAC '11
Journal Article;Ferroni M,Corna A,Damiani A,Brondolin R,Colmenares JA,Hofmeyr S,Kubiatowicz JD,Santambrogio MD;Power Consumption Models for Multi-Tenant Server Infrastructures;ACM Trans. Archit. Code Optim.;2017;14;4;;Association for Computing Machinery;New York, NY, USA;;;;2017-11;;1544-3566;"https://doi.org/10.1145/3148965;http://dx.doi.org/10.1145/3148965";10.1145/3148965;Multi-tenant virtualized infrastructures allow cloud providers to minimize costs through workload consolidation. One of the largest costs is power consumption, which is challenging to understand in heterogeneous environments. We propose a power modeling methodology that tackles this complexity using a divide-and-conquer approach. Our results outperform previous research work, achieving a relative error of 2% on average and under 4% in almost all cases. Models are portable across similar architectures, enabling predictions of power consumption before migrating a tenant to a different hardware platform. Moreover, we show the models allow us to evaluate colocations of tenants to reduce overall consumption.;Virtualization, power consumption models, multi-tenant cloud infrastructures;;
Conference Paper;Bartolini A,Sadri M,Furst JN,Coskun AK,Benini L;Quantifying the Impact of Frequency Scaling on the Energy Efficiency of the Single-Chip Cloud Computer;;2012;;;181–186;EDA Consortium;San Jose, CA, USA;;Proceedings of the Conference on Design, Automation and Test in Europe;Dresden, Germany;2012;9,78398E+12;;;;Dynamic frequency and voltage scaling (DVFS) techniques have been widely used for meeting energy constraints. Single-chip many-core systems bring new challenges owing to the large number of operating points and the shift to message passing interface (MPI) from shared memory communication. DVFS, however, has been mostly studied on single-chip systems with one or few cores, without considering the impact of the communication among cores. This paper evaluates the impact of frequency scaling on the performance and power of many-core systems with MPI. We conduct experiments on the Single-Chip Cloud Computer (SCC), an experimental many-core processor developed by Intel. The paper first introduces the run-time monitoring infrastructure and the application suite we have designed for an in-depth evaluation of the SCC. We provide an extensive analysis quantifying the effects of frequency perturbations on performance and energy efficiency. Experimental results show that run-time communication patterns lead to significant differences in power/performance tradeoffs in many-core systems with MPI.;;;DATE '12
Conference Paper;Kim Y,Kim K;Accelerated Computation and Tracking of AC Optimal Power Flow Solutions Using GPUs;;2023;;;;Association for Computing Machinery;New York, NY, USA;;Workshop Proceedings of the 51st International Conference on Parallel Processing;Bordeaux, France;2023;9,78145E+12;;"https://doi.org/10.1145/3547276.3548631;http://dx.doi.org/10.1145/3547276.3548631";10.1145/3547276.3548631;We present a scalable solution method based on an alternating direction method of multipliers and graphics processing units (GPUs) for rapidly computing and tracking a solution of alternating current optimal power flow (ACOPF) problem. Such a fast computation is particularly useful for mitigating the negative impact of frequent load and generation fluctuations on the optimal operation of a large electrical grid. To this end, we decompose a given ACOPF problem by grid components, resulting in a large number of small independent nonlinear nonconvex optimization subproblems. The computation time of these subproblems is significantly accelerated by employing the massive parallel computing capability of GPUs. In addition, the warm-start ability of our method leads to faster convergence, making the method particularly suitable for fast tracking of optimal solutions. We demonstrate the performance of our method on a 70,000 bus system by solving associated optimal power flow problems with both cold start and warm start.;alternating direction method of multipliers, alternating current optimal power flow, graphics processing unit;;ICPP Workshops '22
Conference Paper;Sehgal P,Tarasov V,Zadok E;Evaluating Performance and Energy in File System Server Workloads;;2010;;;19;USENIX Association;USA;;Proceedings of the 8th USENIX Conference on File and Storage Technologies;San Jose, California;2010;;;;;Recently, power has emerged as a critical factor in designing components of storage systems, especially for power-hungry data centers. While there is some research into power-aware storage stack components, there are no systematic studies evaluating each component's impact separately. This paper evaluates the file system's impact on energy consumption and performance. We studied several popular Linux file systems, with various mount and format options, using the FileBench workload generator to emulate four server workloads: Web, database, mail, and file server. In case of a server node consisting of a single disk, CPU power generally exceeds disk-power consumption. However, file system design, implementation, and available features have a significant effect on CPU/disk utilization, and hence on performance and power. We discovered that default file system options are often suboptimal, and even poor. We show that a carefulmatching of expectedworkloads to file system types and options can improve power-performance efficiency by a factor ranging from 1.05 to 9.4 times.;;;FAST'10
Conference Paper;Azmy NM,El-Maddah IA,Mohamed HK;Adaptive Power Panel of Cloud Computing Controlling Cloud Power Consumption;;2016;;;9–14;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 2nd Africa and Middle East Conference on Software Engineering;Cairo, Egypt;2016;9,78145E+12;;"https://doi.org/10.1145/2944165.2944167;http://dx.doi.org/10.1145/2944165.2944167";10.1145/2944165.2944167;Cloud computing had created a new era of network design, where end-users can get their required services without having to purchase expensive infrastructure or even to care about troubleshooting. Power consumption is a challenge facing the Cloud Providers to operate their Datacenters. One solution to overcome this is the Virtual Machine (VM) migration, which is a technique used to switch under-utilized hosts to sleep mode in order to save power, and to avoid over-utilized hosts from Service Level Agreement (SLA) violation. But still the problem is that the Cloud Service Provider apply a single policy on all nodes. Our proposed solution is an adaptive power panel where different policies can be applied based on both of the nature of the tasks running on hosts, and the Cloud Provider decision.;Virtual Machine, Selection, Cloudsim, Green Computing, Adaptive, Placement, Migration, Allocation;;AMECSE '16
Conference Paper;Gupta P,Talukder Z,Islam MA,Nguyen P;Towards Server-Level Power Monitoring in Data Centers Using Single-Point Voltage Measurement;;2023;;;855–856;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 20th ACM Conference on Embedded Networked Sensor Systems;Boston, Massachusetts;2023;9,78145E+12;;"https://doi.org/10.1145/3560905.3568079;http://dx.doi.org/10.1145/3560905.3568079";10.1145/3560905.3568079;Server-level power monitoring in data centers can significantly contribute to its efficient management. Nevertheless, due to the cost of a dedicated power meter for each server, most data center power management only focuses on UPS or cluster-level power monitoring. In this paper, we propose a low-cost novel power monitoring approach that uses only one sensor to extract power consumption information of all servers. We utilize the conducted electromagnetic interference of server power supplies to measure its power consumption from non-intrusive single-point voltage measurement. Using a pair of commercial grade Dell PowerEdge servers, we demonstrate that our approach can estimate each server's power consumption with 3% mean absolute percentage error.;;;SenSys '22
Conference Paper;Singh J,Chen J;Optimizing Energy Consumption for Cloud Computing: A Cluster and Migration Based Approach (CMBA);;2020;;;28–32;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 2019 3rd International Conference on Computer Science and Artificial Intelligence;Normal, IL, USA;2020;9,78145E+12;;"https://doi.org/10.1145/3374587.3374594;http://dx.doi.org/10.1145/3374587.3374594";10.1145/3374587.3374594;The increased use of IT technologies and number of IT users have triggered cloud computing resource demand including the need for more data centers. Each data center consumes electricity for its un-interrupted operations and maintenance, therefore responsible for the emissions of carbon dioxide, a potent greenhouse gas causing climate change. Hence, there is a necessity to provide a solution through which energy consumption for cloud data centers can be reduced. As virtual machine located in data center are run under loaded to maintain higher performance but it causes wastage of resources and power. While, task overloading severally reduce the performance of data center. To address this issue, we propose CMBA (Cluster and Migration Based Approach) for cloud resource allocation that maps groups of tasks to customized virtual machine types based on processing, memory and network requirements. Proper placement of workload with specific VMs and dynamic migration concept reduce energy consumption for running physical machine and its respective host or data centers. Taking altogether, intelligent customization of virtual machines by adopting CMBA approach will maintain high efficiency of datacenters with reduced energy consumption.;virtual machine migration, Cloud computing, energy consumption, virtualization;;CSAI '19
Conference Paper;Hsu CH,Poole SW;Measuring Server Energy Proportionality;;2015;;;235–240;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 6th ACM/SPEC International Conference on Performance Engineering;Austin, Texas, USA;2015;9,78145E+12;;"https://doi.org/10.1145/2668930.2688049;http://dx.doi.org/10.1145/2668930.2688049";10.1145/2668930.2688049;In performance engineering, metrics are often used to track the progress over time. Concerning the potential bias of using a single metric, performance engineers tend to use multiple metrics for reasoning. However, this approach has its own challenges. In this work we study one of the challenges in the context of analyzing trends in server energy proportionality. We examine a wide range of metrics for measuring energy proportionality, trying to determine which metrics are essential and which are redundant. We do this by comparing the trend curves of the metrics for the published results of the SPECpower_ssj2008 benchmark. While the context is specific, the proposed analysis method is quite general. We hope that this method would help us do performance engineering more effectively.;metrics, energy proportionality, specpower;;ICPE '15
Conference Paper;Atiewi S,Abuhussein A,Saleh MA;Impact of Virtualization on Cloud Computing Energy Consumption: Empirical Study;;2018;;;;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 2nd International Symposium on Computer Science and Intelligent Control;Stockholm, Sweden;2018;9,78145E+12;;"https://doi.org/10.1145/3284557.3284738;http://dx.doi.org/10.1145/3284557.3284738";10.1145/3284557.3284738;Global warming, which is currently one of the greatest environmental challenges, is caused by carbon emissions. A report from the Energy Information Administration indicates that approximately 98% of CO2 emissions can be attributed to energy consumption. The trade-off between efficient and ecologically sound operation represents a major challenge faced by many organizations at present. In addition, numerous companies are currently compelled to pay a carbon tax for the resources they use and the environmental impact of their products and services. Therefore, an energy consumption system can generate actual financial payback. Green information technology involves various approaches, including power management, recycling, telecommunications, and virtualization. This paper focuses on comparing and evaluating techniques used for reducing energy consumption in virtualized environments. We first highlight the impact of virtualization techniques on minimizing energy consumption in cloud computing. Then we present an experimental comparative study between two common energy-efficient task scheduling algorithms in cloud computing (i.e., the green scheduler, the power saver scheduler). These algorithms are discussed briefly and analyzed. The three metrics used to evaluate the task scheduling algorithms are (1) total power consumption, (2) data center load, and (3) virtual machine load. This work aims to gauge and subsequently improve energy consumption efficiency in virtualized environments.;Green cloud, Simulation, Energy, Cloud computing, Green computing, Virtualization, Cloud cconomics;;ISCSIC '18
Journal Article;Hwang CH,Wu AC;A Predictive System Shutdown Method for Energy Saving of Event-Driven Computation;ACM Trans. Des. Autom. Electron. Syst.;2000;5;2;226–241;Association for Computing Machinery;New York, NY, USA;;;;2000-04;;1084-4309;"https://doi.org/10.1145/335043.335046;http://dx.doi.org/10.1145/335043.335046";10.1145/335043.335046;This paper presents a system-level power management technique for energy savings of event-driven application. We present a new predictive system-shutdown method to exploit sleep mode operations for energy saving. We use an exponential-average approach to predict the upcoming idle period. We introduce two mechanisms, prediction-miss correction and prewake-up, to improve the hit ratio and to reduce the delay overhead. Experiments on four different event-driven applications show that our proposed method achieves high hit ratios in a wide range of delay overheads, which results in a high degree of energy with low delay penaties.;sleep mode, power management, system shutdown, predictive, event-drive;;
Conference Paper;Felter W,Rajamani K,Keller T,Rusu C;A Performance-Conserving Approach for Reducing Peak Power Consumption in Server Systems;;2005;;;293–302;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 19th Annual International Conference on Supercomputing;Cambridge, Massachusetts;2005;9,7816E+12;;"https://doi.org/10.1145/1088149.1088188;http://dx.doi.org/10.1145/1088149.1088188";10.1145/1088149.1088188;The combination of increasing component power consumption, a desire for denser systems, and the required performance growth in the face of technology-scaling issues are posing enormous challenges for powering and cooling of server systems. The challenges are directly linked to the peak power consumption of servers.Our solution, Power Shifting, reduces the peak power consumption of servers minimizing the impact on performance. We reduce peak power consumption by using workload-guided dynamic allocation of power among components incorporating real-time performance feedback, activity-related power estimation techniques, and performance-sensitive activity-regulation mechanisms to enforce power budgets.We apply our techniques to a computer system with a single processor and memory. Power shifting adds a system power manager with a dynamic, global view of the system's power consumption to continuously re-budget the available power amongst the two components. Our contributions include:• Demonstration of the greater effectiveness of dynamic power allocation over static budgeting,• Evaluation of different power shifting policies,• Analysis of system and workload factors critical to successful power shifting, and• Proposal of performance-sensitive power budget enforcement mechanisms that ensure system reliability.;power management, power modeling, processor simulation;;ICS '05
Conference Paper;Hwang CH,Wu AC;A Predictive System Shutdown Method for Energy Saving of Event-Driven Computation;;1997;;;28–32;IEEE Computer Society;USA;;Proceedings of the 1997 IEEE/ACM International Conference on Computer-Aided Design;San Jose, California, USA;1997;9,78082E+12;;;;We present a system-level power management technique for power saving of event-driven applications. We present a new predictive system shutdown method to exploit sleep mode operations for power saving. We use an exponential-average approach to predict the upcoming idle period. We introduce two mechanisms, prediction-miss correction and pre-wakeup, to improve the hit ratio and to reduce the delay overhead. Experiments on four different event-driven applications show that our proposed method achieves high hit ratios in a wide range of delay overheads, which results in a high degree of power saving with low delay penalties.;finite state machine, pre-wakeup, VLSI, logic CAD, low delay penalties, exponential-average approach, predictive system shutdown method, idle period, energy saving, sleep mode operations, system-level power management, event-driven computation, hit ratio, prediction-miss correction, VLSI circuit design, delay overhead, power saving;;ICCAD '97
Conference Paper;Ardito L,Torchiano M;Creating and Evaluating a Software Power Model for Linux Single Board Computers;;2018;;;1–8;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 6th International Workshop on Green and Sustainable Software;Gothenburg, Sweden;2018;9,78145E+12;;"https://doi.org/10.1145/3194078.3194079;http://dx.doi.org/10.1145/3194078.3194079";10.1145/3194078.3194079;The number of Single Board Computers (SBCs) is increasing, and so is the cumulative energy consumed by this category of device. Moreover, such devices are often always-on or running on batteries. Therefore, it is worth investigating their energy consumption to provide software developers and users with indicators for understanding how much energy the device is consuming while running a software application. In this paper, we explain a procedure for the creation of an energy consumption model of SBCs based on the usage of its components. We apply the procedure on a Raspberry PI 2 model B to test the model with a set of real applications. The results demonstrate the practical feasibility of the approach and show that estimated consumption values on our device have an average error of 2.2%, which is a good approximation without using external and expensive measuring devices.;software engineering, computer engineering, software energy consumption, energy consumption data, energy consumption;;GREENS '18
Journal Article;Ahmed S,Nawaz M,Bakar A,Bhatti NA,Alizai MH,Siddiqui JH,Mottola L;Demystifying Energy Consumption Dynamics in Transiently Powered Computers;ACM Trans. Embed. Comput. Syst.;2020;19;6;;Association for Computing Machinery;New York, NY, USA;;;;2020-09;;1539-9087;"https://doi.org/10.1145/3391893;http://dx.doi.org/10.1145/3391893";10.1145/3391893;Transiently powered computers (TPCs) form the foundation of the battery-less Internet of Things, using energy harvesting and small capacitors to power their operation. This kind of power supply is characterized by extreme variations in supply voltage, as capacitors charge when harvesting energy and discharge when computing. We experimentally find that these variations cause marked fluctuations in clock speed and power consumption. Such a deceptively minor observation is overlooked in existing literature. Systems are thus designed and parameterized in overly conservative ways, missing on a number of optimizations.We rather demonstrate that it is possible to accurately model and concretely capitalize on these fluctuations. We derive an energy model as a function of supply voltage and prove its use in two settings. First, we develop EPIC, a compile-time energy analysis tool. We use it to substitute for the constant power assumption in existing analysis techniques, giving programmers accurate information on worst-case energy consumption of programs. When using EPIC with existing TPC system support, run-time energy efficiency drastically improves, eventually leading up to a 350% speedup in the time to complete a fixed workload. Further, when using EPIC with existing debugging tools, it avoids unnecessary program changes that hurt energy efficiency. Next, we extend the MSPsim emulator and explore its use in parameterizing a different TPC system support. The improvements in energy efficiency yield up to more than 1000% time speedup to complete a fixed workload.;Transiently powered computers, energy modelling, intermittent computing;;
Conference Paper;Guamán D,Pérez J,Valdiviezo-Diaz P,Canas N;Estimating the Energy Consumption of Software Components from Size, Complexity and Code Smells Metrics;;2022;;;1456–1459;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 37th ACM/SIGAPP Symposium on Applied Computing;Virtual Event;2022;9,78145E+12;;"https://doi.org/10.1145/3477314.3507353;http://dx.doi.org/10.1145/3477314.3507353";10.1145/3477314.3507353;Software quality may influence energy consumption. This work presents the CCsEM green model based on Size, Complexity and Code Smells to estimate energy consumption of architecture software components without being executed. This paper details the definition of this multiple linear regression model constructed from 42 applications, and its results.;software architectures, energy consumption estimation, green software;;SAC '22
Conference Paper;Chen F,Schneider JG,Yang Y,Grundy J,He Q;An Energy Consumption Model and Analysis Tool for Cloud Computing Environments;;2012;;;45–50;IEEE Press;Zurich, Switzerland;;Proceedings of the First International Workshop on Green and Sustainable Software;;2012;9,78147E+12;;;;Cloud computing delivers computing as a utility to users worldwide. A consequence of this model is that cloud data centres have high deployment and operational costs, as well as significant carbon footprints for the environment. We need to develop Green Cloud Computing (GCC) solutions that reduce these deployment and operational costs and thus save energy and reduce adverse environmental impacts. In order to achieve this objective, a thorough understanding of the energy consumption patterns in complex Cloud environments is needed. We present a new energy consumption model and associated analysis tool for Cloud computing environments. We measure energy consumption in Cloud environments based on different runtime tasks. Empirical analysis of the correlation of energy consumption and Cloud data and computational tasks, as well as system performance, will be investigated based on our energy consumption model and analysis tool. Our research results can be integrated into Cloud systems to monitor energy consumption and support static or dynamic system-level optimisation.;green computing, performance analysis, cloud computing, energy consumption;;GREENS '12
Conference Paper;Forshaw M,Thomas N,McGough AS;Trace-Driven Simulation for Energy Consumption in High Throughput Computing Systems;;2014;;;27–34;IEEE Computer Society;USA;;Proceedings of the 2014 IEEE/ACM 18th International Symposium on Distributed Simulation and Real Time Applications;;2014;9,78148E+12;;"https://doi.org/10.1109/DS-RT.2014.12;http://dx.doi.org/10.1109/DS-RT.2014.12";10.1109/DS-RT.2014.12;High Throughput Computing (HTC) is a powerful paradigm allowing vast quantities of independent work to be performed simultaneously. However, until recently little evaluation has been performed on the energy impact of HTC. Many organisations now seek to minimise energy consumption across their IT infrastructure though it is unclear how this will affect the usability of HTC systems. We present here HTC-Sim, a simulation system which allows the evaluation of different energy reduction policies across an HTC system comprising a collection of computational resources dedicated to HTC work and resources provided through cycle scavenging--a Desktop Grid. We demonstrate that our simulation software scales linearly with increasing HTC workload.;;;DS-RT '14
Conference Paper;Chen F,Grundy J,Yang Y,Schneider JG,He Q;Experimental Analysis of Task-Based Energy Consumption in Cloud Computing Systems;;2013;;;295–306;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 4th ACM/SPEC International Conference on Performance Engineering;Prague, Czech Republic;2013;9,78145E+12;;"https://doi.org/10.1145/2479871.2479911;http://dx.doi.org/10.1145/2479871.2479911";10.1145/2479871.2479911;Cloud computing delivers IT solutions as a utility to users. One consequence of this model is that large cloud data centres consume large amounts of energy and produce significant carbon footprints. A common objective of cloud providers is to develop resource provisioning and management solutions that minimise energy consumption while guaranteeing Service Level Agreements (SLAs). In order to achieve this objective, a thorough understanding of energy consumption patterns in complex cloud systems is imperative. We have developed an energy consumption model for cloud computing systems. To operationalise this model, we have conducted extensive experiments to profile the energy consumption in cloud computing systems based on three types of tasks: computation-intensive, data-intensive and communication-intensive tasks. We collected fine-grained energy consumption and performance data with varying system configurations and workloads. Our experimental results show the correlation coefficients of energy consumption, system configuration and workload, as well as system performance in cloud systems. These results can be used for designing energy consumption monitors, and static or dynamic system-level energy consumption optimisation strategies for green cloud computing systems.;energy efficiency, cloud computing, green cloud, performance analysis, energy consumption;;ICPE '13
Conference Paper;Bheda H,Thaker C,Shah S;An Optimized VM Placement Approach to Reduce Energy Consumption in Green Cloud Computing;;2022;;;130–135;Association for Computing Machinery;New York, NY, USA;;Proceedings of the International Conference on Data Science, Machine Learning and Artificial Intelligence;Windhoek, Namibia;2022;9,78145E+12;;"https://doi.org/10.1145/3484824.3484894;http://dx.doi.org/10.1145/3484824.3484894";10.1145/3484824.3484894;As a global digitization advancement, there is a massive need of cloud-based solutions and data centers. Another reason behind excessive need of data centers is because of increasing number of internet users. Increasing demand of data centers simultaneously need huge amount of energy for data center operation and on other end emit enormous amount of CO2. Several approaches have been proposed to reduce energy consumption, but major concern is by looking at one parameter or criteria they must compromise on other. Our proposed approach MIPS-Aware VM Placement in combination with searching of best capable host helps to reduce VM migration and increase mean time for better performance and save energy. Proposed approach identifies overloaded and underloaded hosts and to improve system performance algorithm does not allow to allocate additional workload, which will also help to reduce energy and get better QoS. Proposed approach significantly decreases VM migration and increase mean time before VM migration which in turns helps to reduce energy and associated cost. By using proposed MIPS-Aware VM Placement approach, we can reduce upto 25% more energy consumption compared to traditional approaches.;Energy efficiency, MIPS-Aware VM placement, Green cloud computing, Performance improvement;;DSMLAI '21'
Conference Paper;Rakhmatov D,Vrudhula S,Wallach DA;Battery Lifetime Prediction for Energy-Aware Computing;;2002;;;154–159;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 2002 International Symposium on Low Power Electronics and Design;Monterey, California, USA;2002;9,78158E+12;;"https://doi.org/10.1145/566408.566449;http://dx.doi.org/10.1145/566408.566449";10.1145/566408.566449;Predicting the time of full discharge of a finite-capacity energy source, such as a battery, is important for the design of portable electronic systems and applications. In this paper we present a novel analytical model of a battery that not only can be used to predict battery lifetime, but also can serve as a cost function for optimization of the energy usage in battery-powered systems. The model is physically justified, and involves only two parameters, which are easily estimated. The paper includes the results of extensive experimental evaluation of the model with respect to numerical simulations of the electrochemical cell, as well as measurements taken on a real battery. The model was tested using constant, interrupted, periodic and non-periodic discharge profiles, which were derived from standard applications run on a pocket computer.;low-power design, modeling, battery;;ISLPED '02
Conference Paper;Maheshwari K,Birman K,Wozniak JM,Van Zandt D;Evaluating Cloud Computing Techniques for Smart Power Grid Design Using Parallel Scripting;;2013;;;319–326;IEEE Press;Delft, Netherlands;;Proceedings of the 13th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing;;2013;;;"https://doi.org/10.1109/CCGrid.2013.26;http://dx.doi.org/10.1109/CCGrid.2013.26";10.1109/CCGrid.2013.26;"Applications used to evaluate next-generation electrical power grids (""smart grids"") are anticipated to be compute and data-intensive. In this work, we parallelize and improve performance of one such application which was run sequentially prior to the use of our cloud-based configuration. We examine multiple cloud computing offerings, both commercial and academic, to evaluate their potential for improving the turnaround time for application results. Since the target application does not fit well into existing computational paradigms for the cloud, we employ parallel scripting tool, as a first step toward a broader program of adapting portable, scalable computational tools for use as enablers of the future smart grids. We use multiple clouds as a way to reassure potential users that the risk of cloud-vendor lock-in can be managed. This paper discusses our methods and results. Our experience sheds light on some of the issues facing computational scientists and engineers tasked with adapting new paradigms and infrastructures for existing engineering design problems.";smart grid, cloud computing, parallel scripting;;CCGRID '13
Conference Paper;Harton TW,Walker C,O'Sullivan M;Towards Power Consumption Modeling for Servers at Scale;;2015;;;315–321;IEEE Press;Limassol, Cyprus;;Proceedings of the 8th International Conference on Utility and Cloud Computing;;2015;9,78077E+12;;;;As of 2010 data centers use 1.5% of global electricity production and this is expected to keep growing [1]. There is a need for a near real-time power consumption modeling/monitoring system that could be used at scale within a Software Defined Data Center (SDDC). The power consumption models and information they provide can then be used to make better decisions for data center orchestration, e.g., whether to migrate virtual machines to reduce power consumption. We propose a scalable system that would 1) create initial power consumption models, as needed, for data center components, and 2) could be continually refined while the components are in use. The models will be used for the near real-time monitoring of power consumption, as well as predicting power consumption before and after potential orchestration decisions. The first step towards this goal of whole data center power modeling and prediction is to be able to predict the power consumption of one server effectively, based on high level utilization statistics from that server. In this paper we present a novel method for modeling whole system power consumption for a server, under varying random levels of CPU utilization, with a scalable random forest based model, that utilizes statistics available at the data center management level.;;;UCC '15
Conference Paper;Tudor BM,Teo YM;On Understanding the Energy Consumption of ARM-Based Multicore Servers;;2013;;;267–278;Association for Computing Machinery;New York, NY, USA;;Proceedings of the ACM SIGMETRICS/International Conference on Measurement and Modeling of Computer Systems;Pittsburgh, PA, USA;2013;9,78145E+12;;"https://doi.org/10.1145/2465529.2465553;http://dx.doi.org/10.1145/2465529.2465553";10.1145/2465529.2465553;There is growing interest to replace traditional servers with low-power multicore systems such as ARM Cortex-A9. However, such systems are typically provisioned for mobile applications that have lower memory and I/O requirements than server application. Thus, the impact and extent of the imbalance between application and system resources in exploiting energy efficient execution of server workloads is unclear. This paper proposes a trace-driven analytical model for understanding the energy performance of server workloads on ARM Cortex-A9 multicore systems. Key to our approach is the modeling of the degrees of CPU core, memory and I/O resource overlap, and in estimating the number of cores and clock frequency that optimizes energy performance without compromising execution time. Since energy usage is the product of utilized power and execution time, the model first estimates the execution time of a program. CPU time, which accounts for both cores and memory response time, is modeled as an M/G/1 queuing system. Workload characterization of high performance computing, web hosting and financial computing applications shows that bursty memory traffic fits a Pareto distribution, and non-bursty memory traffic is exponentially distributed. Our analysis using these server workloads reveals that not all server workloads might benefit from higher number of cores or clock frequencies. Applying our model, we predict the configurations that increase energy efficiency by 10% without turning off cores, and up to one third with shutting down unutilized cores. For memory-bounded programs, we show that the limited memory bandwidth might increase both execution time and energy usage, to the point where energy cost might be higher than on a typical x64 multicore system. Lastly, we show that increasing memory and I/O bandwidth can improve both the execution time and the energy usage of server workloads on ARM Cortex-A9 systems.;energy, analytical model, servers, low-power, performance, multicore;;SIGMETRICS '13
Journal Article;Tudor BM,Teo YM;On Understanding the Energy Consumption of ARM-Based Multicore Servers;SIGMETRICS Perform. Eval. Rev.;2013;41;1;267–278;Association for Computing Machinery;New York, NY, USA;;;;2013-06;;0163-5999;"https://doi.org/10.1145/2494232.2465553;http://dx.doi.org/10.1145/2494232.2465553";10.1145/2494232.2465553;There is growing interest to replace traditional servers with low-power multicore systems such as ARM Cortex-A9. However, such systems are typically provisioned for mobile applications that have lower memory and I/O requirements than server application. Thus, the impact and extent of the imbalance between application and system resources in exploiting energy efficient execution of server workloads is unclear. This paper proposes a trace-driven analytical model for understanding the energy performance of server workloads on ARM Cortex-A9 multicore systems. Key to our approach is the modeling of the degrees of CPU core, memory and I/O resource overlap, and in estimating the number of cores and clock frequency that optimizes energy performance without compromising execution time. Since energy usage is the product of utilized power and execution time, the model first estimates the execution time of a program. CPU time, which accounts for both cores and memory response time, is modeled as an M/G/1 queuing system. Workload characterization of high performance computing, web hosting and financial computing applications shows that bursty memory traffic fits a Pareto distribution, and non-bursty memory traffic is exponentially distributed. Our analysis using these server workloads reveals that not all server workloads might benefit from higher number of cores or clock frequencies. Applying our model, we predict the configurations that increase energy efficiency by 10% without turning off cores, and up to one third with shutting down unutilized cores. For memory-bounded programs, we show that the limited memory bandwidth might increase both execution time and energy usage, to the point where energy cost might be higher than on a typical x64 multicore system. Lastly, we show that increasing memory and I/O bandwidth can improve both the execution time and the energy usage of server workloads on ARM Cortex-A9 systems.;low-power, multicore, energy, analytical model, performance, servers;;
Conference Paper;Zhang Y,Ren H,Khailany B;GRANNITE: Graph Neural Network Inference for Transferable Power Estimation;;2020;;;;IEEE Press;Virtual Event, USA;;Proceedings of the 57th ACM/EDAC/IEEE Design Automation Conference;;2020;9,78145E+12;;;;This paper introduces GRANNITE, a GPU-accelerated novel graph neural network (GNN) model for fast, accurate, and transferable vector-based average power estimation. During training, GRANNITE learns how to propagate average toggle rates through combinational logic: a netlist is represented as a graph, register states and unit inputs from RTL simulation are used as features, and combinational gate toggle rates are used as labels. A trained GNN model can then infer average toggle rates on a new workload of interest or new netlists from RTL simulation results in a few seconds. Compared to traditional power analysis using gate-level simulations, GRANNITE achieves >18.7X speedup with an error of only <5.5% across a diverse set of benchmark circuits. Compared to a GPU-accelerated conventional probabilistic switching activity estimation approach, GRANNITE achieves much better accuracy (on average 25.9% lower error) at similar runtimes.;graph neural network, power estimation, machine learning;;DAC '20
Conference Paper;Johann T,Dick M,Naumann S,Kern E;How to Measure Energy-Efficiency of Software: Metrics and Measurement Results;;2012;;;51–54;IEEE Press;Zurich, Switzerland;;Proceedings of the First International Workshop on Green and Sustainable Software;;2012;9,78147E+12;;;;In the field of information and computer technology (ICT), saving energy has its focus set on energy efficient hardware and its operation. Recently, efforts have also been made in the area of computer software. However, the development of energy efficient software requires metrics, which measure the software's energy consumption as well as models to monitor and minimize it. In software and software development processes they hardly exist. In this work we present a generic metric to measure software and a method to apply it in a software engineering process.;green software, energy efficiency, metrics;;GREENS '12
Conference Paper;Yang T,Ukezono T,Sato T;Reducing Power Consumption Using Approximate Encoding for CNN Accelerators at the Edge;;2022;;;229–235;Association for Computing Machinery;New York, NY, USA;;Proceedings of the Great Lakes Symposium on VLSI 2022;Irvine, CA, USA;2022;9,78145E+12;;"https://doi.org/10.1145/3526241.3530315;http://dx.doi.org/10.1145/3526241.3530315";10.1145/3526241.3530315;Convolutional neural networks (CNNs) have demonstrated significant potential across a range of applications due to their superior accuracy. Edge inference, in which inference is performed locally in embedded systems with limited power resources, is researched for its energy efficiency. An approximate encoder is proposed in this study for decreasing switching activity, which minimizes power consumption in CNN accelerators at the edge. The proposed encoder performs approximate encoding based on a pattern matching of a comparison pattern and current data. Software determines the value of the comparison pattern and the availability of the recommended encoder. Experiments with a CIFAR-10 dataset utilizing LeNet5 show that using the suggested encoder, depending upon the comparison pattern, power consumption of a CNN accelerator can be reduced by 21.5% with 1.59% degradation on inference quality.;reducing switching activity, convolutional neural network, low-power CNN accelerator, approximate encoding;;GLSVLSI '22
Journal Article;Nishida S,Sakuraba S,Asai K,Hamada M;Estimating Energy Parameters for RNA Secondary Structure Predictions Using Both Experimental and Computational Data;IEEE/ACM Trans. Comput. Biol. Bioinformatics;2019;16;5;1645–1655;IEEE Computer Society Press;Washington, DC, USA;;;;2019-09;;1545-5963;"https://doi.org/10.1109/TCBB.2018.2813388;http://dx.doi.org/10.1109/TCBB.2018.2813388";10.1109/TCBB.2018.2813388;Computational RNA secondary structure prediction depends on a large number of nearest-neighbor free-energy parameters, including 10 parameters for Watson-Crick stacked base pairs that were estimated from experimental measurements of the free energies of 90 RNA duplexes. These experimental data are provided by time-consuming and cost-intensive experiments. In contrast, various modified nucleotides in RNAs, which would affect not only their structures but also functions, have been found, and rapid determination of energy parameters for a such modified nucleotides is needed. To reduce the high cost of determining energy parameters, we propose a novel method to estimate energy parameters from both experimental and computational data, where the computational data are provided by a recently developed molecular dynamics simulation protocol. We evaluate our method for Watson-Crick stacked base pairs, and show that parameters estimated from 10 experimental data items and 10 computational data items can predict RNA secondary structures with accuracy comparable to that using conventional parameters. The results indicate that the combination of experimental free-energy measurements and molecular dynamics simulations is capable of estimating the thermodynamic properties of RNA secondary structures at lower cost.;;;
Conference Paper;Pinto G,Castor F,Liu YD;Mining Questions about Software Energy Consumption;;2014;;;22–31;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 11th Working Conference on Mining Software Repositories;Hyderabad, India;2014;9,78145E+12;;"https://doi.org/10.1145/2597073.2597110;http://dx.doi.org/10.1145/2597073.2597110";10.1145/2597073.2597110;A growing number of software solutions have been proposed to address application-level energy consumption problems in the last few years. However, little is known about how much software developers are concerned about energy consumption, what aspects of energy consumption they consider important, and what solutions they have in mind for improving energy efficiency. In this paper we present the first empirical study on understanding the views of application programmers on software energy consumption problems. Using StackOverflow as our primary data source, we analyze a carefully curated sample of more than 300 questions and 550 answers from more than 800 users. With this data, we observed a number of interesting findings. Our study shows that practitioners are aware of the energy consumption problems: the questions they ask are not only diverse -- we found 5 main themes of questions -- but also often more interesting and challenging when compared to the control question set. Even though energy consumption-related questions are popular when considering a number of different popularity measures, the same cannot be said about the quality of their answers. In addition, we observed that some of these answers are often flawed or vague. We contrast the advice provided by these answers with the state-of-the-art research on energy consumption. Our summary of software energy consumption problems may help researchers focus on what matters the most to software developers and end users.;Software Energy Consumption, Q&A, Practitioners;;MSR 2014
Conference Paper;Gaska B,Gniady C,Surdeanu M;MLStar: Machine Learning in Energy Profile Estimation of Android Apps;;2018;;;216–225;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 15th EAI International Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services;New York, NY, USA;2018;9,78145E+12;;"https://doi.org/10.1145/3286978.3287011;http://dx.doi.org/10.1145/3286978.3287011";10.1145/3286978.3287011;Improving the energy efficiency of smartphones is critical for increasing the utility that they provide to the users. With most mobile operating systems, users are responsible for managing their phone's battery efficiency by utilizing the various settings provided by the operating system, as well as selecting energy-efficient apps. However, current app marketplaces do not provide users with information about app energy efficiency, which makes it challenging for the user to make informed decision when selecting an app. This paper presents a novel machine learning approach to estimate app energy efficiency by utilizing textual information available in the Google Play store such as an app's description, user reviews, as well as system permissions. Our detailed analysis of the resulting system shows that hardware permissions, app description, and user reviews correlate well with energy efficiency ratings. We evaluate five models that represent popular classes of machine learning algorithms in their ability to predict energy efficiency ratings. Finally, we compare our approach to gold truth ratings obtained by the actual energy profiling of the app, demonstrating that the proposed system is able to estimate an app's energy efficiency within less than 1 point on the 1-5 scale provided by the profiler, without requiring any kind of profiling.;energy, mobile, machine learning;;MobiQuitous '18
Journal Article;Ismail L,Materwala H;Computing Server Power Modeling in a Data Center: Survey, Taxonomy, and Performance Evaluation;ACM Comput. Surv.;2020;53;3;;Association for Computing Machinery;New York, NY, USA;;;;2020-06;;0360-0300;"https://doi.org/10.1145/3390605;http://dx.doi.org/10.1145/3390605";10.1145/3390605;Data centers are large-scale, energy-hungry infrastructure serving the increasing computational demands as the world is becoming more connected in smart cities. The emergence of advanced technologies such as cloud-based services, internet of things (IoT), and big data analytics has augmented the growth of global data centers, leading to high energy consumption. This upsurge in energy consumption of the data centers not only incurs the issue of surging high cost (operational and maintenance) but also has an adverse effect on the environment. Dynamic power management in a data center environment requires the cognizance of the correlation between the system and hardware-level performance counters and the power consumption. Power consumption modeling exhibits this correlation and is crucial in designing energy-efficient optimization strategies based on resource utilization. Several works in power modeling are proposed and used in the literature. However, these power models have been evaluated using different benchmarking applications, power-measurement techniques, and error-calculation formulas on different machines. In this work, we present a taxonomy and evaluation of 24 software-based power models using a unified environment, benchmarking applications, power-measurement techniques, and error formulas, with the aim of achieving an objective comparison. We use different server architectures to assess the impact of heterogeneity on the models’ comparison. The performance analysis of these models is elaborated in the article.;green computing, resource utilization, energy-efficiency, Data center, machine learning, server power consumption modeling;;
Conference Paper;Jamali H,Karimi A,Haghighizadeh M;A New Method of Cloud-Based Computation Model for Mobile Devices: Energy Consumption Optimization in Mobile-to-Mobile Computation Offloading;;2018;;;32–37;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 6th International Conference on Communications and Broadband Networking;Singapore, Singapore;2018;9,78145E+12;;"https://doi.org/10.1145/3193092.3193103;http://dx.doi.org/10.1145/3193092.3193103";10.1145/3193092.3193103;Today, cell phones have great important role in everyday lives. They are the most effective and achievable communication and computation devices that necessitate no exact time or location. Fast development of movable calculation created an energetic power to develop a synchronous technology in the markets. Anyhow movable devices are encountering with various challenges concerning available resources such as: battery, storage, bandwidth, security, and mobility. On the other side, limitation of resources in these phones has a great effect on the quality of the services. The most important limitation is the energy consuming that leads us to use the device for calculation, in spite of the accessibility of other sources. The best suggested solutions are getting rid of computation on the cloud and use of portable cloud for computation. But If there was no accessible cloud or an accurace of disconnection with the cloud, cell phones are the best to use. According to a mathematical algorithm based on Lyapunov optimization and regarding the requirement time for suitable program, we will try to introduce a dynamic way which its limitation analysis reveals that offlooding the computation considering the limitation of time leads to a less spending of power and energy resembled the current algorithm.;Energy consuming, Mobile cloud computing, energy minimization, Cloud computing, computation offloading;;ICCBN 2018
Conference Paper;Altamimi ML,Naik K;A Computing Profiling Procedure for Mobile Developers to Estimate Energy Cost;;2015;;;301–305;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 18th ACM International Conference on Modeling, Analysis and Simulation of Wireless and Mobile Systems;Cancun, Mexico;2015;9,78145E+12;;"https://doi.org/10.1145/2811587.2811627;http://dx.doi.org/10.1145/2811587.2811627";10.1145/2811587.2811627;Mobile devices are constrained by the limited capacities of their small batteries. However, profiling the energy consumed in the task execution is crucial to help the developers to build energy efficient applications. Therefore, the major challenge in the profiling approach is to accurately estimating the energy consumed for an application by the hardware components, such as CPU, memory, storage unit, and network interfaces. In this work, we develop and validate hardware and software profiling models and procedures. We profile smartphone CPU, where we consider multi-core CPUs and the impact of Dynamic Voltage and Frequency Scaling mechanism on the power consumption. In addition, we profile smartphone storage unit by taking into account the writing and reading rate to the unit. Moreover, we experimentally validated these profiles on two diverse smartphones with different versions of operating systems. The experimental results reveal that our profiles are able to estimate the application energy accurately.;application energy, power and energy consumption, dynamic voltage and frequency scaling, cpu, multi-core cpu, mobile devices;;MSWiM '15
Conference Paper;Hampau RM,Kaptein M,van Emden R,Rost T,Malavolta I;An Empirical Study on the Performance and Energy Consumption of AI Containerization Strategies for Computer-Vision Tasks on the Edge;;2022;;;50–59;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 26th International Conference on Evaluation and Assessment in Software Engineering;Gothenburg, Sweden;2022;9,78145E+12;;"https://doi.org/10.1145/3530019.3530025;http://dx.doi.org/10.1145/3530019.3530025";10.1145/3530019.3530025;Context. The rise of use cases of AI catered towards the Edge, where devices have limited computation power and storage capabilities, motivates the need for better understating of how AI performs and consumes energy. Goal. The aim of this paper is to empirically assess the impact of three different AI containerization strategies on the energy consumption, execution time, CPU, and memory usage for computer-vision tasks on the Edge. Method. In this paper we conduct an experiment with the used containerization strategy as main factor, with three treatments: ONNX Runtime, WebAssembly, and Docker. The subjects of the experiment are four widely-used computer-vision algorithms. We then orchestrate a series of runs where we deploy the four subjects on different generations of Raspberry Pi devices, with different hardware capabilities. A total of 120 runs (per device) are recorded to gather data on energy, execution time, CPU, and memory. Results. We found a statistically significant difference between the three containerization strategies on all dependent variables. Specifically, WebAssembly proves to be a valuable alternative for devices with reduced disk space and computation power. Conclusions. For computer-vision tasks with limited disk space and RAM memory requirements, developers should prefer WebAssembly for deployment. The (non-dockerized) ONNX Runtime resulted to be the best choice in terms of energy consumption and execution time.;;;EASE '22
Conference Paper;Berkelaar MR,Buurman PH,Jess JA;Computing the Entire Active Area/Power Consumption versus Delay Trade-off Curve for Gate Sizing with a Piecewise Linear Simulator;;1994;;;474–480;IEEE Computer Society Press;Washington, DC, USA;;Proceedings of the 1994 IEEE/ACM International Conference on Computer-Aided Design;San Jose, California, USA;1994;9,7809E+12;;;;The gate sizing problem is the problem of finding load drive capabilities for all gates in a given Boolean network such, that a given delay limit is kept, and the necessary cost in terms of active area usage and/or power consumption is minimal. This paper describes a way to obtain the entire cost versus delay trade-off curve of a combinational logic circuit in an efficient way. Every point on the resulting curve is the global optimum of the corresponding gate sizing problem. The problem is solved by mapping it onto piecewise linear models in such a way, that a piecewise linear (circuit) simulator can do the job. It is shown that this setup is very efficient, and can produce trade-off curves for large circuits (thousands of gates) in a few minutes. Benchmark results for the entire set of MCNC '91 two-level examples are given.;;;ICCAD '94
Conference Paper;Chakraborty I,Chandan V,Vrabie D;A Sequential DNN Based Baseline Energy Prediction Framework with Long Term Error Mitigation;;2019;;;508–515;Association for Computing Machinery;New York, NY, USA;;Proceedings of the Tenth ACM International Conference on Future Energy Systems;Phoenix, AZ, USA;2019;9,78145E+12;;"https://doi.org/10.1145/3307772.3331027;http://dx.doi.org/10.1145/3307772.3331027";10.1145/3307772.3331027;In this paper, we present a novel sequential framework containing two deep network architectures for baseline energy prediction of a building. The proposed framework utilizes convolution layers to extract features from the input data space without changing spatial relations between variables. These features are memorized by tensor train based gated recurrent units for an accurate long-term prediction. We observe that this sequential framework helps to improve long term prediction accuracy, thereby mitigating prediction error accumulation over time. Although architectures comprising the amalgamation of convolution layers and memory cell have shown promising results in domains such as social media analysis, language modeling, video frame prediction, and image recognition, this paper extends its scope to the context of energy applications. Furthermore, the addition of tensor train based gated recurrent units is motivated by the necessity of computational time reduction during the training process, thereby making this framework suitable for field deployment. Results on a commercial building dataset show that the current framework outperforms other existing machine learning based methods, in both short-term and long-term prediction categories.;;;e-Energy '19
Journal Article;Klostermeyer WF,Srinivas K;Reducing Disk Power Consumption in a Portable Computer;SIGOPS Oper. Syst. Rev.;1995;29;2;27–32;Association for Computing Machinery;New York, NY, USA;;;;1995-04;;0163-5980;"https://doi.org/10.1145/202213.202218;http://dx.doi.org/10.1145/202213.202218";10.1145/202213.202218;The problem of minimizing disk power consumption in portable personal computers is studied. Two online algorithms for determining when to stop spinning a disk are presented and analyzed using competitive analysis techniques.;;;
Conference Paper;Hindle A,Wilson A,Rasmussen K,Barlow EJ,Campbell JC,Romansky S;GreenMiner: A Hardware Based Mining Software Repositories Software Energy Consumption Framework;;2014;;;12–21;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 11th Working Conference on Mining Software Repositories;Hyderabad, India;2014;9,78145E+12;;"https://doi.org/10.1145/2597073.2597097;http://dx.doi.org/10.1145/2597073.2597097";10.1145/2597073.2597097;Green Mining is a field of MSR that studies software energy consumption and relies on software performance data. Unfortunately there is a severe lack of publicly available software power use performance data. This means that green mining researchers must generate this data themselves by writing tests, building multiple revisions of a product, and then running these tests multiple times (10+) for each software revision while measuring power use. Then, they must aggregate these measurements to estimate the energy consumed by the tests for each software revision. This is time consuming and is made more difficult by the constraints of mobile devices and their OSes. In this paper we propose, implement, and demonstrate Green Miner: the first dedicated hardware mining software repositories testbed. The Green Miner physically measures the energy consumption of mobile devices (Android phones) and automates the testing of applications, and the reporting of measurements back to developers and researchers. The Green Miner has already produced valuable results for commercial Android application developers, and has been shown to replicate other power studies' results.;Software Change, Software Energy Consumption, Android;;MSR 2014
Conference Paper;Moussa RR,Dewidar KM;Evaluating the Efficiency of Energy-Scape Software;;2019;;;60–65;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 8th International Conference on Software and Information Engineering;Cairo, Egypt;2019;9,78145E+12;;"https://doi.org/10.1145/3328833.3328868;http://dx.doi.org/10.1145/3328833.3328868";10.1145/3328833.3328868;Recently, researchers were focusing on integrating renewable energy (RE) within urban environment instead of integrating renewables with buildings due to the large occupies of urban areas. Urban areas have a great potential in generating sufficient amount of energy that could satisfy the needs of urban neighborhoods. Energy-scape elements are sustainable elements that integrates RE devices with landscape elements. This research focuses on analyzing the importance and efficiency of Energy-scape software through a qualitative method. The efficiency of Energy-scape web-based application will be tested using qualitative method and a site survey. The research concludes that Energy-scape software application is an effective tool for landscape designers in using Energy-scape elements, it identifies the optimum type and location of Energy-scape elements within their projects, and it calculates the impact of using Energy-scape elements in term of energy-savings and carbon emission (CO2) reduction.;Renewable energy, Landscape elements, Energy-scape database, Energy-scape Elements, Energy-scape software;;ICSIE '19
Conference Paper;van Gastel B;Analysing Energy Consumption of Systems Controlled by Software;;2017;;;;Association for Computing Machinery;New York, NY, USA;;Companion Proceedings of the 1st International Conference on the Art, Science, and Engineering of Programming;Brussels, Belgium;2017;9,78145E+12;;"https://doi.org/10.1145/3079368.3079396;http://dx.doi.org/10.1145/3079368.3079396";10.1145/3079368.3079396;Energy consumption analysis of IT-controlled systems can play a major role in minimising the overall energy consumption of such IT systems, during the development phase, or for optimisation in the field. As software is increasingly embedded in our daily life, with IT using more and more energy, the software industry should become aware of their energy footprint, and methods must be developed to assist in reducing this footprint.Recently, we developed a precise energy analysis, to analyse software in conjunction with hardware. It has the property of being parametric with regard to the hardware. In principle, this creates the opportunity to investigate which is the best software implementation for given hardware, or the other way around: choose the best hardware for a given algorithm.;;;Programming '17
Journal Article;Ha TM,Samejima M,Komoda N;Power and Performance Estimation for Fine-Grained Server Power Capping via Controlling Heterogeneous Applications;ACM Trans. Manage. Inf. Syst.;2017;8;4;;Association for Computing Machinery;New York, NY, USA;;;;2017-08;;2158-656X;"https://doi.org/10.1145/3086449;http://dx.doi.org/10.1145/3086449";10.1145/3086449;Power capping is a method to save power consumption of servers by limiting performance of the servers. Although users frequently run applications on different virtual machines (VMs) for keeping their performance and having them isolated from the other applications, power capping may degrade performance of all the applications running on the server. We present fine-grained power capping by limiting performance of each application individually. For keeping performance defined in Quality of Service (QoS) requirements, it is important to estimate applications’ performance and power consumption after the fine-grained power capping is applied. We propose the estimation method of physical CPU usage when limiting virtual CPU usage of applications on VMs. On servers where multiple VMs run, VM’s usage of physical CPU is interrupted by the other VMs, and a hypervisor uses physical CPU to control VMs. These VMs’ and hypervisor’s behaviors make it difficult to estimate performance and power consumption by straightforward methods, such as linear regression and polynomial regression. The proposed method uses Piecewise Linear Regression to estimate physical CPU usage by assuming that VM’s access to physical CPU is not interrupted by the other VMs. Then we estimate how much physical CPU usage is reduced by the interruption. Because physical CPU usage is not stable soon after limiting CPU usage, the proposed method estimates a convergence value of CPU usage after many interruptions are repeated.;piecewise linear regression, Server power capping, response time, power consumption;;
Conference Paper;Rieger F,Bockisch C;Survey of Approaches for Assessing Software Energy Consumption;;2017;;;19–24;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 2nd ACM SIGPLAN International Workshop on Comprehension of Complex Systems;Vancouver, BC, Canada;2017;9,78145E+12;;"https://doi.org/10.1145/3141842.3141846;http://dx.doi.org/10.1145/3141842.3141846";10.1145/3141842.3141846;Though the energy consumption of software-controlled ICT systems ranging from mobile devices to data centers is increasingly gaining attention, energy optimization is still far from an established task in the software development process. Therefore, we have surveyed the available research on assessing the energy consumption of software systems, which showed a lack of development tools, but several approaches exist for measuring the energy consumption. We group these approaches according to how measurement data is made available and compare several characteristics of the collected data. The survey shows that not only development tools for software energy optimization are still missing, but there is also a lack of fine-grained measurement approaches as well as approaches for general-purpose platforms.;Survey, Software Energy Behavior, Energy Analysis, Energy Measurement;;CoCoS 2017
Conference Paper;Noureddine A,Rouvoy R,Seinturier L;Unit Testing of Energy Consumption of Software Libraries;;2014;;;1200–1205;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 29th Annual ACM Symposium on Applied Computing;Gyeongju, Republic of Korea;2014;9,78145E+12;;"https://doi.org/10.1145/2554850.2554932;http://dx.doi.org/10.1145/2554850.2554932";10.1145/2554850.2554932;The development of energy-efficient software has become a key requirement for a large number of devices, from smartphones to data centers. However, measuring accurately this consumption is a major challenge that state-of-the-art approaches have tried to tackle with a limited success. While monitoring applications' consumption offers a clear insight on where the energy is being spent, it does not help in understanding how the energy is consumed. In this paper, we therefore introduce JalenUnit, a software framework that infers the energy consumption model of software libraries from execution traces. This model can then be used to diagnose application code for detecting energy bugs, understanding energy distribution, establishing energy profiles and classifications, and comparing software libraries against their energy consumption.;energy measurement, software metrics, empirical benchmarking, energy, modeling, benchmarks, power modeling;;SAC '14
Conference Paper;Jagroep E,Broekman J,van der Werf JM,Brinkkemper S,Lago P,Blom L,van Vliet R;Awakening Awareness on Energy Consumption in Software Engineering;;2017;;;76–85;IEEE Press;Buenos Aires, Argentina;;Proceedings of the 39th International Conference on Software Engineering: Software Engineering in Society Track;;2017;9,78154E+12;;"https://doi.org/10.1109/ICSE-SEIS.2017.10;http://dx.doi.org/10.1109/ICSE-SEIS.2017.10";10.1109/ICSE-SEIS.2017.10;Software producing organizations have the ability to address the energy impact of their ICT solutions during the development process. However, while industry is convinced of the energy impact of hardware, the role of software has mostly been acknowledged by researchers in software engineering. Strengthened by the limited practical knowledge to reduce the energy consumption, organizations have less control over the energy impact of their products and lose the contribution of software towards energy related strategies. Consequently, industry risks not being able to meet customer requirements or even fulfill corporate sustainability goals.In this paper we perform an exploratory case study on how to create and maintain awareness on an energy consumption perspective for software among stakeholders involved with the development of software products. During the study, we followed the development process of two commercial software products and provided direct feedback to the stakeholders on the effects of their development efforts, specifically concerning energy consumption and performance, using an energy dashboard. Multiple awareness measurements allowed us to keep track of changes over time on specific aspects affecting software development. Our results show that, despite a mixed sentiment towards the dashboard, changed awareness has triggered discussion on the energy consumption of software.;software engineering, energy consumption perspective, software energy consumption, awareness;;ICSE-SEIS '17
Conference Paper;Ournani Z,Rouvoy R,Rust P,Penhoat J;On Reducing the Energy Consumption of Software: From Hurdles to Requirements;;2020;;;;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 14th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM);Bari, Italy;2020;9,78145E+12;;"https://doi.org/10.1145/3382494.3410678;http://dx.doi.org/10.1145/3382494.3410678";10.1145/3382494.3410678;Background. As software took control over hardware in many domains, the question of the energy footprint induced by the software is becoming critical for our society, as the resources powering the underlying infrastructure are finite. Yet, beyond this growing interest, energy consumption remains a difficult concept to master for a developer.Aims. The purpose of this study is to better understand the root causes that prevent the issue of software energy consumption to be more widely considered by developers and companies.Method. To investigate this issue, this paper reports on a qualitative study we conducted in an industrial context. We applied an in-depth analysis of the interviews of 10 experienced developers and summarized a set of implications.Results. We argue that our study delivers i) insightful feedback on how green software design is considered among the interviewed developers and ii) a set of findings to build helpful tools, motivate further research, and establish better development strategies to promote green software design.Conclusion. This paper covers an industrial case study of developers' awareness of green software design and how to promote it within the company. While it might not be generalizable for any company, we believe our results deliver a common body of knowledge with implications to be considered for similar cases and further researches.;;;ESEM '20
Conference Paper;Rodionov D,Daniali S,Khortabi F,Moqaddasnejad A;Study Effect of Information and Communication Technology (ICT) on Energy Consumption and Greenhouse Gas Emissions in Selected Oil-Producing Countries;;2022;;;35–41;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 3rd International Scientific Conference on Innovations in Digital Economy;Saint - Petersburg, Russian Federation;2022;9,78145E+12;;"https://doi.org/10.1145/3527049.3527123;http://dx.doi.org/10.1145/3527049.3527123";10.1145/3527049.3527123;The current trend of increasing energy consumption in the world has brought about two major crises for humanity. The first one is environmental pollution and the running out of energy resources. Energy supply is one of the fundamental requirements of economic development and improving the quality of human life. Since the industrial revolution in the mid-18th century, the ever-growing consumption of energy in the world has been continued. Further, Carbon Dioxide emitted during the combustion of fossil fuels has exerted irreversible and threatening changes in the world. Today, experts and researchers are looking for sustainable development and some strategies to reduce energy consumption and environmental pollution. Using ICT has been considered to be one of the effective solutions to achieve these goals. Due to the adverse economic and environmental consequences, as well as the continuous increase of greenhouse gas emissions, there is an emergent need to conduct some inquiries to identify the source of problems and propose some efficient coping strategies. The present study is conducted to investigate the relationship between carbon dioxide emissions, energy consumption, ICT development index, and economic growth. For this purpose, the data panel method was utilized for selected oil producing countries from 1998 to 2011. The results of model estimation revealed that the Kuznets hypothesis was confirmed in the period under investigation and also the development of ICT had a positive effect on reducing energy consumption and air pollution.;;;SPBPU IDE '21
Conference Paper;Li Z,Grosu R,Sehgal P,Smolka SA,Stoller SD,Zadok E;On the Energy Consumption and Performance of Systems Software;;2011;;;;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 4th Annual International Conference on Systems and Storage;Haifa, Israel;2011;9,78145E+12;;"https://doi.org/10.1145/1987816.1987827;http://dx.doi.org/10.1145/1987816.1987827";10.1145/1987816.1987827;Models of energy consumption and performance are necessary to understand and identify system behavior, prior to designing advanced controls that can balance out performance and energy use. This paper considers the energy consumption and performance of servers running a relatively simple file-compression workload. We found that standard techniques for system identification do not produce acceptable models of energy consumption and performance, due to the intricate interplay between the discrete nature of software and the continuous nature of energy and performance. This motivated us to perform a detailed empirical study of the energy consumption and performance of this system with varying compression algorithms and compression levels, file types, persistent storage media, CPU DVFS levels, and disk I/O schedulers. Our results identify and illustrate factors that complicate the system's energy consumption and performance, including nonlinearity, instability, and multi-dimensionality. Our results provide a basis for future work on modeling energy consumption and performance to support principled design of controllable energy-aware systems.;energy efficiency, data compression, system identification;;SYSTOR '11
Conference Paper;Guégain É,Quinton C,Rouvoy R;On Reducing the Energy Consumption of Software Product Lines;;2021;;;89–99;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A;Leicester, United Kingdom;2021;9,78145E+12;;"https://doi.org/10.1145/3461001.3471142;http://dx.doi.org/10.1145/3461001.3471142";10.1145/3461001.3471142;Along the last decade, several studies considered green software design as a key development concern to improve the energy efficiency of software. Yet, few techniques address this concern for Software Product Lines (SPL). In this paper, we therefore introduce two approaches to measure and reduce the energy consumption of a SPL by analyzing a limited set of products sampled from this SPL. While the first approach relies on the analysis of individual feature consumptions, the second one takes feature interactions into account to better mitigate energy consumption of resulting products.Our experimental results on a real-world SPL indicate that both approaches succeed to produce significant energy improvements on a large number of products, while consumption data was modeled from a small set of sampled products. Furthermore, we show that taking feature interactions into account leads to more products improved with higher energy savings per product.;mitigation, consumption, measurement, software product lines, energy;;SPLC '21
Conference Paper;Juang P,Oki H,Wang Y,Martonosi M,Peh LS,Rubenstein D;Energy-Efficient Computing for Wildlife Tracking: Design Tradeoffs and Early Experiences with ZebraNet;;2002;;;96–107;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 10th International Conference on Architectural Support for Programming Languages and Operating Systems;San Jose, California;2002;9,78158E+12;;"https://doi.org/10.1145/605397.605408;http://dx.doi.org/10.1145/605397.605408";10.1145/605397.605408;"Over the past decade, mobile computing and wireless communication have become increasingly important drivers of many new computing applications. The field of wireless sensor networks particularly focuses on applications involving autonomous use of compute, sensing, and wireless communication devices for both scientific and commercial purposes. This paper examines the research decisions and design tradeoffs that arise when applying wireless peer-to-peer networking techniques in a mobile sensor network designed to support wildlife tracking for biology research.The ZebraNet system includes custom tracking collars (nodes) carried by animals under study across a large, wild area; the collars operate as a peer-to-peer network to deliver logged data back to researchers. The collars include global positioning system (GPS), Flash memory, wireless transceivers, and a small CPU; essentially each node is a small, wireless computing device. Since there is no cellular service or broadcast communication covering the region where animals are studied, ad hoc, peer-to-peer routing is needed. Although numerous ad hoc protocols exist, additional challenges arise because the researchers themselves are mobile and thus there is no fixed base station towards which to aim data. Overall, our goal is to use the least energy, storage, and other resources necessary to maintain a reliable system with a very high `data homing' success rate. We plan to deploy a 30-node ZebraNet system at the Mpala Research Centre in central Kenya. More broadly, we believe that the domain-centric protocols and energy tradeoffs presented here for ZebraNet will have general applicability in other wireless and sensor applications.";;;ASPLOS X
Journal Article;Juang P,Oki H,Wang Y,Martonosi M,Peh LS,Rubenstein D;Energy-Efficient Computing for Wildlife Tracking: Design Tradeoffs and Early Experiences with ZebraNet;SIGARCH Comput. Archit. News;2002;30;5;96–107;Association for Computing Machinery;New York, NY, USA;;;;2002-10;;0163-5964;"https://doi.org/10.1145/635506.605408;http://dx.doi.org/10.1145/635506.605408";10.1145/635506.605408;"Over the past decade, mobile computing and wireless communication have become increasingly important drivers of many new computing applications. The field of wireless sensor networks particularly focuses on applications involving autonomous use of compute, sensing, and wireless communication devices for both scientific and commercial purposes. This paper examines the research decisions and design tradeoffs that arise when applying wireless peer-to-peer networking techniques in a mobile sensor network designed to support wildlife tracking for biology research.The ZebraNet system includes custom tracking collars (nodes) carried by animals under study across a large, wild area; the collars operate as a peer-to-peer network to deliver logged data back to researchers. The collars include global positioning system (GPS), Flash memory, wireless transceivers, and a small CPU; essentially each node is a small, wireless computing device. Since there is no cellular service or broadcast communication covering the region where animals are studied, ad hoc, peer-to-peer routing is needed. Although numerous ad hoc protocols exist, additional challenges arise because the researchers themselves are mobile and thus there is no fixed base station towards which to aim data. Overall, our goal is to use the least energy, storage, and other resources necessary to maintain a reliable system with a very high `data homing' success rate. We plan to deploy a 30-node ZebraNet system at the Mpala Research Centre in central Kenya. More broadly, we believe that the domain-centric protocols and energy tradeoffs presented here for ZebraNet will have general applicability in other wireless and sensor applications.";;;
Journal Article;Juang P,Oki H,Wang Y,Martonosi M,Peh LS,Rubenstein D;Energy-Efficient Computing for Wildlife Tracking: Design Tradeoffs and Early Experiences with ZebraNet;SIGPLAN Not.;2002;37;10;96–107;Association for Computing Machinery;New York, NY, USA;;;;2002-10;;0362-1340;"https://doi.org/10.1145/605432.605408;http://dx.doi.org/10.1145/605432.605408";10.1145/605432.605408;"Over the past decade, mobile computing and wireless communication have become increasingly important drivers of many new computing applications. The field of wireless sensor networks particularly focuses on applications involving autonomous use of compute, sensing, and wireless communication devices for both scientific and commercial purposes. This paper examines the research decisions and design tradeoffs that arise when applying wireless peer-to-peer networking techniques in a mobile sensor network designed to support wildlife tracking for biology research.The ZebraNet system includes custom tracking collars (nodes) carried by animals under study across a large, wild area; the collars operate as a peer-to-peer network to deliver logged data back to researchers. The collars include global positioning system (GPS), Flash memory, wireless transceivers, and a small CPU; essentially each node is a small, wireless computing device. Since there is no cellular service or broadcast communication covering the region where animals are studied, ad hoc, peer-to-peer routing is needed. Although numerous ad hoc protocols exist, additional challenges arise because the researchers themselves are mobile and thus there is no fixed base station towards which to aim data. Overall, our goal is to use the least energy, storage, and other resources necessary to maintain a reliable system with a very high `data homing' success rate. We plan to deploy a 30-node ZebraNet system at the Mpala Research Centre in central Kenya. More broadly, we believe that the domain-centric protocols and energy tradeoffs presented here for ZebraNet will have general applicability in other wireless and sensor applications.";;;
Journal Article;Juang P,Oki H,Wang Y,Martonosi M,Peh LS,Rubenstein D;Energy-Efficient Computing for Wildlife Tracking: Design Tradeoffs and Early Experiences with ZebraNet;SIGOPS Oper. Syst. Rev.;2002;36;5;96–107;Association for Computing Machinery;New York, NY, USA;;;;2002-10;;0163-5980;"https://doi.org/10.1145/635508.605408;http://dx.doi.org/10.1145/635508.605408";10.1145/635508.605408;"Over the past decade, mobile computing and wireless communication have become increasingly important drivers of many new computing applications. The field of wireless sensor networks particularly focuses on applications involving autonomous use of compute, sensing, and wireless communication devices for both scientific and commercial purposes. This paper examines the research decisions and design tradeoffs that arise when applying wireless peer-to-peer networking techniques in a mobile sensor network designed to support wildlife tracking for biology research.The ZebraNet system includes custom tracking collars (nodes) carried by animals under study across a large, wild area; the collars operate as a peer-to-peer network to deliver logged data back to researchers. The collars include global positioning system (GPS), Flash memory, wireless transceivers, and a small CPU; essentially each node is a small, wireless computing device. Since there is no cellular service or broadcast communication covering the region where animals are studied, ad hoc, peer-to-peer routing is needed. Although numerous ad hoc protocols exist, additional challenges arise because the researchers themselves are mobile and thus there is no fixed base station towards which to aim data. Overall, our goal is to use the least energy, storage, and other resources necessary to maintain a reliable system with a very high `data homing' success rate. We plan to deploy a 30-node ZebraNet system at the Mpala Research Centre in central Kenya. More broadly, we believe that the domain-centric protocols and energy tradeoffs presented here for ZebraNet will have general applicability in other wireless and sensor applications.";;;
Conference Paper;Poroseva SV;On Reducing Computational Complexity in Evaluating the Topological Survivability of Power Systems;;2010;;;27–31;Society for Modeling & Simulation International;Vista, CA;;Proceedings of the 2010 Conference on Grand Challenges in Modeling & Simulation;Ottawa, Ontario, Canada;2010;;;;;The topological survivability of a power system is the system ability due to its topology -- the number of system elements that generate and demand power and connections between them -- to supply power to loads under massive sudden damage. Previously we showed that the topological survivability of a power system can be quantified by analyzing the impact of all possible combinations of unrecoverable faults (fault scenarios) on the availability and connectivity of the system elements. The number of possible fault scenarios grows as 2M with increasing the number M of the system elements. Clearly, such an analysis is a computational challenge for large-scale systems. The paper discusses possibilities of reducing computational complexity of the problem.;survivability, computational analysis, power systems, network topology;;GCMS '10
Conference Paper;Zhang Y,Liu J,Wilson E,Kandemir M;Software-Directed Data Access Scheduling for Reducing Disk Energy Consumption;;2011;;;281–282;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 20th International Symposium on High Performance Distributed Computing;San Jose, California, USA;2011;9,78145E+12;;"https://doi.org/10.1145/1996130.1996175;http://dx.doi.org/10.1145/1996130.1996175";10.1145/1996130.1996175;;power optimization, multi-speed disk, i/o storage, compiler-directed, mpi-io, spin-down disk, data access scheduling;;HPDC '11
Conference Paper;Carvalho S,Sullivan J,Dias DM,Naredo E,Ryan C;Using Grammatical Evolution for Modelling Energy Consumption on a Computer Numerical Control Machine;;2021;;;1557–1563;Association for Computing Machinery;New York, NY, USA;;Proceedings of the Genetic and Evolutionary Computation Conference Companion;Lille, France;2021;9,78145E+12;;"https://doi.org/10.1145/3449726.3463185;http://dx.doi.org/10.1145/3449726.3463185";10.1145/3449726.3463185;Discrete manufacturing is known to be a high consumer of energy and much work has been done in continuous improvement and energy saving methods addressing this issue. Computer Numerical Control (CNC) machines, commonly used in the manufacturing of metal parts, are highly energy-demanding because of many required sub-systems, such as cooling, lubrication, logical interfaces and electric motors. For this reason, there is a large body of work focusing on modelling the energy needs of this class of machine.This paper applies Grammatical Evolution (GE) for developing auto-regressive models for the energy consumption of a CNC machine. Empirical data from three 24-hour work shifts comprising three different types of products are used as inputs. We also introduce an autocorrelation-informed approach for the grammar, which benefits from a prior analysis of the training data for better capturing periodic or close to periodic behaviour. Finally, we compare the outcomes from real and predicted energy profiles through the use of an existing analysis tool, which is capable of extracting production-related information such as total and average KW consumption, number of parts produced and breakdown of production and idle hours. Results show that GE yields accurate and explainable models for the analysed scenario.;energy consumption, real-world applications, grammatical evolution, CNC machines;;GECCO '21
Conference Paper;Yang H,Gao GR,Leung C;On Achieving Balanced Power Consumption in Software Pipelined Loops;;2002;;;210–217;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 2002 International Conference on Compilers, Architecture, and Synthesis for Embedded Systems;Grenoble, France;2002;9,78158E+12;;"https://doi.org/10.1145/581630.581663;http://dx.doi.org/10.1145/581630.581663";10.1145/581630.581663;While a significant body of work in compilers has been devoted to reducing energy consumption in embedded systems, the role of a compiler in harnessing the power variation has not been widely explored. Since sharp power variations across time steps cause power supply noises and degrade reliability of functional blocks, power variation is a design constraint in embedded systems. With the advent of high performance embedded systems and extensive deployment of fine grain clock-gating, reducing variations in power is becoming increasingly important.This paper studies how compilation techniques, more specifically instruction scheduling, can ameliorate variations in power due to functional units during program execution. By extending our previous work on rate-optimal software pipelining, this paper formulates the problem of constructing a performance-optimal schedule that minimizes power variations as an integer linear programming (ILP) problem. The formulation can be solved using an ILP solver. We applied our approach on SPEC NAS benchmarks to construct software pipelined schedules that have minimum power variations. The benchmarks are executed on the Wattch power simulator. In comparison to the original (power-unaware) scheduler implemented in the MIPSpro compiler, our power-aware approach generates schedules which have significantly lower power variations while maintaining the same performance. Such schedules have the potential to reduce hardware cost on power delivery in designing embedded systems.;instruction level parallelism, power-aware compilation, software pipelining;;CASES '02
Conference Paper;Liu J,Chang Y,Wei D,Wang D,Zhang T;An Information Monitoring Platform for Thermal Energy Storage Systems Using Cloud Computing;;2018;;;19–24;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 2018 International Conference on Cloud Computing and Internet of Things;Singapore, Singapore;2018;9,78145E+12;;"https://doi.org/10.1145/3291064.3291069;http://dx.doi.org/10.1145/3291064.3291069";10.1145/3291064.3291069;Energy storage plays a key element to use new energy to replace traditional coal and petrochemical energy, and it plays an important role of shifting energy utilization ways. Nowadays energy internet is a way of renewable and sharing energy. A cloud platform for monitoring energy information of thermal storage systems is developed by using cloud computing, IoT and energy storage technologies. This platform allows a user to monitor the running conditions of regional thermal energy systems in real-time from anywhere as the condition data are synchronized to the client-side website, and data are stored into a data storage server. To quickly locate any thermal energy storage system, the geographical locations of the systems built anywhere are marked on the map of the web page. The condition data of all systems can be collected and communicated between the control unit installed in the heat system and the servers on cloud. The gathered data can be worked out for further allocating energy consumption. This platform has been launched after successfully tested on a number of practical thermal energy storage systems at different regions. This will provide a guarantee for further intelligent analysis and optimization of energy deployment.;Monitoring, Energy internet, Thermal energy storage, Cloud computing;;CCIOT '18
Conference Paper;Duarte LM,da Silva Alves D,Toresan BR,Maia PH,Silva D;A Model-Based Framework for the Analysis of Software Energy Consumption;;2019;;;67–72;Association for Computing Machinery;New York, NY, USA;;Proceedings of the XXXIII Brazilian Symposium on Software Engineering;Salvador, Brazil;2019;9,78145E+12;;"https://doi.org/10.1145/3350768.3353813;http://dx.doi.org/10.1145/3350768.3353813";10.1145/3350768.3353813;Software is present in all types of devices, some of them with restrictions as to the amount of energy they can spend to execute software applications. For this reason, energy costs are becoming an important factor during software development and evolution. However, there is still little support for creating energy-efficient software. In this work, we introduce a possible framework for software energy costs evaluation based on model analysis. We model software as Labelled Transitions Systems (LTS) and annotate these models with energy costs, which can be obtained using existing tools. We can then apply graph-based algorithms to traverse the models to obtain information about energy consumption related to software behaviour, such as its most/least costly execution, the cost of a specific execution, and the average cost of executing the software. No existing tool currently provides all the necessary analyses, even though they are essential for energy-consumption evaluation. We have conducted a small experiment with our framework where we employed jRAPL to measure energy costs. We annotated the models with the collected energy costs using an extended version of the LoTuS tool, where we have also implemented some of the desired analyses. Based on this support and on our initial results, we believe developers could create software more energy-efficient and consider possible trade-offs related to time, space, and energy costs when producing new versions of their systems.;Embedded Software, Labelled Transition System, Energy Consumption Evaluation, Model-based Analysis;;SBES '19
Conference Paper;Kim Y,Mercati P,More A,Shriver E,Rosing T;P4: Phase-Based Power/Performance Prediction of Heterogeneous Systems via Neural Networks;;2017;;;683–690;IEEE Press;Irvine, California;;Proceedings of the 36th International Conference on Computer-Aided Design;;2017;;;;;The emergence of Internet of Things increases the complexity and the heterogeneity of computing platforms. Migrating workload between various platforms is one way to improve both energy efficiency and performance. Effective migration decisions require accurate estimates of its costs and benefits. To date, these estimates were done by either instrumenting the source code/binaries, thus causing high overhead, or by using power estimates from hardware performance counters, which work well for individual machines, but until now have not been accurate for predicting across different architectures. In this paper, we propose P4, a new Phase-based Power and Performance Prediction framework which identifies cross-platform application power and performance at runtime for heterogeneous computing systems. P4 analyzes and detects machine-independent application phases by characterizing computing platforms offline with a set of benchmarks, and then builds neural network-based models to automatically identify and generalize the complex cross-platform relationships for each benchmark phase. It then leverages these models along with performance counter measurements collected at runtime to estimate performance and power consumption if it were running on a completely different computing platform, including a different CPU architecture, without ever having to run it on there. We evaluate the proposed framework on four commercial heterogeneous platforms, ranging from X86 servers to mobile ARM-based architecture, with 129 industry-standard benchmarks. Our experimental results show that P4 can predict the power and performance changes with only 6.8% and 5.6% error, respectively, even for completely different architectures from the ones applications ran on.;phase recognition, power prediction, neural networks, performance prediction;;ICCAD '17
Conference Paper;Papadopoulos L,Marantos C,Digkas G,Ampatzoglou A,Chatzigeorgiou A,Soudris D;Interrelations between Software Quality Metrics, Performance and Energy Consumption in Embedded Applications;;2018;;;62–65;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 21st International Workshop on Software and Compilers for Embedded Systems;Sankt Goar, Germany;2018;9,78145E+12;;"https://doi.org/10.1145/3207719.3207736;http://dx.doi.org/10.1145/3207719.3207736";10.1145/3207719.3207736;Source code refactorings and transformations are extensively used by embedded system developers to improve the quality of applications, often supported by various open source and proprietary tools. They either aim at improving the design time quality such as the maintainability and reusability of software artifacts, or the runtime quality such as performance and energy efficiency. However, an inherent trade-off between design- and run-time qualities is often present posing challenges to embedded software development. This work is a first step towards the investigation of the impact of transformations for improving the performance and the energy efficiency on software quality metrics and the impact of refactorings for increasing the design time quality on the execution time, the memory and the energy consumption. Based on a set of embedded applications from widely used benchmark suites and typical transformations and refactorings, we identify interrelations and trade-offs between the aforementioned metrics.;;;SCOPES '18
Conference Paper;Krasov A,Pestov I,Gelfand A,Kazantsev A,Polyanicheva A;Using Mathematical Forecasting Methods to Estimate the Load on the Computing Power of the IoT Network;;2021;;;;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 4th International Conference on Future Networks and Distributed Systems;St.Petersburg, Russian Federation;2021;9,78145E+12;;"https://doi.org/10.1145/3440749.3442605;http://dx.doi.org/10.1145/3440749.3442605";10.1145/3440749.3442605;The size of the network, the number of nodes and connected devices are exponentially increasing due to the development of the Internet of Things (IoT). It becomes difficult to administer the monitoring of heterogeneous networks. It is necessary to use predictive models (Model Predictive Control) to deploy decision support systems related to the IoT network security. The article examines three popular mathematical forecasting methods, evaluates their accuracy and their using possibility in predictive models to solve the problem of assessing the load on the computing power of IoT devices, including servers and services.;;;ICFNDS '20
Conference Paper;Bouhenguel R,Mahgoub I,Llyas M;An Energy Efficient Model for Monitoring and Detecting Atrial Fibrillation in Wearable Computing;;2012;;;59–65;ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering);Brussels, BEL;;Proceedings of the 7th International Conference on Body Area Networks;Oslo, Norway;2012;9,78194E+12;;;;Current portable healthcare monitoring systems are small, battery-operated electrocardiograph devices that are used to record the heart's rhythm and activity. These on-body healthcare devices fall short on delivering real-time continuous monitoring of early detection of cardiac atrial fibrillation (A-Fib) when the symptoms last only a short period of time and require a long battery life. The focus of this paper is the design of an energy efficient model for real-time early detection of A-Fib in a wearable computing device. The design is realized by incorporating an A-Fib risk factor and a real-time A-Fib incidence-based detection algorithm. The results of the design show that the proposed energy efficient model performs better than a telemetry energy model. The design shows promising results in meeting the energy needs of real-time monitoring, detecting and reporting required in wearable computing healthcare applications.;wearable computing, real-time detection of cardiac atrial fibrillation, real-time monitoring, logistic regression model of atrial fibrillation, energy-aware model;;BodyNets '12
Conference Paper;Hasegawa T,Nakai Y,Ohsugi K,Takemasa J,Koizumi Y,Psaras I;Empirically Modeling How a Multicore Software ICN Router and an ICN Network Consume Power;;2014;;;157–166;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 1st ACM Conference on Information-Centric Networking;Paris, France;2014;9,78145E+12;;"https://doi.org/10.1145/2660129.2660142;http://dx.doi.org/10.1145/2660129.2660142";10.1145/2660129.2660142;ICN (Information Centric Networking) has received much attention due to its built-in functionalities such as caching and mobility-support. One of the important research challenges is to reduce the power consumed by ICN networks because ICN's packet forwarding and packet-level caching are power-hungry. As the first step to achieve power-efficient ICN networks, this paper develops a power consumption model of a multicore software ICN router while taking into account the power consumed by power-hungry computation. This paper makes the following three contributions: First, the model is one of the first realistic models which consider ICN packet forwarding and packet-level caching. Second, the model is represented as a concise set of equations with just a few parameters. Third, we apply the model to estimate power consumed by simple networks.;ndn (named data networking), power consumption model, icn (information centric networking), multicore software router, green network;;ACM-ICN '14
Conference Paper;Zhang C,Hindle A;A Green Miner's Dataset: Mining the Impact of Software Change on Energy Consumption;;2014;;;400–403;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 11th Working Conference on Mining Software Repositories;Hyderabad, India;2014;9,78145E+12;;"https://doi.org/10.1145/2597073.2597130;http://dx.doi.org/10.1145/2597073.2597130";10.1145/2597073.2597130;With the advent of mobile computing, the responsibility of software developers to update and ship energy efficient applications has never been more pronounced. Green mining attempts to address this responsibility by examining the impact of software change on energy consumption. One problem with green mining is that power performance data is not readily available, unlike many other forms of MSR research. Green miners have to create tests and run them across numerous versions of a software project because power performance data was either missing or never existed for that particular project. In this paper we describe multiple open green mining datasets used in prior green mining work. The dataset includes numerous power traces and parallel system call and CPU/IO/Memory traces of multiple versions of multiple products. These datasets enable those more interested in data-mining and modeling to work on green mining problems as well.;Software Energy Consumption, Software Change, Dataset;;MSR 2014
Conference Paper;Maio V,Nae V,Prodan R;Evaluating Energy Efficiency of Gigabit Ethernet and Infiniband Software Stacks in Data Centres;;2014;;;21–28;IEEE Computer Society;USA;;Proceedings of the 2014 IEEE/ACM 7th International Conference on Utility and Cloud Computing;;2014;9,78148E+12;;"https://doi.org/10.1109/UCC.2014.10;http://dx.doi.org/10.1109/UCC.2014.10";10.1109/UCC.2014.10;Reducing energy consumption has become a key issue for data centres, not only because of economical benefits but also for environmental and marketing reasons. Many approaches tackle this problem from the point of view of different hardware components, such as CPUs, storage and network interface cards (NIC). To this date, few works focused on the energy consumption of network transfers at the software level comprising their complete stacks with different energy characteristics, and the way the NIC selection impacts the energy consumption of applications. Since data centres often install multiple NICs on each node, investigating and comparing them at the software level has high potential to enhance the energy efficiency of applications on Cloud infrastructures. We present a comparative analysis of the energy consumption of the software stack of two of today's most used NICs in data centres, Ethernet and Infiniband. We carefully design for this purpose a set of benchmark experiments to assess the impact of different traffic patterns and interface settings on energy consumption. Using our benchmark results, we derive an energy consumption model for network transfers and evaluate its accuracy for a virtual machine migration scenario. Finally, we propose guidelines for NIC selection from an energy efficiency perspective for different application classes.;ethernet, energy awareness, green networking, green computing, data centres, infiniband;;UCC '14
Conference Paper;M. Pittman J;DRAT - A Dynamic Resource Allocation Tool for Estimating Compute Power in a Cybersecurity Engineering Learning Facility;;2021;;;39;Association for Computing Machinery;New York, NY, USA;;Proceedings of 5th Conference on Computing Education Practice;Durham, United Kingdom;2021;9,78145E+12;;"https://doi.org/10.1145/3437914.3437980;http://dx.doi.org/10.1145/3437914.3437980";10.1145/3437914.3437980;Cybersecurity laboratory infrastructure has direct impact on the quality of student learning experiences. Because of this, the computing education field has developed a variety of approaches to designing and implementing these learning facilities. Yet, little work has gone into how to properly size cybersecurity laboratory infrastructure relative to student population and curricular compute power demands. The result has been laboratory infrastructures that do not scale with degree programs. Consequently, laboratories are either underpowered, thus limiting learning experiences, or overpowered which wastes financial resources. Accordingly, this work presents DRAT, an open-source software tool, for estimating necessary compute power in a cybersecurity engineering learning facility. More specifically, DRAT is designed to estimate the required discrete compute power on a per exercise basis in a cybersecurity engineering learning facility operating in a private cloud model. Such discrete estimations are intended to communicate physical host hardware requirements such as physical CPU core count, virtual RAM, and total Hard Disk space. The first step in designing DRAT was to forge a model estimator function. Then, we identified a series of scalar abstractions representing learning facility hardware infrastructure and behaving as conversion factors between the model function and output. Because the goal of this work was to provide estimates for cloud compute power requirements, DRAT outputs the number of physical cores, total RAM, total Disk, and total (virtual or physical) Network interfaces required to run the indicated scenario. The implication is that such estimates can inform purchasing and configuration decisions which directly impact student learning outcomes.;;;CEP '21
Journal Article;Nasser Y,Sau C,Prévotet JC,Fanni T,Palumbo F,Hélard M,Raffo L;NeuPow: A CAD Methodology for High-Level Power Estimation Based on Machine Learning;ACM Trans. Des. Autom. Electron. Syst.;2020;25;5;;Association for Computing Machinery;New York, NY, USA;;;;2020-08;;1084-4309;"https://doi.org/10.1145/3388141;http://dx.doi.org/10.1145/3388141";10.1145/3388141;In this article, we present a new, simple, accurate, and fast power estimation technique that can be used to explore the power consumption of digital system designs at an early design stage. We exploit the machine learning techniques to aid the designers in exploring the design space of possible architectural solutions, and more specifically, their dynamic power consumption, which is application-, technology-, frequency-, and data-stimuli dependent. To model the power and the behavior of digital components, we adopt the Artificial Neural Networks (ANNs), while the final target technology is Application Specific Integrated Circuit (ASIC). The main characteristic of the proposed method, called NeuPow, is that it relies on propagating the signals throughout connected ANN models to predict the power consumption of a composite system. Besides a baseline version of the NeuPow methodology that works for a given predefined operating frequency, we also derive an upgraded version that is frequency-aware, where the same operating frequency is taken as additional input by the ANN models. To prove the effectiveness of the proposed methodology, we perform different assessments at different levels. Moreover, technology and scalability studies have been conducted, proving the NeuPow robustness in terms of these design parameters. Results show a very good estimation accuracy with less than 9% of relative error independently from the technology and the size/layers of the design. NeuPow is also delivering a speed-up factor of about 84× with respect to the classical power estimation flow.;modeling, Power consumption, methodology, neural networks, estimation;;
Conference Paper;Bautista E,Romanus M,Davis T,Whitney C,Kubaska T;Collecting, Monitoring, and Analyzing Facility and Systems Data at the National Energy Research Scientific Computing Center;;2019;;;;Association for Computing Machinery;New York, NY, USA;;Workshop Proceedings of the 48th International Conference on Parallel Processing;Kyoto, Japan;2019;9,78145E+12;;"https://doi.org/10.1145/3339186.3339213;http://dx.doi.org/10.1145/3339186.3339213";10.1145/3339186.3339213;As high-performance computing (HPC) resources continue to grow in size and complexity, so too does the volume and velocity of the operational data that is associated with them. At such scales, new mechanisms and technologies are required to continuously gather, store, and analyze this data in near-real time from heterogeneous and distributed sources without impacting the underlying data center operations or HPC resource utilization. In this paper, we describe our experiences in designing and implementing an infrastructure for extreme-scale operational data collection, known as the Operations Monitoring and Notification Infrastructure (OMNI) at the National Energy Research Scientific Computing (NERSC) center at Lawrence Berkeley National Laboratory. OMNI currently holds over 522 billion records of online operational data (totaling over 125TB) and can ingest new data points at an average rate of 25,000 data points per second. Using OMNI as a central repository, facilities and environmental data can be seamlessly integrated and correlated with machine metrics, job scheduler information, network errors, and more, providing a holistic view of data center operations. To demonstrate the value of real-time operational data collection, we present a number of real-world case studies for which having OMNI data readily available led to key operational insights at NERSC. The case results include a reduction in the downtime of an HPC system during a facility transition, as well as a $2.5 million electrical substation savings for the next-generation Perlmutter HPC system.;high-performance computing, time series data, data collection, monitoring, operations, data centers, operational data analytics, Green HPC;;ICPP Workshops '19
Conference Paper;Peši? S,Toši? M,Ikovi? O,Radovanovi? M,Ivanovi? M,Boškovi? D;Bluetooth Low Energy Microlocation Asset Tracking (BLEMAT) in a Context-Aware Fog Computing System;;2018;;;;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 8th International Conference on Web Intelligence, Mining and Semantics;Novi Sad, Serbia;2018;9,78145E+12;;"https://doi.org/10.1145/3227609.3227652;http://dx.doi.org/10.1145/3227609.3227652";10.1145/3227609.3227652;In this paper we present a Bluetooth Low Energy Microlocation Asset Tracking system (BLEMAT) that performs real-time position estimation and asset tracking based on BLE beacons and scanners. It is built on a context-aware fog computing system comprising Internet of Things controllers, sensors and a cloud platform, helped by machine-learning models and techniques. The BLEMAT system offers detecting signal propagation obstacles, performing signal perturbation correction and beacon paths exploration as well as auto discovery and onboarding of fog controller devices. These are the key characteristics of semi-supervised indoor position estimation services. In this paper we have shown there are solid basis that a fog computing system can efficiently carry out semi-supervised machine learning procedures for high-precision indoor position estimation and space modeling without the need for detailed input information (i.e. floor plan, signal propagation map, scanner position). In addition, the fog computing system inherently brings high level of system robustness, integrity, privacy and trust.;space modeling, machine learning, indoor positioning, Fog computing;;WIMS '18
Conference Paper;Noureddine A,Bourdon A,Rouvoy R,Seinturier L;Runtime Monitoring of Software Energy Hotspots;;2012;;;160–169;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering;Essen, Germany;2012;9,78145E+12;;"https://doi.org/10.1145/2351676.2351699;http://dx.doi.org/10.1145/2351676.2351699";10.1145/2351676.2351699;GreenIT has emerged as a discipline concerned with the optimization of software solutions with regards to their energy consumption. In this domain, most of the state-of-the-art solutions concentrate on coarse-grained approaches to monitor the energy consumption of a device or a process. However, none of the existing solutions addresses in-process energy monitoring to provide in-depth analysis of a process energy consumption. In this paper, we therefore report on a fine-grained runtime energy monitoring framework we developed to help developers to diagnose energy hotspots with a better accuracy than the state-of-the-art. Concretely, our approach adopts a 2-layer architecture including OS-level and process-level energy monitoring. OS-level energy monitoring estimates the energy consumption of processes according to different hardware devices (CPU, network card). Process-level energy monitoring focuses on Java-based applications and builds on OS-level energy monitoring to provide an estimation of energy consumption at the granularity of classes and methods. We argue that this per-method analysis of energy consumption provides better insights to the application in order to identify potential energy hotspots. In particular, our preliminary validation demonstrates that we can monitor energy hotspots of Jetty web servers and monitor their variations under stress scenarios.;Bytecode Instrumentation, Profiling, Power Model, Power Monitoring;;ASE '12
Conference Paper;Cano JC,Cano JM,González E,Calafate C,Manzoni P;Evaluation of the Energetic Impact of Bluetooth Low-Power Modes for Ubiquitous Computing Applications;;2006;;;1–8;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 3rd ACM International Workshop on Performance Evaluation of Wireless Ad Hoc, Sensor and Ubiquitous Networks;Terromolinos, Spain;2006;9,7816E+12;;"https://doi.org/10.1145/1163610.1163612;http://dx.doi.org/10.1145/1163610.1163612";10.1145/1163610.1163612;"In order to further increase the applicability of Bluetooth in real applications, reducing the energy consumption and hardware cost are important research topics. In this paper we present a wireless communication prototype to support ubiquitous computing, which has been implemented based on commercial Bluetooth off-the-shelf components. It allows every object to be augmented with processing and communication capabilities in order to make them ""smart"". We investigate on the power characteristics of our Bluetooth prototype which supports the use of low-power modes providing helpful information for protocol developers and software designers. We assess if Bluetooth modules implementing low-power modes can significantly alleviate the power consumption of Bluetooth enabled devices. Our prototype has been used in a museum application to support spontaneous and ubiquitous connections between devices without requiring a priori knowledge of each other";Bluetooth measurements, Bluetooth, ubiquitous computing, power consumption;;PE-WASUN '06
Journal Article;Deiana E,Latouche G,Remiche MA;Fluid Flow Model for Energy-Aware Server Performance Evaluation;SIGMETRICS Perform. Eval. Rev.;2018;45;3;204–209;Association for Computing Machinery;New York, NY, USA;;;;2018-03;;0163-5999;"https://doi.org/10.1145/3199524.3199560;http://dx.doi.org/10.1145/3199524.3199560";10.1145/3199524.3199560;We use a fluid flow model with reactive bounds to analyse a data processing center with energy-aware servers. The servers switch between four energy states depending on the level of the buffer content and on three reactive bounds. Every state consumes different amounts of energy. We use a regenerative approach to calculate the stationary distribution of the system and the expected energy consumption.;;;
Conference Paper;Monni C,Pezzè M;Energy-Based Anomaly Detection a New Perspective for Predicting Software Failures;;2019;;;69–72;IEEE Press;Montreal, Quebec, Canada;;Proceedings of the 41st International Conference on Software Engineering: New Ideas and Emerging Results;;2019;;;"https://doi.org/10.1109/ICSE-NIER.2019.00026;http://dx.doi.org/10.1109/ICSE-NIER.2019.00026";10.1109/ICSE-NIER.2019.00026;The ability of predicting failures before their occurrence is a fundamental enabler for reducing field failures and improving the reliability of complex software systems. Recent research proposes many techniques to detect anomalous values of system metrics, and demonstrates that collective anomalies are a good symptom of failure-prone states.In this paper (i) we observe the analogy of complex software systems with multi-particle and network systems, (ii) propose to use energy-based models commonly exploited in physics and statistical mechanics to precisely reveal failure-prone behaviors without training with seeded errors, and (iii) present some preliminary experimental results that show the feasibility of our approach.;failure prediction, complex systems, anomaly detection;;ICSE-NIER '19
Conference Paper;Dunkels A,Österlind F,Tsiftes N,He Z;Software-Based Sensor Node Energy Estimation;;2007;;;409–410;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 5th International Conference on Embedded Networked Sensor Systems;Sydney, Australia;2007;9,7816E+12;;"https://doi.org/10.1145/1322263.1322319;http://dx.doi.org/10.1145/1322263.1322319";10.1145/1322263.1322319;Being able to estimate the energy consumption of sensor nodes is essential both for evaluating existing sensor network mechanisms and for constructing new energy-aware mechanisms. We present a software-based mechanism for estimating the energy consumption of sensor node at run-time. Unlike previous energy estimation mechanisms, our mechanism does not require any additional hardware components or add-ons.Our demonstration shows the energy estimation in practice on a small network of Tmote Sky motes running the Contiki operating system. A PC connected to one of the motes shows the real-time energy estimation of the network nodes and where the energy is spent: CPU active, CPU sleeping, radio transmitting, radio listening, and LEDs.;energy estimation, Contiki, wireless sensor networks;;SenSys '07
Conference Paper;Blanco-Filgueira B,García-Lesta D,Fernández-Sanjurjo M,Brea VM,López P;Live Demonstration: Deep Learning-Based Multiple Object Detection and Tracking on a Low-Power Embedded System;;2018;;;;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 12th International Conference on Distributed Smart Cameras;Eindhoven, Netherlands;2018;9,78145E+12;;"https://doi.org/10.1145/3243394.3243712;http://dx.doi.org/10.1145/3243394.3243712";10.1145/3243394.3243712;Despite the hopeful prospect for image processing using Convolutional Neural Netwoks, CNNs, the gap between software and hardware solutions is already considerable for embedded applications due to their high power consumption. This demo performs low-power and real time deep learning-based multiple object tracking implemented on an NVIDIA Jetson TX2 development kit. The performance of the proposed algorithm is exemplified under challenging real scenarios, demonstrating the feasibility of deep learning algorithms on embedded platforms.;edge computing, CNN, visual tracking, IoT;;ICDSC '18
Conference Paper;Muttreja A,Raghunathan A,Ravi S,Jha NK;Hybrid Simulation for Embedded Software Energy Estimation;;2005;;;23–26;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 42nd Annual Design Automation Conference;Anaheim, California, USA;2005;9,7816E+12;;"https://doi.org/10.1145/1065579.1065590;http://dx.doi.org/10.1145/1065579.1065590";10.1145/1065579.1065590;Software energy estimation is a critical step in the design of energy-efficient embedded systems. Instruction-level simulation techniques, despite several advances, remain too slow for iterative use in system-level exploration. In this paper, we propose a methodology called hybrid simulation, which combines instruction set simulation with selective native execution (execution of some parts of the program directly on the simulation host computer), thereby overcoming the disadvantages of instruction-level simulation (low speed) and pure native execution (estimation accuracy, inapplicability to target-dependent code), while exploiting their advantages. Previously developed techniques for software energy macromodeling are utilized to estimate energy consumption for natively executed sub-programs. We identify and address the main challenges involved in hybrid simulation, and present an automatic tool flow for it, which analyzes a given program and selects functions for native execution in order to achieve maximum estimation efficiency while limiting estimation error. We have applied the proposed hybrid simulation methodology to a variety of embedded software programs, resulting in an average speed-up of 70% and estimation error of at most 6%, compared to one of the fastest publicly-available instruction set simulators.;hybrid simulation, embedded software, energy macromodels, energy estimation, pointers analysis;;DAC '05
Conference Paper;Sama A,Theeuwen JF,Balakrishnan M;Speeding up Power Estimation of Embedded Software;;2000;;;191–196;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 2000 International Symposium on Low Power Electronics and Design;Rapallo, Italy;2000;9,78158E+12;;"https://doi.org/10.1145/344166.344580;http://dx.doi.org/10.1145/344166.344580";10.1145/344166.344580;Power is increasingly becoming a design constraint for embedded systems. A processor is responsible for energy consumption on account of the software component of the embedded system. The power estimation of this component is a major concern due to the rising complexities of processors and the slow estimation tools. This work attempts to estimate the energy dissipation of the PR1900 processor based on instruction set model with improved accuracy. The model is integrated in a simulation framework and validated. Over 200 times speedup has been obtained with average 1.4% loss in accuracy over gate level estimation. Analysis of the energy dissipated by the instruction vis a vis the processor architecture has been carried out and a substantial reduction in the measurement effort to build the processor energy model has been achieved.;;;ISLPED '00
Conference Paper;Faustine A,Pereira L,Bousbiat H,Kulkarni S;UNet-NILM: A Deep Neural Network for Multi-Tasks Appliances State Detection and Power Estimation in NILM;;2020;;;84–88;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 5th International Workshop on Non-Intrusive Load Monitoring;Virtual Event, Japan;2020;9,78145E+12;;"https://doi.org/10.1145/3427771.3427859;http://dx.doi.org/10.1145/3427771.3427859";10.1145/3427771.3427859;Over the years, an enormous amount of research has been exploring Deep Neural Networks (DNN), particularly Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) for estimating the energy consumption of appliances from a single point source such as smart meters - Non-Intrusive Load Monitoring (NILM). However, most of the existing DNNs models for NILM use a single-task learning approach in which a neural network is trained exclusively for each appliance. This strategy is computationally expensive and ignores the fact that multiple appliances can be active simultaneously and dependencies between them. In this work, we propose UNet-NILM for multi-task appliances' state detection and power estimation, applying a multi-label learning strategy and multi-target quantile regression. The UNet-NILM is a one-dimensional CNN based on the U-Net architecture initially proposed for image segmentation. Empirical evaluation on the UK-DALE dataset suggests promising performance against traditional single-task learning.;Quantile Regression, Non-Intrusive Load Monitoring, Multi-label Classification, Multi-task Learning, Deep Neural Networks, Convolutional Neural Networks, Energy Disaggregation;;NILM'20
Conference Paper;Kothiyal R,Tarasov V,Sehgal P,Zadok E;Energy and Performance Evaluation of Lossless File Data Compression on Server Systems;;2009;;;;Association for Computing Machinery;New York, NY, USA;;Proceedings of SYSTOR 2009: The Israeli Experimental Systems Conference;Haifa, Israel;2009;9,78161E+12;;"https://doi.org/10.1145/1534530.1534536;http://dx.doi.org/10.1145/1534530.1534536";10.1145/1534530.1534536;Data compression has been claimed to be an attractive solution to save energy consumption in high-end servers and data centers. However, there has not been a study to explore this. In this paper, we present a comprehensive evaluation of energy consumption for various file compression techniques implemented in software. We apply various compression tools available on Linux to a variety of data files, and we try them on server class and workstation class systems. We compare their energy and performance results against raw reads and writes. Our results reveal that software based data compression cannot be considered as a universal solution to reduce energy consumption. Various factors like the type of the data file, the compression tool being used, the read-to-write ratio of the workload, and the hardware configuration of the system impact the efficacy of this technique. In some cases, however, we found compression to save substantial energy and improve performance.;performance evaluation, storage, data compression, energy;;SYSTOR '09
Conference Paper;Loseu V,Mannil J,Jafari R;Lightweight Power Aware and Scalable Movement Monitoring for Wearable Computers: A Mining and Recognition Technique at the Fingertip of Sensors;;2011;;;;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 2nd Conference on Wireless Health;San Diego, California;2011;9,78145E+12;;"https://doi.org/10.1145/2077546.2077554;http://dx.doi.org/10.1145/2077546.2077554";10.1145/2077546.2077554;Activity monitoring using Body Sensor Networks(BSN) has gained much attention from the scientific community due to its recreational and medical applications. Suggested techniques for activity monitoring face two major problem. First, systems have to be trained for the individual subjects due to the heterogeneity of the BSN data. While most solutions can address this problem on a small data set, they have no mechanics for automatic scaling of the solution as the data set increases. Second, the battery limitations of the BSN severely limit the maximum deployment time for the continuous monitoring. This problem is often solved by shifting some processing to the local sensor nodes to avoid a very heavy communication cost. However, little work has been done to optimize the sensing and processing cost of the action recognition. In this paper, we propose an action recognition approach based on the BSN repository. We show how the information of a large repository can be automatically used to customize the processing on sensor nodes based on a limited and automated training process. We also investigate the power cost of such a repository mining approach on the sensor nodes based on our implementation. To assess the power requirement, we define an energy model for data sensing and processing. We demonstrate the relationship between the activity recognition precision and the power consumption of the system during continuous action monitoring. We demonstrate the energy effectiveness of our approach with a classification accuracy constraint based on limited data repository.;string templates, data mining, n-grams, power optimization, Patricia tree, body sensor networks;;WH '11
Conference Paper;Brandolese C,Fornaciari W,Pomante L,Salice F,Sciuto D;A Multi-Level Strategy for Software Power Estimation;;2000;;;187–192;IEEE Computer Society;USA;;Proceedings of the 13th International Symposium on System Synthesis;Madrid, Spain;2000;9,78158E+12;;;;In this paper a comprehensive methodology for software power estimation is presented. The methodology is supported by rigorous mathematical models of power consumption at three different levels of abstraction. The methodology has been validated in a complete framework developed within the TOSCA co-design environment.;;;ISSS '00
Conference Paper;Dunkels A,Osterlind F,Tsiftes N,He Z;Software-Based on-Line Energy Estimation for Sensor Nodes;;2007;;;28–32;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 4th Workshop on Embedded Networked Sensors;Cork, Ireland;2007;9,7816E+12;;"https://doi.org/10.1145/1278972.1278979;http://dx.doi.org/10.1145/1278972.1278979";10.1145/1278972.1278979;Energy is of primary importance in wireless sensor networks. By being able to estimate the energy consumption of the sensor nodes, applications and routing protocols are able to make informed decisions that increase the lifetime of the sensor network. However, it is in general not possible to measure the energy consumption on popular sensor node platforms. In this paper, we present and evaluate a software-based on-line energy estimation mechanism that estimates the energy consumption of a sensor node. We evaluate the mechanism by comparing the estimated energy consumption with the lifetime of capacitor-powered sensor nodes. By implementing and evaluating the X-MAC protocol, we show how software-based on-line energy estimation can be used to empirically evaluate the energy efficiency of sensor network protocols.;;;EmNets '07
Conference Paper;Nixon L,Ciesielski K,Philipp B;AI for Audience Prediction and Profiling to Power Innovative TV Content Recommendation Services;;2019;;;42–48;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 1st International Workshop on AI for Smart TV Content Production, Access and Delivery;Nice, France;2019;9,78145E+12;;"https://doi.org/10.1145/3347449.3357485;http://dx.doi.org/10.1145/3347449.3357485";10.1145/3347449.3357485;In contemporary TV audience prediction, outliers are considered mere anomalies in the otherwise cyclical trend and seasonality components that can be used to make predictions. In the ReTV project, we want to provide more accurate audience predictions in order to enable innovative services for TV content recommendation. This paper presents a concept for identifying the source of outliers and factoring TV content categories and the occurrence of events as additional features for training TV audience prediction. We show how this can improve the accuracy of the audience prediction. Finally, we outline how this work could also be combined with AI-enabled audience profiling to power new content recommendation services.;audience forecasting, viewer profiling, tv content recommendation, predictive analytics, prediction, audience segmentation;;AI4TV '19
Journal Article;Wu W,Jin L,Yang J,Liu P,Tan SX;Efficient Power Modeling and Software Thermal Sensing for Runtime Temperature Monitoring;ACM Trans. Des. Autom. Electron. Syst.;2008;12;3;;Association for Computing Machinery;New York, NY, USA;;;;2008-05;;1084-4309;"https://doi.org/10.1145/1255456.1255462;http://dx.doi.org/10.1145/1255456.1255462";10.1145/1255456.1255462;The evolution of microprocessors has been hindered by increasing power consumption and heat dissipation on die. An excessive amount of heat creates reliability problems, reduces the lifetime of a processor, and elevates the cost of cooling and packaging considerably. It is therefore imperative to be able to monitor the temperature variations across the die in a timely and accurate manner.Most current techniques rely on on-chip thermal sensors to report the temperature of the processor. Unfortunately, significant variation in chip temperature both spatially and temporally exposes the limitation of the sensors. We present a compensating approach to tracking chip temperature through an OS resident software module that generates live power and thermal profiles of the processor. We developed such a software thermal sensor (STS) in a Linux system with a Pentium 4 Northwood core. We employed highly efficient numerical methods in our model to minimize the overhead of temperature calculation. We also developed an efficient algorithm for functional unit power modeling. Our power and thermal models are calibrated and validated against on-chip sensor readings, thermal images of the Northwood heat spreader, and the thermometer measurements on the package. The resulting STS offers detailed power and temperature breakdowns of each functional unit at runtime, enabling more efficient online power and thermal monitoring and management at a higher level, such as the operating system.;Power, thermal;;
Conference Paper;Ahlendorf H,Göpfert L;Hardware/Software Design Challenges of Low-Power Sensor Nodes for Condition Monitoring;;2010;;;659;European Design and Automation Association;Leuven, BEL;;Proceedings of the Conference on Design, Automation and Test in Europe;Dresden, Germany;2010;9,78398E+12;;;;Structural Health Monitoring (SHM) is a new challenge in wireless sensor design. In our project we are proposing an ultra-sonic measurement system to find malfunctions within structures. Examples of such structures are aircraft bodies and the wings of a wind turbine. It is required to allow for long term monitoring from a single battery or even using energy scavenging techniques.;;;DATE '10
Conference Paper;Brandolese C,Corbetta S,Fornaciari W;Software Energy Estimation Based on Statistical Characterization of Intermediate Compilation Code;;2011;;;333–338;IEEE Press;Fukuoka, Japan;;Proceedings of the 17th IEEE/ACM International Symposium on Low-Power Electronics and Design;;2011;9,78161E+12;;;;Early estimation of embedded software power consumption is a critical issue that can determine the quality and, sometimes, the feasibility of a system. Architecture-specific, cycle-accurate simulators are valuable tools for fine-tuning performance of critical sections of the application but are often too slow for the simulation of entire systems. This paper proposes a fast and statistically accurate methodology to evaluate the energy performance of embedded software and describes the associated toolchain. The methodology is based on a static characterization of the target instruction set to allow estimation on an equivalent, target-independent intermediate code representation.;instruction-level characterization, software energy estimation;;ISLPED '11
Conference Paper;Scogland TR,Steffen CP,Wilde T,Parent F,Coghlan S,Bates N,Feng WC,Strohmaier E;A Power-Measurement Methodology for Large-Scale, High-Performance Computing;;2014;;;149–159;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 5th ACM/SPEC International Conference on Performance Engineering;Dublin, Ireland;2014;9,78145E+12;;"https://doi.org/10.1145/2568088.2576795;http://dx.doi.org/10.1145/2568088.2576795";10.1145/2568088.2576795;Improvement in the energy efficiency of supercomputers can be accelerated by improving the quality and comparability of efficiency measurements. The ability to generate accurate measurements at extreme scale are just now emerging. The realization of system-level measurement capabilities can be accelerated with a commonly adopted and high quality measurement methodology for use while running a workload, typically a benchmark. This paper describes a methodology that has been developed collaboratively through the Energy Efficient HPC Working Group to support architectural analysis and comparative measurements for rankings, such as the Top500 and Green500. To support measurements with varying amounts of effort and equipment required we present three distinct levels of measurement, which provide increasing levels of accuracy. Level 1 is similar to the Green500 run rules today, a single average power measurement extrapolated from a subset of a machine. Level 2 is more comprehensive, but still widely achievable. Level 3 is the most rigorous of the three methodologies but is only possible at a few sites. However, the Level 3 methodology generates a high quality result that exposes details that the other methodologies may miss. In addition, we present case studies from the Leibniz Supercomputing Centre (LRZ), Argonne National Laboratory (ANL) and Calcul Québec Université Laval that explore the benefits and difficulties of gathering high quality, system-level measurements on large-scale machines.;green500, top500, high-performance computing, datacenter, power-measurement methodology;;ICPE '14
Conference Paper;Kang K,Kim J,Shim H,Kyung CM;Software Power Estimation Using IPI(Inter-Prefetch Interval) Power Model for Advanced off-the-Shelf Processor;;2007;;;594–599;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 17th ACM Great Lakes Symposium on VLSI;Stresa-Lago Maggiore, Italy;2007;9,7816E+12;;"https://doi.org/10.1145/1228784.1228923;http://dx.doi.org/10.1145/1228784.1228923";10.1145/1228784.1228923;This paper addresses a problem of modeling the power consumption of advanced off-the-shelf processors. Unlike existing methods for processor power estimation, where the internal information of processor architecture such as activation of specfic modules such as pipeline stages, etc.) is available via simulation or runtime counters, power modeling method presented in this paper is to estimate the power consumption of complex off-the-shelf RISC processor without such a detailed information, only based on the information of the processor I/O signals (i.e memory access). To tackle this problem, we propose a new power model, called IPI(Instruction-Prefetch Interval) power model. The IPI represents the time interval between two consecutive instruction prefetchs. Our model has two major advantages. First, this model can consider prefetch mechanism. Most of advanced RISC processors have prefetch mechanism which makes processor power estimation difficult. IPI model is the first approach to model prefetch mechanism in processor power estimation. Second, this model can provide power variation in time and therefore it overcomes the limitation of previous work, such as instruction-level energy model. Experiments show that the proposed model yields 96% accuracy on theaverage in case of ARM1136JF-S test chip.;black-box power model, software power estimation, prefetch power model, processor power modeling, IPI power model;;GLSVLSI '07
Conference Paper;Milosevic M,Dzhagaryan A,Jovanov E,Milenkovi? A;An Environment for Automated Power Measurements on Mobile Computing Platforms;;2013;;;;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 51st ACM Southeast Conference;Savannah, Georgia;2013;9,78145E+12;;"https://doi.org/10.1145/2498328.2500064;http://dx.doi.org/10.1145/2498328.2500064";10.1145/2498328.2500064;Mobile computing devices such as smartphones, tablet computers, and e-readers have become the dominant personal computing platforms. Energy efficiency is a prime design requirement for mobile device manufacturers and smart application developers alike. Runtime power measurements on mobile platforms provide insights that can eventually lead to more energy-efficient operation. In this paper we describe mPowerProfile - an environment for automated power measurements of programs running on a mobile development platform. We discuss mPowerProfile's main functions and its utilization in several example studies based on the Pandaboard and Raspberry Pi platforms.;energy-efficiency, power profiling;;ACMSE '13
Conference Paper;Aasaraai K,Baniasadi A,Atoofian E;Computational and Storage Power Optimizations for the O-GEHL Branch Predictor;;2007;;;105–112;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 4th International Conference on Computing Frontiers;Ischia, Italy;2007;9,7816E+12;;"https://doi.org/10.1145/1242531.1242549;http://dx.doi.org/10.1145/1242531.1242549";10.1145/1242531.1242549;In recent years, highly accurate branch predictors have been proposed primarily for high performance processors. Unfortunately such predictors are extremely energy consuming and in some cases not practical as they come with excessive prediction latency. One example of such predictors is the O-GEHL predictor. To achieve high accuracy, O-GEHL relies on large tables and extensive computations and requires high energy and long prediction delay.In this work we propose power optimization techniques that aim at reducing both computational complexity and storage size for the O-GEHL predictor. We show that by eliminating unnecessary data from computations, we can reduce both predictor's energy consumption and delay. Moreover, we apply information theory findings to remove redundant storage, without any significant accuracy penalty. We reduce the dynamic and static power dissipated in the computational parts of the predictor by up to 74% and 65% respectively. Meantime we improve performance by up to 12% as we make faster prediction possible.;branch prediction, power-aware microarchitectures, O-GEHL;;CF '07
Conference Paper;Metz CA,Goli M,Drechsler R;Early Power Estimation of CUDA-Based CNNs on GPGPUs: Work-in-Progress;;2021;;;29–30;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 2021 International Conference on Hardware/Software Codesign and System Synthesis;Virtual Event;2021;9,78145E+12;;"https://doi.org/10.1145/3478684.3479255;http://dx.doi.org/10.1145/3478684.3479255";10.1145/3478684.3479255;The increasing application of Machine Learning (ML) techniques in the Internet of Things (IoT) devices has led designers to leverage ML accelerators like GPGPUs in such devices. However, choosing the most appropriate accelerator for such IoT devices is very challenging as they commonly should adhere to tight constraints e.g., low power consumption, long battery lifetime, and low cost of the final products. As a consequence, designing such application-specific IoT devices becomes a non-trivial and difficult task. In this paper, we present a novel approach to estimate power consumption of CUDA-based Convolutional Neural Networks (CNNs) on GPGPUs in the design phase. Our approach is able to provide designers with an early prediction of CNNs power consumption up to an absolute error of less than 2% in comparison to the real hardware execution.;;;CODES/ISSS '21
Conference Paper;Aggarwal K,Zhang C,Campbell JC,Hindle A,Stroulia E;The Power of System Call Traces: Predicting the Software Energy Consumption Impact of Changes;;2014;;;219–233;IBM Corp.;USA;;Proceedings of 24th Annual International Conference on Computer Science and Software Engineering;Markham, Ontario, Canada;2014;;;;;Battery is a critical resource for smartphones. Software developers as the builders and maintainers of applications, are responsible for updating and deploying energy efficient applications to end users. Unfortunately, the impact of software change on energy consumption is still unclear. Estimation based on software metrics has proved difficult. As energy consumption profiling requires special infrastructure, developers have difficulty assessing the impact of their actions on energy consumption. System calls are the interface between applications and the OS kernel and provide insight into how software utilizes hardware and software resources. As profiling system calls requires no specialized infrastructure, unlike energy consumption, it is much easier for the developers to track changes to system calls. Thus we relate software change to energy consumption by tracing the changes in an application's pattern of system call invocations. We find that significant changes to system call profiles often induce significant changes in energy consumption.;;;CASCON '14
Conference Paper;Metz CA,Goli M,Drechsler R;Towards Neural Hardware Search: Power Estimation of CNNs for GPGPUs with Dynamic Frequency Scaling;;2022;;;103–109;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 2022 ACM/IEEE Workshop on Machine Learning for CAD;Virtual Event, China;2022;9,78145E+12;;"https://doi.org/10.1145/3551901.3556481;http://dx.doi.org/10.1145/3551901.3556481";10.1145/3551901.3556481;Machine Learning (ML) algorithms are essential for emerging technologies such as autonomous driving and application-specific Internet of Things(IoT) devices. Convolutional Neural Network(CNN) is one of the major techniques used in such systems. This leads to leveraging ML accelerators like GPGPUs to meet the design constraints. However, GPGPUs have high power consumption, and selecting the most appropriate accelerator requires Design Space Exploration(DSE), which is usually time-consuming and needs high manual effort. Neural Hardware Search(NHS) is an upcoming approach to automate the DSE for Neural Networks. Therefore, automatic approaches for power, performance, and memory estimations are needed.In this paper, we present a novel approach, enabling designers to fast and accurately estimate the power consumption of CNNs inferencing on GPGPUs with Dynamic Frequency Scaling(DFS) in the early stages of the design process. The proposed approach uses static analysis for feature extraction and Random Forest Tree regression analysis for predictive model generation. Experimental results demonstrate that our approach can predict the CNNs power consumption with a Mean Absolute Percentage Error(MAPE) of 5.03% compared to the actual hardware.;machine learning, GPGPU, power estimation, neural hardware search, CNN;;MLCAD '22
Conference Paper;Kiertscher S,Schnor B;Scalability Evaluation of an Energy-Aware Resource Management System for Clusters of Web Servers;;2015;;;1–8;Society for Computer Simulation International;San Diego, CA, USA;;Proceedings of the International Symposium on Performance Evaluation of Computer and Telecommunication Systems;Chicago, Illinois;2015;9,78151E+12;;;;For green cluster computing resource management systems have to be energy-aware. CHERUB is such an energy-aware resource management system which works together with the Linux Virtual Server. Experiments in a small cluster setup with two nodes have shown the benefit of CHERUB. This paper presents necessary design changes to make CHERUB also work in big cluster setups. Our methodological approach is two-fold. First, we present unit measurements to evaluate the scaling of the re-implemented functions. Second, a cluster simulator is presented and validated which makes it possible to test CHERUB for backend clusters of arbitrary size.;cluster computing, energy efficiency, scalability studies, green-IT, energy awareness, simulation, integrated modeling and measurement;;Spects '15
Journal Article;Zhao Y,Li S,Hu S,Wang H,Yao S,Shao H,Abdelzaher T;An Experimental Evaluation of Datacenter Workloads on Low-Power Embedded Micro Servers;Proc. VLDB Endow.;2016;9;9;696–707;VLDB Endowment;;;;;2016-05;;2150-8097;"https://doi.org/10.14778/2947618.2947625;http://dx.doi.org/10.14778/2947618.2947625";10.14778/2947618.2947625;This paper presents a comprehensive evaluation of an ultra-low power cluster, built upon the Intel Edison based micro servers. The improved performance and high energy efficiency of micro servers have driven both academia and industry to explore the possibility of replacing conventional brawny servers with a larger swarm of embedded micro servers. Existing attempts mostly focus on mobile-class micro servers, whose capacities are similar to mobile phones. We, on the other hand, target on sensor-class micro servers, which are originally intended for uses in wearable technologies, sensor networks, and Internet-of-Things. Although sensor-class micro servers have much less capacity, they are touted for minimal power consumption (< 1 Watt), which opens new possibilities of achieving higher energy efficiency in datacenter workloads. Our systematic evaluation of the Edison cluster and comparisons to conventional brawny clusters involve careful workload choosing and laborious parameter tuning, which ensures maximum server utilization and thus fair comparisons. Results show that the Edison cluster achieves up to 3.5x improvement on work-done-per-joule for web service applications and data-intensive MapReduce jobs. In terms of scalability, the Edison cluster scales linearly on the throughput of web service workloads, and also shows satisfactory scalability for MapReduce workloads despite coordination overhead.;;;
Conference Paper;Amsel N,Tomlinson B;Green Tracker: A Tool for Estimating the Energy Consumption of Software;;2010;;;3337–3342;Association for Computing Machinery;New York, NY, USA;;CHI '10 Extended Abstracts on Human Factors in Computing Systems;Atlanta, Georgia, USA;2010;9,78161E+12;;"https://doi.org/10.1145/1753846.1753981;http://dx.doi.org/10.1145/1753846.1753981";10.1145/1753846.1753981;The energy consumption of computers has become an important environmental issue. This paper describes the development of Green Tracker, a tool that estimates the energy consumption of software in order to help concerned users make informed decisions about the software they use. We present preliminary results gathered from this system's initial usage. Ultimately the information gathered from this tool will be used to raise awareness and help make the energy consumption of software a more central concern among software developers.;green computing, software, sustainability, green it;;CHI EA '10
Conference Paper;Huijgens H,Lamping R,Stevens D,Rothengatter H,Gousios G,Romano D;Strong Agile Metrics: Mining Log Data to Determine Predictive Power of Software Metrics for Continuous Delivery Teams;;2017;;;866–871;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering;Paderborn, Germany;2017;9,78145E+12;;"https://doi.org/10.1145/3106237.3117779;http://dx.doi.org/10.1145/3106237.3117779";10.1145/3106237.3117779;ING Bank, a large Netherlands-based internationally operating bank, implemented a fully automated continuous delivery pipe-line for its software engineering activities in more than 300 teams, that perform more than 2500 deployments to production each month on more than 750 different applications. Our objective is to examine how strong metrics for agile (Scrum) DevOps teams can be set in an iterative fashion. We perform an exploratory case study that focuses on the classification based on predictive power of software metrics, in which we analyze log data derived from two initial sources within this pipeline. We analyzed a subset of 16 metrics from 59 squads. We identified two lagging metrics and assessed four leading metrics to be strong.;Software Economics, Continuous Delivery, Data Mining, Agile Metrics, DevOps, Prediction Modelling, Scrum, Software Analytics;;ESEC/FSE 2017
Conference Paper;Estrin G,Muntz RR,Uzgalis RC;Modeling, Measurement and Computer Power;;1971;;;725–738;Association for Computing Machinery;New York, NY, USA;;Proceedings of the May 16-18, 1972, Spring Joint Computer Conference;Atlantic City, New Jersey;1971;9,78145E+12;;"https://doi.org/10.1145/1478873.1478967;http://dx.doi.org/10.1145/1478873.1478967";10.1145/1478873.1478967;"Since the early 1960s the literature reveals increasing concern with effectiveness of information processing systems and our ability to predict influences of system parameters. A recent survey paper discusses methods of performance evaluation related to three practical goals: selection of the best among several existing systems; design of a not-yet existing system; and analysis of an existing accessible system. The classification of goals is useful, but we can point to neither the models nor the measures nor the measurement tools to allow reliable judgments with respect to those three important goals at this time.";;;AFIPS '72 (Spring)
Conference Paper;Nguyen QH,Dressler F;The Accuracy of Android Energy Measurements for Offloading Computational Expensive Tasks: Poster;;2016;;;393–394;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 17th ACM International Symposium on Mobile Ad Hoc Networking and Computing;Paderborn, Germany;2016;9,78145E+12;;"https://doi.org/10.1145/2942358.2942412;http://dx.doi.org/10.1145/2942358.2942412";10.1145/2942358.2942412;Computational offloading from smartphones into the cloud has proved to be one of useful approaches for improving energy efficiency. To assess the benefit of offloading and decide the most suitable offloading strategies, it is essential to account the energy consumption of smartphones. In this paper, we investigate the accuracy and capabilities of the Android smart battery interface. For comparison, we measure the energy consumption using an oscilloscope. We experimentally investigate the energy consumption of different applications on a modern smartphones including local computation and network communication over WiFi. Our results show that both of methods bring high accuracy. Our work builds the basis for next generation offloading algorithms.;offloading, energy measurement, smartphone;;MobiHoc '16
Conference Paper;Caiazza C,Luconi V,Vecchio A;Saving Energy on Smartphones through Edge Computing: An Experimental Evaluation;;2022;;;20–25;Association for Computing Machinery;New York, NY, USA;;Proceedings of the ACM SIGCOMM Workshop on Networked Sensing Systems for a Sustainable Society;Amsterdam, Netherlands;2022;9,78145E+12;;"https://doi.org/10.1145/3538393.3544935;http://dx.doi.org/10.1145/3538393.3544935";10.1145/3538393.3544935;Edge computing is a network architecture in which computing and storage capabilities are moved at the fringes of the Internet, close to the end-users. The main goal of edge computing is to enable responsive services, thanks to much shorter paths compared to the ones encountered when communicating with remotely positioned cloud servers. In this paper, we report experimental results concerning an overlooked benefit of edge computing: energy is saved on client devices. We carried out an experimental evaluation using both software-based and hardware-based energy estimation methods. Results show that, for HTTP-based communication, the lifetime of a device can be extended significantly when using the edge instead of a remote cloud.;energy consumption, edge computing, LTE;;NET4us '22
Conference Paper;Rodriguez Arreola A,Balsamo D,Das AK,Weddell AS,Brunelli D,Al-Hashimi BM,Merrett GV;Approaches to Transient Computing for Energy Harvesting Systems: A Quantitative Evaluation;;2015;;;3–8;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 3rd International Workshop on Energy Harvesting & Energy Neutral Sensing Systems;Seoul, South Korea;2015;9,78145E+12;;"https://doi.org/10.1145/2820645.2820652;http://dx.doi.org/10.1145/2820645.2820652";10.1145/2820645.2820652;Systems operating from harvested sources typically integrate batteries or supercapacitors to smooth out rapid changes in harvester output. However, such energy storage devices require time for charging and increase the size, mass and cost of the system. A recent approach to address this is to power systems directly from the harvester output, termed transient computing. To solve the problem of having to restart computation from the start due to power-cycles, a number of techniques have been proposed to deal with transient power sources. In this paper, we quantitatively evaluate three state-of-the-art approaches on a Texas Instruments MSP430 microcontroller characterizing the application scenarios where each performs best. Finally, recommendations are provided to system designers for selecting the most suitable approach.;wind turbines, quickrecall, IoT, transient computing, checkpoint, energy harvesting, photo voltaic cells, mementos, hibernus;;ENSsys '15
Conference Paper;Symeonidis AL,Gountis VP,Andreou GT;A Software Agent Framework for Exploiting Demand-Side Consumer Social Networks in Power Systems;;2011;;;30–33;IEEE Computer Society;USA;;Proceedings of the 2011 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology - Volume 02;;2011;9,78077E+12;;"https://doi.org/10.1109/WI-IAT.2011.245;http://dx.doi.org/10.1109/WI-IAT.2011.245";10.1109/WI-IAT.2011.245;This work introduces Energy City, a multi-agent framework designed and developed in order to simulate the power system and explore the potential of Consumer Social Networks (CSNs) as a means to promote demand-side response and raise social awareness towards energy consumption. The power system with all its involved actors (Consumers, Producers, Electricity Suppliers, Transmission and Distribution Operators) and their requirements are modeled. The semantic infrastructure for the formation and analysis of electricity CSNs is discussed, and the basic consumer attributes and CSN functionality are identified. Authors argue that the formation of such CSNs is expected to increase the electricity consumer market power by enabling them to act in a collective way.;Consumer Social Networks, Power system simulation framework, Multiagent modeling;;WI-IAT '11
Conference Paper;Verdecchia R,Procaccianti G,Malavolta I,Lago P,Koedijk J;Estimating Energy Impact of Software Releases and Deployment Strategies: The KPMG Case Study;;2017;;;257–266;IEEE Press;Markham, Ontario, Canada;;Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement;;2017;9,78151E+12;;"https://doi.org/10.1109/ESEM.2017.39;http://dx.doi.org/10.1109/ESEM.2017.39";10.1109/ESEM.2017.39;Background. Often motivated by optimization objectives, software products are characterized by different subsequent releases and deployed through different strategies. The impact of these two aspects of software on energy consumption has still to be completely understood and can be improved by carrying out ad-hoc analyses for specific software products.Aims. In this research we report on an industrial collaboration aiming at assessing the different impact that releases and deployment strategies of a software product can have on the energy consumption of its underlying hardware infrastructure.Method. We designed and performed an empirical experiment in a controlled environment. Deployment strategies, releases and use case scenarios of an industrial third-party software product were adopted as experimental factors. The use case scenarios were used as a blocking factor and adopted to dynamically load-test the software product. Power consumption and execution time were selected as response variables to measure the energy consumption.Results. We observed that both deployment strategies and software releases significantly influence the energy consumption of the hardware infrastructure. A strong interaction between the two factors was identified. The impact of such interaction highly varied depending on which use case scenario was considered, making the identification of the most frequently adopted use case scenario critical for energy optimisation. The collaboration between industry and academia has been productive for both parties, even if some practitioners manifested low interest/awareness on software energy efficiency.Conclusions. For the software product considered there is no absolute preferable release or deployment strategy with respect to energy efficiency, as the interaction of these factors has to be considered. The number of machines involved in a software deployment strategy does not simply constitute an additive effect of the energy consumption of the underlying hardware infrastructure.;energy, software releases, deployment;;ESEM '17
Conference Paper;Großschädl J,Tillich S,Rechberger C,Hofmann M,Medwed M;Energy Evaluation of Software Implementations of Block Ciphers under Memory Constraints;;2007;;;1110–1115;EDA Consortium;San Jose, CA, USA;;Proceedings of the Conference on Design, Automation and Test in Europe;Nice, France;2007;9,78398E+12;;;;"Software implementations of modern block ciphers often require large lookup tables along with code size increasing optimizations like loop unrolling to reach peak performance on general-purpose processors. Therefore, block ciphers are difficult to implement efficiently on embedded devices like cell phones or sensor nodes where run-time memory and program ROM are scarce resources. In this paper we analyze and compare the performance, energy consumption, run-time memory requirements, and code size of the five block ciphers RC6, Rijndael, Serpent, Twofish, and XTEA on the StrongARM SA-1100 processor. Most previous evaluations of block ciphers considered performance as the sole metric of interest and did not care about memory requirements or code size. In contrast to previous work, our study of the performance and energy characteristics of block ciphers has been conducted with ""lightweight"" implementations which restrict the size of lookup tables to 1 kB and also impose constraints on the code size. We found that Rijndael and RC6 can be well optimized for high performance and energy efficiency, while at the same time meeting the demand for low memory (RAM and ROM) footprint. In addition, we discuss the impact of key expansion and modes of operation on the overall performance and energy consumption of each block cipher. Our simulation results show that RC6 is the most energy-efficient block cipher under memory constraints and thus the best choice for resource-restricted devices.";code size reduction, energy optimization, lightweight cryptography, memory footprint, symmetric cipher;;DATE '07
Conference Paper;Qu Y,Zheng Q,Liu T,Li J,Guan X;In-Depth Measurement and Analysis on Densification Power Law of Software Execution;;2014;;;55–58;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 5th International Workshop on Emerging Trends in Software Metrics;Hyderabad, India;2014;9,78145E+12;;"https://doi.org/10.1145/2593868.2593878;http://dx.doi.org/10.1145/2593868.2593878";10.1145/2593868.2593878;Measuring software execution is important for many software engineering tasks. In this paper, Densification Power Law (DPL) of software execution is measured and studied as a feature of growing software complexity. Densification means that during a networked system's evolution, it usually becomes denser and the number of edges and nodes grows with a consistent super linear relation. This feature was discovered and reported in 2005. In this paper, based on a measurement of 15 open-source Java programs, we show that when software systems are modeled as a series of dynamic Call Graphs during their executions, they always obey DPL with very close correlation. Then a comparison between static Call Graph and DPL is presented, showing that DPL's properties cannot be derived statically. An explanation for DPL of software execution is given and verified. We believe the universality of DPL makes it an appropriate metric for software execution process.;Software metrics, Densification Power Law, software execution;;WETSoM 2014
Conference Paper;Schubert K;The Evaluation of Ada Software to Support the Space Station Power Management and Distribution System;;1989;;;344–362;Association for Computing Machinery;New York, NY, USA;;Proceedings of the Conference on TRI-Ada '88;Charleston, West Virginia, USA;1989;9,7809E+12;;"https://doi.org/10.1145/76619.77034;http://dx.doi.org/10.1145/76619.77034";10.1145/76619.77034;;;;TRI-Ada '88
Conference Paper;Couto M,Maia D,Saraiva J,Pereira R;On Energy Debt: Managing Consumption on Evolving Software;;2020;;;62–66;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 3rd International Conference on Technical Debt;Seoul, Republic of Korea;2020;9,78145E+12;;"https://doi.org/10.1145/3387906.3388628;http://dx.doi.org/10.1145/3387906.3388628";10.1145/3387906.3388628;"This paper introduces the concept of energy debt: a new metric, reflecting the implied cost in terms of energy consumption over time, of choosing a flawed implementation of a software system rather than a more robust, yet possibly time consuming, approach. A flawed implementation is considered to contain code smells, known to have a negative influence on the energy consumption.Similar to technical debt, if energy debt is not properly addressed, it can accumulate an energy ""interest"". This interest will keep increasing as new versions of the software are released, and eventually reach a point where the interest will be higher than the initial energy debt. Addressing the issues/smells at such a point can remove energy debt, at the cost of having already consumed a significant amount of energy which can translate into high costs. We present all underlying concepts of energy debt, bridging the connection with the existing concept of technical debt and show how to compute the energy debt through a motivational example.";code analysis, green software, energy debt;;TechDebt '20
Conference Paper;Chowdhury SA,Hindle A;GreenOracle: Estimating Software Energy Consumption with Energy Measurement Corpora;;2016;;;49–60;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 13th International Conference on Mining Software Repositories;Austin, Texas;2016;9,78145E+12;;"https://doi.org/10.1145/2901739.2901763;http://dx.doi.org/10.1145/2901739.2901763";10.1145/2901739.2901763;Software energy consumption is a relatively new concern for mobile application developers. Poor energy performance can harm adoption and sales of applications. Unfortunately for the developers, the measurement of software energy consumption is expensive in terms of hardware and difficult in terms of expertise. Many prior models of software energy consumption assume that developers can use hardware instrumentation and thus cannot evaluate software running within emulators or virtual machines. Some prior models require actual energy measurements from the previous versions of applications in order to model the energy consumption of later versions of the same application.In this paper, we take a big-data approach to software energy consumption and present a model that can estimate software energy consumption mostly within 10% error (in joules) and does not require the developer to train on energy measurements of their own applications. This model leverages a big-data approach whereby a collection of prior applications' energy measurements allows us to train, transmit, and apply the model to estimate any foreign application's energy consumption for a test run. Our model is based on the dynamic traces of system calls and CPU utilization.;;;MSR '16
Conference Paper;Gordon ML;Harnessing Disagreement to Create AI-Powered Systems That Reflect Our Values;;2021;;;171–174;Association for Computing Machinery;New York, NY, USA;;Adjunct Proceedings of the 34th Annual ACM Symposium on User Interface Software and Technology;Virtual Event, USA;2021;9,78145E+12;;"https://doi.org/10.1145/3474349.3477590;http://dx.doi.org/10.1145/3474349.3477590";10.1145/3474349.3477590;How do we build artificial intelligence systems that reflect our values? Competing potential values we may want to choose from are, at their core, made up of disagreements between individual people. But while the raw datasets that most ML systems rely on are made up of individuals, today’s approaches to building AI typically abstract the individuals out of the pipeline. My thesis contributes a set of algorithms and interactive systems that re-imagine the pipeline for designing and evaluating AI systems, requiring that we deal with competing values in an informed and intentional way. I start from the insight that at each stage of the pipeline, we need to treat individuals as the key unit of operation, rather than the abstractions or aggregated pseudo-humans in use by today’s approaches. I instantiate this insight to address two problems that arise from today’s AI pipeline. First, evaluation metrics produce actively misleading scores about the extent to which some people’s values are being reflected. I introduce a mathematical transformation that more closely aligns metrics with the values and methods of user-facing performance measures. Second, the resulting AI systems either surreptitiously choose which values to listen to without input from users, or simply present several different outcomes to users without mechanisms to help them select an outcome grounded in their values. I propose a new interaction paradigm for deploying classifiers, asking users to compose a jury consisting of the individual people they’d like their classifiers’ decisions to come from.;;;UIST '21 Adjunct
Conference Paper;Ko J,Lee J,Choi YJ;Poster: A Novel Computation Offloading Technique for Reducing Energy Consumption of Smart Watch;;2016;;;46;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 14th Annual International Conference on Mobile Systems, Applications, and Services Companion;Singapore, Singapore;2016;9,78145E+12;;"https://doi.org/10.1145/2938559.2948825;http://dx.doi.org/10.1145/2938559.2948825";10.1145/2938559.2948825;;smartphone, smart watch, cloud server, computation offloading;;MobiSys '16 Companion
Conference Paper;Brady TF;Energy Production and Trading: Using Computer Simulation to Mitigate Risk in Electricity Generation/Consumption Collaboration Policies;;2002;;;1575–1577;Winter Simulation Conference;San Diego, California;;Proceedings of the 34th Conference on Winter Simulation: Exploring New Frontiers;;2002;9,78078E+12;;;;The electric utility industry has undergone fundamental change in the last decade. Foremost of these changes have been numerous deregulation attempts. Producers and large consumers have built business models based upon large volume transactions, which lead to smooth production and volume discounting. The risks associated with using these traditional business models in deregulated markets are many. This paper describes the development of a computer simulation environment that models a novel collaborative strategy proposed by a local electricity utility to mitigate highly varying load situations demanded by the largest steel-producing region in the United States. Through the use of this model, collaborative strategies for effective electricity generation and usage are developed and analyzed.;;;WSC '02
Conference Paper;Shim JS,Han B,Kim Y,Kim J;DeepPM: Transformer-Based Power and Performance Prediction for Energy-Aware Software;;2022;;;1491–1496;European Design and Automation Association;Leuven, BEL;;Proceedings of the 2022 Conference & Exhibition on Design, Automation & Test in Europe;Antwerp, Belgium;2022;9,78398E+12;;;;Many system-level management and optimization techniques need accurate estimates of power consumption and performance. Earlier research has proposed many high-level/source-level estimation modeling works, particularly for basic blocks. However, most of them still need to execute the target software at least once on a fine-grained simulator or real hardware to extract required features. This paper proposes a performance/power prediction framework, called Deep Power Meter (DeepPM), which estimates them accurately only using the compiled binary. Inspired by the deep learning techniques in natural language processing, we convert the program instructions in the form of vectors and predict the average power and performance of basic blocks based on a transformer model. In addition, unlike existing works based on a Long Short-Term Memory (LSTM) model structure, which only works for basic blocks with a small number of instructions, DeepPM provides highly accurate results for long basic blocks, which takes the majority of the execution time for actual application runs. In our evaluation conducted with SPEC2006 benchmark suite, we show that DeepPM can provide accurate prediction for performance and power consumption with 10.2% and 12.3% error, respectively. DeepPM also outperforms the LSTM-based model by up to 67.2% and 34.9% error for performance and power, respectively.;system resource prediction, transformer, power and performance modeling;;DATE '22
Conference Paper;Tian M,Vishwanath A,Venkataramani G,Subramaniam S;SpinSmart: Exploring Optimal Server Fan Speeds to Improve Overall System Energy Consumption;;2020;;;474–481;Association for Computing Machinery;New York, NY, USA;;Proceedings of the Eleventh ACM International Conference on Future Energy Systems;Virtual Event, Australia;2020;9,78145E+12;;"https://doi.org/10.1145/3396851.3402655;http://dx.doi.org/10.1145/3396851.3402655";10.1145/3396851.3402655;"Cost of data centers has risen sharply in the past few years. Today, it represents about 3% of total US energy consumption with projections to increase further in the coming years. In this paper, we focus on the server infrastructure and observe that workload consolidation techniques, which maximize power efficiency of server systems, do not automatically optimize the overall system power efficiency especially when compute engines and the corresponding on-board cooling systems are considered holistically. We design SpinSmart, a framework that explores optimal server fan speeds to minimize the overall system energy consumption. We explore core capping strategies that estimate the desired number of CPU cores to be used at any given time to minimize combined CPU+fan power. Our experimental results show that we are able to achieve 1) energy savings of up to 10% of total energy and 80% of cooling energy when compared to workload consolidation without core capping strategy; 2) cooling energy savings up to 42% when compared to the strategy that randomly assigns jobs to all the servers and cores.";Energy optimization, Fan speed control, Data centers;;e-Energy '20
Conference Paper;Reif S,Herzog B,Hemp J,Schröder-Preikschat W,Hönig T;AI Waste Prevention: Time and Power Estimation for Edge Tensor Processing Units: Poster;;2021;;;300–301;Association for Computing Machinery;New York, NY, USA;;Proceedings of the Twelfth ACM International Conference on Future Energy Systems;Virtual Event, Italy;2021;9,78145E+12;;"https://doi.org/10.1145/3447555.3466579;http://dx.doi.org/10.1145/3447555.3466579";10.1145/3447555.3466579;"Artificial Intelligence (AI) has changed our daily lives. The evolution from centralised cloud-hosted services towards embedded and mobile devices has shifted the focus from quality-related aspects towards the resource demand of machine learning. Its pervasiveness demands for ""green"" AI---both the development and the operation of AI models still include significant resource investments in terms of processing time and power demand. In order to prevent such AI Waste, this paper presents Precious, an approach, as well as practical implementation, that estimates execution time and power draw of neural networks (NNs) that execute on a commercially-available off-the-shelf accelerator hardware (i.e., Google Coral Edge TPU). The evaluation of our implementations shows that Precious accurately estimates time and power demand.";Resource Awareness, Green AI, Neural Network Accelerators;;e-Energy '21
Conference Paper;Akhlaghi V,Gao S,Gupta RK;LEMAX: Learning-Based Energy Consumption Minimization in Approximate Computing with Quality Guarantee;;2018;;;;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 55th Annual Design Automation Conference;San Francisco, California;2018;9,78145E+12;;"https://doi.org/10.1145/3195970.3196069;http://dx.doi.org/10.1145/3195970.3196069";10.1145/3195970.3196069;Approximate computing aims to trade accuracy for energy efficiency. Various approximate methods have been proposed in the literature that demonstrate the effectiveness of relaxing accuracy requirements in a specific unit. This provides a basis for exploring simultaneous use of multiple approximate units to improve efficiency under guarantees on quality of results. In this paper, we explore the effect of combining multiple approximate units on the energy consumption and identify the best setting that minimizes energy consumption under a quality constraint. Our approach also enables changes in unit configurations throughout the program. To do this effectively, we need a method to examine the combined impact of multiple approximate units on the output quality, and configure individual units accordingly. To solve this problem, we propose LEMAX that uses gradient descent approach to identify the best configuration of the individual approximate units for a given program. We evaluate the efficacy of LEMAX in minimizing the energy consumption of several machine learning applications with varying size (i.e., number of operations) under different quality constraints. Our evaluation shows that the configuration provided by LEMAX for a system with multiple approximate units improves the energy consumption by on average, 97.7%, 83.12%, and 73.95% for quality loss of 5%, 2% and 0.5%, respectively, compared to configurations obtained for a system with a single approximate resource.;approximate computing, design automation, machine learning;;DAC '18
Conference Paper;Evans MW,Picinich LM;Power: A Tool for Quantitative Evaluation of Software Project Effectiveness;;1984;;;138–142;IEEE Press;Orlando, Florida, USA;;Proceedings of the 7th International Conference on Software Engineering;;1984;9,78082E+12;;;;In this paper a tool for quantitative method of evaluating software projects is presented. This method constitutes the Project Observation Workbench and Evaluation Reportor (POWER). A representative project evaluation with POWER is presented to demonstrate its application.;;;ICSE '84
Conference Paper;Yu Y,Bhatti SN;The Cost of Virtue: Reward as Well as Feedback Are Required to Reduce User ICT Power Consumption;;2014;;;157–169;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 5th International Conference on Future Energy Systems;Cambridge, United Kingdom;2014;9,78145E+12;;"https://doi.org/10.1145/2602044.2602063;http://dx.doi.org/10.1145/2602044.2602063";10.1145/2602044.2602063;"We show that students in a school lab environment will change their behaviour to be more energy efficient, when appropriate incentives are in place, and when measurement-based, real-time feedback about their energy usage is provided. Rewards incentivise `non-green' users to be `green' as well as encouraging those users who already claim to be `green'. Measurement-based feedback improves user energy awareness and helps users to explore and adjust their use of computers to become `greener', but is not sufficient by itself. In our measurements, weekly mean group energy use as a whole reduced by up to 16%; and weekly individual user energy consumption reduced by up to 56% during active use. The findings are drawn from our longitudinal study that involved 83 Computer Science students; lasted 48 weeks across 2 academic years; monitored a total of 26778 hours of active computer use; collected approximately 2TB of raw data.";energy feedback, energy monitoring, energy usage, green ICT, energy efficiency, user behaviour;;e-Energy '14
Conference Paper;Couto M,Borba P,Cunha J,Fernandes JP,Pereira R,Saraiva J;Products Go Green: Worst-Case Energy Consumption in Software Product Lines;;2017;;;84–93;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 21st International Systems and Software Product Line Conference - Volume A;Sevilla, Spain;2017;9,78145E+12;;"https://doi.org/10.1145/3106195.3106214;http://dx.doi.org/10.1145/3106195.3106214";10.1145/3106195.3106214;The optimization of software to be (more) energy efficient is becoming a major concern for the software industry. Although several techniques have been presented to measure energy consumption for software, none has addressed software product lines (SPLs). Thus, to measure energy consumption of a SPL, the products must be generated and measured individually, which is too costly.In this paper, we present a technique and a prototype tool to statically estimate the worst case energy consumption for SPL. The goal is to provide developers with techniques and tools to reason about the energy consumption of all products in a SPL, without having to produce, run and measure the energy in all of them.Our technique combines static program analysis techniques and worst case execution time prediction with energy consumption analysis. This technique analyzes all products in a feature-sensitive manner, that is, a feature used in several products is analyzed only once, while the energy consumption is estimated once per product.We implemented our technique in a tool called Serapis. We did a preliminary evaluation using a product line for image processing implemented in C. Our experiments considered 7 products from such line and our initial results show that the tool was able to estimate the worst-case energy consumption with a mean error percentage of 9.4% and standard deviation of 6.2% when compared with the energy measured when running the products.;;;SPLC '17
Conference Paper;Hindle A;Green Mining: A Methodology of Relating Software Change to Power Consumption;;2012;;;78–87;IEEE Press;Zurich, Switzerland;;Proceedings of the 9th IEEE Working Conference on Mining Software Repositories;;2012;9,78147E+12;;;;"Power consumption is becoming more and more important with the increased popularity of smart-phones, tablets and laptops. The threat of reducing a customer's battery-life now hangs over the software developer who asks, ""will this next change be the one that causes my software to drain a customer's battery?"" One solution is to detect power consumption regressions by measuring the power usage of tests, but this is time-consuming and often noisy. An alternative is to rely on software metrics that allow us to estimate the impact that a change might have on power consumption thus relieving the developer from expensive testing. This paper presents a general methodology for investigating the impact of software change on power consumption, we relate power consumption to software changes, and then investigate the impact of static OO software metrics on power consumption. We demonstrated that software change can effect power consumption using the Firefox web-browser and the Azureus/Vuze BitTorrent client. We found evidence of a potential relationship between some software metrics and power consumption. In conclusion, we explored the effect of software change on power consumption on two projects; and we provide an initial investigation on the impact of software metrics on power consumption.";power, sustainable-software, dynamic analysis, power consumption, software metrics, mining software repositories;;MSR '12
Conference Paper;Du Bois K,Schaeps T,Polfliet S,Ryckbosch F,Eeckhout L;SWEEP: Evaluating Computer System Energy Efficiency Using Synthetic Workloads;;2011;;;159–166;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 6th International Conference on High Performance and Embedded Architectures and Compilers;Heraklion, Greece;2011;9,78145E+12;;"https://doi.org/10.1145/1944862.1944886;http://dx.doi.org/10.1145/1944862.1944886";10.1145/1944862.1944886;"Energy efficiency is a key design concern in contemporary processor and system design, in the embedded domain as well as in the enterprise domain. The focus on energy efficiency has led to a number of power benchmarking methods recently. For example, EEMBC released EnergyBench and SPEC released SPECpower to quantify a system's energy efficiency; also academics have proposed power benchmarks, such as JouleSort. A major limitation for each of these proposals is that they are tied to a specific benchmark, and hence, they provide limited insight with respect to why one system may be more energy-efficient than another.This paper proposes SWEEP, Synthetic Workloads for Energy Efficiency and Performance evaluation, a framework for generating synthetic workloads with specific behavioral characteristics. We employ SWEEP to generate a wide range of synthetic workloads while varying the instruction mix, ILP, memory access patterns, and I/O-intensiveness; and we use SWEEP to evaluate the energy efficiency of commercial computer systems across the workload space and learn about how the energy efficiency of a computer system is tied to its workload's characteristics.This paper also presents the Energy-Delay Diagram (EDD), a novel method for visualizing energy efficiency. The EDD clearly illustrates the energy versus performance trade-off, and provides more intuitive insight than the traditionally used EDP and ED2P metrics.";generation, energy-efficiency, workload characterization;;HiPEAC '11
Conference Paper;Ferreira MA,Hoekstra E,Merkus B,Visser B,Visser J;SEFLab: A Lab for Measuring Software Energy Footprints;;2013;;;30–37;IEEE Press;San Francisco, California;;Proceedings of the 2nd International Workshop on Green and Sustainable Software;;2013;9,78147E+12;;;;Hardware dissipates energy because software tells it to. But attributing hardware energy usage to particular software functions is complicated due to distribution, resource sharing, and layering of software. To enable research on energy usage attribution, we have created the Software Energy Footprint Lab. We explain the experimental setup offered by the lab and the measurement and analysis methodology that it supports. We also describe some preliminary results aimed at deciphering hardware dissipation profiles for various types of servers under various forms of software stress. Finally, we provide an outlook of how energy footprint measurements can contribute to a body of knowledge on software-level energy optimization.;energy efficiency, software engineering, green products;;GREENS '13
Conference Paper;Colmant M,Felber P,Rouvoy R,Seinturier L;WattsKit: Software-Defined Power Monitoring of Distributed Systems;;2017;;;514–523;IEEE Press;Madrid, Spain;;Proceedings of the 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing;;2017;9,78151E+12;;"https://doi.org/10.1109/CCGRID.2017.27;http://dx.doi.org/10.1109/CCGRID.2017.27";10.1109/CCGRID.2017.27;"The design and the deployment of energy-efficient distributed systems is a challenging task, which requires software engineers to consider all the layers of a system, from hardware to software. In particular, monitoring and analyzing the power consumption of a distributed system spanning several---potentially heterogeneous---nodes becomes particularly tedious when aiming at a finer granularity than observing the power consumption of hosting nodes. While the state-of-the-art in software-defined power meters fails to deliver adaptive solutions to offer such service-level perspective and to cope with the diversity of hardware CPU architectures, this paper proposes to automatically learn the power models of the nodes supporting a distributed system, and then to use these inferred power models to better understand how the power consumption of the system's processes is distributed across nodes at runtime.Our solution, named WattsKit, offers a modular toolkit to build software-defined power meters ""à la carte"", thus dealing with the diversity of user and hardware requirements. Beyond the demonstrated capability of covering a wide diversity of CPU architectures with high accuracy, we illustrate the benefits of adopting software-defined power meters to analyze the power consumption of complex layered and distributed systems. In particular, we illustrate the capability of our approach to monitor the power consumption of a system composed of Docker Swarm, Weave,Elasticsearch, and Apache ZooKeeper. Thanks to WattsKit, developers and administrators are now able to identify potential power leaks in their software infrastructure.";;;CCGrid '17
Conference Paper;Clarke S,Whittlestone J;A Survey of the Potential Long-Term Impacts of AI: How AI Could Lead to Long-Term Changes in Science, Cooperation, Power, Epistemics and Values;;2022;;;192–202;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society;Oxford, United Kingdom;2022;9,78145E+12;;"https://doi.org/10.1145/3514094.3534131;http://dx.doi.org/10.1145/3514094.3534131";10.1145/3514094.3534131;It is increasingly recognised that advances in artificial intelligence could have large and long-lasting impacts on society. However, what form those impacts will take, just how large and long-lasting they will be, and whether they will ultimately be positive or negative for humanity, is far from clear. Based on surveying literature on the societal impacts of AI, we identify and discuss five potential long-term impacts of AI: how AI could lead to long-term chances in science, cooperation, power, epistemics, and values. We review the state of existing research in each of these areas and highlight priority questions for future research.;foresight, conflict, societal impacts of ai, transformative ai, scientific progress, ai alignment, epistemic processes, cooperation, power and inequality;;AIES '22
Journal Article;Alves P,Liu S,Wang D,Gerstein M;Multiple-Swarm Ensembles: Improving the Predictive Power and Robustness of Predictive Models and Its Use in Computational Biology;IEEE/ACM Trans. Comput. Biol. Bioinformatics;2018;15;3;926–933;IEEE Computer Society Press;Washington, DC, USA;;;;2018-05;;1545-5963;"https://doi.org/10.1109/TCBB.2017.2691329;http://dx.doi.org/10.1109/TCBB.2017.2691329";10.1109/TCBB.2017.2691329;"Machine learning is an integral part of computational biology, and has already shown its use in various applications, such as prognostic tests. In the last few years in the non-biological machine learning community, ensembling techniques have shown their power in data mining competitions such as the Netflix challenge; however, such methods have not found wide use in computational biology. In this work, we endeavor to show how ensembling techniques can be applied to practical problems, including problems in the field of bioinformatics, and how they often outperform other machine learning techniques in both predictive power and robustness. Furthermore, we develop a methodology of ensembling, Multi-Swarm Ensemble MSWE by using multiple particle swarm optimizations and demonstrate its ability to further enhance the performance of ensembles.";;;
Conference Paper;Di Nucci D,Palomba F,Prota A,Panichella A,Zaidman A,De Lucia A;PETrA: A Software-Based Tool for Estimating the Energy Profile of Android Applications;;2017;;;3–6;IEEE Press;Buenos Aires, Argentina;;Proceedings of the 39th International Conference on Software Engineering Companion;;2017;9,78154E+12;;"https://doi.org/10.1109/ICSE-C.2017.18;http://dx.doi.org/10.1109/ICSE-C.2017.18";10.1109/ICSE-C.2017.18;Energy efficiency is a vital characteristic of any mobile application, and indeed is becoming an important factor for user satisfaction. For this reason, in recent years several approaches and tools for measuring the energy consumption of mobile devices have been proposed. Hardware-based solutions are highly precise, but at the same time they require costly hardware toolkits. Model-based techniques require a possibly difficult calibration of the parameters needed to correctly create a model on a specific hardware device. Finally, software-based solutions are easier to use, but they are possibly less precise than hardware-based solution. In this demo, we present PETrA, a novel software-based tool for measuring the energy consumption of Android apps. With respect to other tools, PETrA is compatible with all the smartphones with Android 5.0 or higher, not requiring any device specific energy profile. We also provide evidence that our tool is able to perform similarly to hardware-based solutions.;energy consumption, mobile apps, estimation;;ICSE-C '17
Journal Article;Rostaminia S,Mayberry A,Ganesan D,Marlin B,Gummeson J;ILid: Low-Power Sensing of Fatigue and Drowsiness Measures on a Computational Eyeglass;Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.;2017;1;2;;Association for Computing Machinery;New York, NY, USA;;;;2017-06;;;"https://doi.org/10.1145/3090088;http://dx.doi.org/10.1145/3090088";10.1145/3090088;"The ability to monitor eye closures and blink patterns has long been known to enable accurate assessment of fatigue and drowsiness in individuals. Many measures of the eye are known to be correlated with fatigue including coarse-grained measures like the rate of blinks as well as fine-grained measures like the duration of blinks and the extent of eye closures. Despite a plethora of research validating these measures, we lack wearable devices that can continually and reliably monitor them in the natural environment. In this work, we present a low-power system, iLid, that can continually sense fine-grained measures such as blink duration and Percentage of Eye Closures (PERCLOS) at high frame rates of 100fps. We present a complete solution including design of the sensing, signal processing, and machine learning pipeline; implementation on a prototype computational eyeglass platform; and extensive evaluation under many conditions including illumination changes, eyeglass shifts, and mobility. Our results are very encouraging, showing that we can detect blinks, blink duration, eyelid location, and fatigue-related metrics such as PERCLOS with less than a few percent error.";Fatigue, Blinks, Drowsiness, Eyeglasses, Eyelid, PERCLOS;;
Conference Paper;Rottleuthner M,Schmidt TC,Wählisch M;Eco: A Hardware-Software Co-Design for In Situ Power Measurement on Low-End IoT Systems;;2019;;;22–28;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 7th International Workshop on Energy Harvesting & Energy-Neutral Sensing Systems;New York, NY, USA;2019;9,78145E+12;;"https://doi.org/10.1145/3362053.3363495;http://dx.doi.org/10.1145/3362053.3363495";10.1145/3362053.3363495;In this paper, we present Eco, a hardware-software co-design enabling generic energy management on IoT nodes. Eco is tailored to devices with limited resources and thus targets most of the upcoming IoT scenarios. The proposed measurement module combines commodity components with common system interfaces to achieve easy, flexible integration with various hardware platforms and the RIOT IoT operating system. We thoroughly evaluate and compare accuracy and overhead. Our findings indicate that our commodity design competes well with highly optimized solutions, while being significantly more versatile. We employ Eco for energy management on RIOT and validate its readiness for deployment in a five-week field trial integrated with energy harvesting.;power measurement, Energy harvesting, IoT;;ENSsys'19
Conference Paper;Jain M,Arjunan P;FATEsys 2021: The First ACM SIGEnergy Workshop on Fair, Accountable, Transparent, and Ethical (FATE) AI for Smart Environments and Energy Systems;;2021;;;252–254;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 8th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation;Coimbra, Portugal;2021;9,78145E+12;;"https://doi.org/10.1145/3486611.3492408;http://dx.doi.org/10.1145/3486611.3492408";10.1145/3486611.3492408;The advent of IoT, ubiquitous and smart sensing, and high performance computing has resulted in a big shift in the adoption of data-driven black-box modeling (also referred to as machine/deep learning) to make our environments smarter and energy systems efficient. Several studies have shown that these AI-enabled solutions for smart buildings, smart cities, smart grids, electric transportation, among others are much more accurate and efficient. However, these data-driven black-box solutions are rarely held accountable for the impact of their actions on the human in the loop which significantly impacts their real-world adoption. To truly conceptualize the idea of smart systems for everyone, it is critical to study these AI-enabled smart environments and energy systems for not just efficiency, but also for affordability and accessibility for all. The first ACM SIGEnergy workshop on Fair, Accountable, Transparent, and Ethical AI for Smart Environments and Energy Systems intends to bring together researchers from diverse backgrounds and discuss key issues, challenges, breakthroughs, and socio-economic impact in developing fair, accountable, transparent and ethical AI techniques for smart environments and energy systems.;smart energy systems, explainability, interpretability, transparency, ethics, fairness, smart environments, accountability;;BuildSys '21
Conference Paper;Jain M,Arjunan P;FATEsys 2022: The Second ACM SIGEnergy Workshop on Fair, Accountable, Transparent, and Ethical (FATE) AI for Smart Environments and Energy Systems;;2022;;;307–309;Association for Computing Machinery;New York, NY, USA;;Proceedings of the 9th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation;Boston, Massachusetts;2022;9,78145E+12;;"https://doi.org/10.1145/3563357.3568731;http://dx.doi.org/10.1145/3563357.3568731";10.1145/3563357.3568731;The advent of IoT, ubiquitous and smart sensing and high-performance computing has resulted in a big shift in the adoption of data-driven black-box modelling (also referred to as machine/deep learning) to make our environments smarter and energy systems efficient. Several studies have shown that these AI-enabled solutions for smart buildings, smart cities, smart grids, and electric transportation, among others, are much more accurate and efficient. However, these data-driven black-box solutions are rarely held accountable for the impact of their actions on the human in the loop which significantly impacts their real-world adoption. To truly conceptualize the idea of smart systems for everyone, it is critical to study these AI-enabled smart environments and energy systems for not just efficiency, but also for affordability and accessibility for all. The first ACM SIGEnergy workshop on Fair, Accountable, Transparent, and Ethical AI for Smart Environments and Energy Systems intends to bring together researchers from diverse backgrounds and discuss key issues, challenges, breakthroughs, and socioeconomic impact in developing fair, accountable, transparent and ethical AI techniques for smart environments and energy systems.;accountability, ethics, explainability, fairness, interpretability, transparency, smart energy systems, smart environments;;BuildSys '22
