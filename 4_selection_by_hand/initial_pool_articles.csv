Key;Item Type;Publication Year;Author;Title;Publication Title;ISBN;ISSN;DOI;Url;Abstract Note;Date;Date Added;Date Modified;Access Date;Pages;Num Pages;Issue;Volume;Number Of Volumes;Journal Abbreviation;Short Title;Series;Series Number;Series Text;Series Title;Publisher;Place;Language;Rights;Type;Archive;Archive Location;Library Catalog;Call Number;Extra;Notes;File Attachments;Link Attachments;Manual Tags;Automatic Tags;Editor;Series Editor;Translator;Contributor;Attorney Agent;Book Author;Cast Member;Commenter;Composer;Cosponsor;Counsel;Interviewer;Producer;Recipient;Reviewed Author;Scriptwriter;Words By;Guest;Number;Edition;Running Time;Scale;Medium;Artwork Size;Filing Date;Application Number;Assignee;Issuing Authority;Country;Meeting Name;Conference Name;Court;References;Reporter;Legal Status;Priority Numbers;Programming Language;Version;System;Code;Code Number;Section;Session;Committee;History;Legislative Body
5HDRNAYE;preprint;2020;"Anthony, Lasse F. Wolff; Kanding, Benjamin; Selvan, Raghavendra";Carbontracker: Tracking and Predicting the Carbon Footprint of Training Deep Learning Models;;;;;http://arxiv.org/abs/2007.03051;Deep learning (DL) can achieve impressive results across a wide variety of tasks, but this often comes at the cost of training models for extensive periods on specialized hardware accelerators. This energy-intensive workload has seen immense growth in recent years. Machine learning (ML) may become a significant contributor to climate change if this exponential trend continues. If practitioners are aware of their energy and carbon footprint, then they may actively take steps to reduce it whenever possible. In this work, we present Carbontracker, a tool for tracking and predicting the energy and carbon footprint of training DL models. We propose that energy and carbon footprint of model development and training is reported alongside performance metrics using tools like Carbontracker. We hope this will promote responsible computing in ML and encourage research into energy-efficient deep neural networks.;06/07/2020;18/04/2023 06:22;18/04/2023 06:22;10/04/2023 12:14;;;;;;;Carbontracker;;;;;arXiv;;;;;;;arXiv.org;;arXiv:2007.03051 [cs, eess, stat];;"C:\Users\charlotte.rodriguez\Zotero\storage\8RINSNRS\Anthony et al. - 2020 - Carbontracker Tracking and Predicting the Carbon .pdf; C:\Users\charlotte.rodriguez\Zotero\storage\9G85XPWM\2007.html";;;"Computer Science - Computers and Society; Computer Science - Machine Learning; Electrical Engineering and Systems Science - Signal Processing; Statistics - Machine Learning";;;;;;;;;;;;;;;;;;;arXiv:2007.03051;;;;;;;;;;;;;;;;;;;;;;;;;;;
34VCG97C;journalArticle;2022;"Budennyy, S. A.; Lazarev, V. D.; Zakharenko, N. N.; Korovin, A. N.; Plosskaya, O. A.; Dimitrov, D. V.; Akhripkin, V. S.; Pavlov, I. V.; Oseledets, I. V.; Barsola, I. S.; Egorov, I. V.; Kosterina, A. A.; Zhukov, L. E.";eco2AI: Carbon Emissions Tracking of Machine Learning Models as the First Step Towards Sustainable AI;Doklady Mathematics;;1531-8362;10.1134/S1064562422060230;https://doi.org/10.1134/S1064562422060230;The size and complexity of deep neural networks used in AI applications continue to grow exponentially, significantly increasing energy consumption for training and inference by these models. We introduce an open-source package eco2AI to help data scientists and researchers to track the energy consumption and equivalent CO2 emissions of their models in a straightforward way. In eco2AI we focus on accurate tracking of energy consumption and regional CO2 emissions accounting. We encourage the research for community to search for new optimal Artificial Intelligence (AI) architectures with lower computational cost. The motivation also comes from the concept of AI-based greenhouse gases sequestrating cycle with both Sustainable AI and Green AI pathways. The code and documentation are hosted on Github under the Apache 2.0 license https://github.com/sb-ai-lab/Eco2AI.;01/12/2022;18/04/2023 06:22;18/04/2023 06:22;10/04/2023 12:19;S118-S128;;1;106;;Dokl. Math.;eco2AI;;;;;;;en;;;;;Springer Link;;;;C:\Users\charlotte.rodriguez\Zotero\storage\UBJPVHX9\Budennyy et al. - 2022 - eco2AI Carbon Emissions Tracking of Machine Learn.pdf;;;"AI; carbon footprint; CO2 emissions; ecology; ESG; GHG; sustainability";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
87E69WLT;journalArticle;2020;"Henderson, Peter; Hu, Jieru; Romoff, Joshua; Brunskill, Emma; Jurafsky, Dan; Pineau, Joelle";Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning;Journal of Machine Learning Research;;1533-7928;;http://jmlr.org/papers/v21/20-312.html;Accurate reporting of energy and carbon usage is essential for understanding the potential climate impacts of machine learning research. We introduce a framework that makes this easier by providing a simple interface for tracking realtime energy consumption and carbon emissions, as well as generating standardized online appendices. Utilizing this framework, we create a leaderboard for energy efficient reinforcement learning algorithms to incentivize responsible research in this area as an example for other areas of machine learning. Finally, based on case studies using our framework, we propose strategies for mitigation of carbon emissions and reduction of energy consumption. By making accounting easier, we hope to further the sustainable development of machine learning experiments and spur more research into energy efficient algorithms.;2020;18/04/2023 06:22;18/04/2023 06:22;10/04/2023 12:20;janv-43;;248;21;;;;;;;;;;;;;;;jmlr.org;;;;C:\Users\charlotte.rodriguez\Zotero\storage\5EC4ERWA\experiment-impact-tracker.html;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
6VFR7LDC;preprint;2019;"Lacoste, Alexandre; Luccioni, Alexandra; Schmidt, Victor; Dandres, Thomas";Quantifying the Carbon Emissions of Machine Learning;;;;10.48550/arXiv.1910.09700;http://arxiv.org/abs/1910.09700;From an environmental standpoint, there are a few crucial aspects of training a neural network that have a major impact on the quantity of carbon that it emits. These factors include: the location of the server used for training and the energy grid that it uses, the length of the training procedure, and even the make and model of hardware on which the training takes place. In order to approximate these emissions, we present our Machine Learning Emissions Calculator, a tool for our community to better understand the environmental impact of training ML models. We accompany this tool with an explanation of the factors cited above, as well as concrete actions that individual practitioners and organizations can take to mitigate their carbon emissions.;04/11/2019;18/04/2023 06:22;18/04/2023 06:22;10/04/2023 12:23;;;;;;;;;;;;arXiv;;;;;;;arXiv.org;;arXiv:1910.09700 [cs];;"C:\Users\charlotte.rodriguez\Zotero\storage\Z5EJE7IN\Lacoste et al. - 2019 - Quantifying the Carbon Emissions of Machine Learni.pdf; C:\Users\charlotte.rodriguez\Zotero\storage\DDYEH6WL\1910.html";;;"Computer Science - Computers and Society; Computer Science - Machine Learning";;;;;;;;;;;;;;;;;;;arXiv:1910.09700;;;;;;;;;;;;;;;;;;;;;;;;;;;
4EP4IN2C;journalArticle;2021;"Lannelongue, Loïc; Grealey, Jason; Inouye, Michael";Green Algorithms: Quantifying the Carbon Footprint of Computation;Advanced Science;;2198-3844;10.1002/advs.202100707;https://onlinelibrary.wiley.com/doi/abs/10.1002/advs.202100707;Climate change is profoundly affecting nearly all aspects of life on earth, including human societies, economies, and health. Various human activities are responsible for significant greenhouse gas (GHG) emissions, including data centers and other sources of large-scale computation. Although many important scientific milestones are achieved thanks to the development of high-performance computing, the resultant environmental impact is underappreciated. In this work, a methodological framework to estimate the carbon footprint of any computational task in a standardized and reliable way is presented and metrics to contextualize GHG emissions are defined. A freely available online tool, Green Algorithms (www.green-algorithms.org) is developed, which enables a user to estimate and report the carbon footprint of their computation. The tool easily integrates with computational processes as it requires minimal information and does not interfere with existing code, while also accounting for a broad range of hardware configurations. Finally, the GHG emissions of algorithms used for particle physics simulations, weather forecasts, and natural language processing are quantified. Taken together, this study develops a simple generalizable framework and freely available tool to quantify the carbon footprint of nearly any computation. Combined with recommendations to minimize unnecessary CO2 emissions, the authors hope to raise awareness and facilitate greener computation.;2021;18/04/2023 06:22;18/04/2023 06:22;10/04/2023 12:27;2100707;;12;8;;;Green Algorithms;;;;;;;en;;;;;Wiley Online Library;;_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/advs.202100707;;"C:\Users\charlotte.rodriguez\Zotero\storage\33PPFMVP\Lannelongue et al. - 2021 - Green Algorithms Quantifying the Carbon Footprint.pdf; C:\Users\charlotte.rodriguez\Zotero\storage\FEZM5DUW\advs.html";;;"climate change; computational research; green computing";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
LR4R7J6X;preprint;2019;"Lottick, Kadan; Susai, Silvia; Friedler, Sorelle A.; Wilson, Jonathan P.";Energy Usage Reports: Environmental awareness as part of algorithmic accountability;;;;10.48550/arXiv.1911.08354;http://arxiv.org/abs/1911.08354;The carbon footprint of algorithms must be measured and transparently reported so computer scientists can take an honest and active role in environmental sustainability. In this paper, we take analyses usually applied at the industrial level and make them accessible for individual computer science researchers with an easy-to-use Python package. Localizing to the energy mixture of the electrical power grid, we make the conversion from energy usage to CO2 emissions, in addition to contextualizing these results with more human-understandable benchmarks such as automobile miles driven. We also include comparisons with energy mixtures employed in electrical grids around the world. We propose including these automatically-generated Energy Usage Reports as part of standard algorithmic accountability practices, and demonstrate the use of these reports as part of model-choice in a machine learning context.;16/12/2019;18/04/2023 06:22;18/04/2023 06:22;10/04/2023 12:30;;;;;;;Energy Usage Reports;;;;;arXiv;;;;;;;arXiv.org;;arXiv:1911.08354 [cs, stat];;"C:\Users\charlotte.rodriguez\Zotero\storage\2A8W79X2\Lottick et al. - 2019 - Energy Usage Reports Environmental awareness as p.pdf; C:\Users\charlotte.rodriguez\Zotero\storage\EACD79Y9\1911.html";;;"Computer Science - Computers and Society; Computer Science - Machine Learning; Statistics - Machine Learning";;;;;;;;;;;;;;;;;;;arXiv:1911.08354;;;;;;;;;;;;;;;;;;;;;;;;;;;
6TC48R9J;conferencePaper;2022;Noureddine, Adel;PowerJoular and JoularJX: Multi-Platform Software Power Monitoring Tools;;;;10.1109/IE54923.2022.9826760;https://hal.science/hal-03608223;Monitoring the power consumption of applications and source code is an important step in writing green software. In this paper, we propose PowerJoular and JoularJX, our software power monitoring tools. We aim to help software developers in understanding and analyzing the power consumption of their programs, and help system administrators and automated tools in monitoring the power consumption of large numbers of heterogeneous devices.;20/06/2022;18/04/2023 06:22;18/04/2023 06:22;10/04/2023 12:39;;;;;;;PowerJoular and JoularJX;;;;;;;en;;;;;hal.science;;;;"C:\Users\charlotte.rodriguez\Zotero\storage\8HX6WK7G\9826760.html; C:\Users\charlotte.rodriguez\Zotero\storage\6U5EMXFC\Noureddine - 2022 - PowerJoular and JoularJX Multi-Platform Software .pdf";;;"Codes; Energy Analysis; Linux; Measurement; Operating systems; Power Consumption; Power demand; Power Monitoring; Real-time systems; Software; Writing";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;18th International Conference on Intelligent Environments;;;;;;;;;;;;;;;
WYGMYQDS;conferencePaper;2022;"Pathania, Priyavanshi; Mehra, Rohit; Sharma, Vibhu Saujanya; Kaulgud, Vikrant; Podder, Sanjay; Burden, Adam P.";ESAVE: Estimating Server and Virtual Machine Energy;37th IEEE/ACM International Conference on Automated Software Engineering;;;10.1145/3551349.3561170;http://arxiv.org/abs/2209.07394;Sustainable software engineering has received a lot of attention in recent times, as we witness an ever-growing slice of energy use, for example, at data centers, as software systems utilize the underlying infrastructure. Characterizing servers for their energy use accurately without being intrusive, is therefore important to make sustainable software deployment choices. In this paper, we introduce ESAVE which is a machine learning-based approach that leverages a small set of hardware attributes to characterize a server or virtual machine's energy usage across different levels of utilization. This is based upon an extensive exploration of multiple ML approaches, with a focus on a minimal set of required attributes, while showcasing good accuracy. Early validations show that ESAVE has only around 12% average prediction error, despite being non-intrusive.;10/10/2022;18/04/2023 06:22;18/04/2023 06:22;10/04/2023 12:43;01-mars;;;;;;ESAVE;;;;;;;;;;;;arXiv.org;;arXiv:2209.07394 [cs];;"C:\Users\charlotte.rodriguez\Zotero\storage\DKE67W5E\Pathania et al. - 2022 - ESAVE Estimating Server and Virtual Machine Energ.pdf; C:\Users\charlotte.rodriguez\Zotero\storage\HHKF87JV\2209.html; C:\Users\charlotte.rodriguez\Zotero\storage\RJ8C7QA4\Pathania et al. - 2022 - ESAVE Estimating Server and Virtual Machine Energ.pdf";;;Computer Science - Software Engineering;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
WI5YTX43;conferencePaper;2018;"Rodrigues, Crefeda; Riley, Graham; Luján, Mikel";SyNERGY: An energy measurement and prediction framework for Convolutional Neural Networks on Jetson TX1;;;;;;"There is a huge demand for on-device execution of deep learning algorithms on mobile and embedded platforms. These devices present constraints on the application due to limited hardware resources and power. However, current evaluation studies in existing deep learning frameworks (for example, Caffe, Tensorflow, Torch and others) are limited to performance measurements of these applications on high-end CPUs and GPUs. In this work, we propose ""SyNERGY"" a fine-grained energy measurement (that is, at specific layers) and prediction framework for deep neu-ral networks on embedded platforms. We integrate ARM's Streamline Performance Analyser with standard deep learning frameworks such as Caffe and CuDNNv5 to quantify the energy-use of deep convolutional neural networks on the Nvidia Jetson Tegra X1. Our measurement framework provides an accurate breakdown of actual energy consumption and performance across all layers in the neural network while our prediction framework models the energy-use in terms of target-specific performance counters such as SIMD and bus accesses and application specific parameters such as Multiply and Accumulate (MAC) counts. Our experimental results using 9 representative Deep Convolutional Neural Network shows that a multi-variable linear regression model based on hardware performance counters alone achieves an average prediction test error of 8.04 ± 5.96% compared to actual energy measurements. Surprisingly, we find that it is possible to refine the model to predict the number of SIMD instructions and main memory accesses solely from the application's Multiply-Accumulate (MAC) counts with an average prediction test error of 0.81 ± 0.77% and 17.97 ± 15.29% respectively. This alleviates the need for actual measurements giving a final average prediction test error of 7.08 ± 5.05% using solely the application's MAC counts as input.";01/10/2018;18/04/2023 06:22;18/04/2023 06:22;;;;;;;;SyNERGY;;;;;;;;;;;;ResearchGate;;;;"C:\Users\charlotte.rodriguez\Zotero\storage\6QCPPEXV\Rodrigues et al. - 2018 - SyNERGY An energy measurement and prediction fram.pdf; ";https://www.researchgate.net/publication/327988512_SyNERGY_An_energy_measurement_and_prediction_framework_for_Convolutional_Neural_Networks_on_Jetson_TX1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;Int'l Conf. Par. and Dist. Proc. Tech. and Appl. | PDPTA'18;;;;;;;;;;;;;;;
C6N9DRPX;preprint;2021;"Shahid, Arsalan; Fahad, Muhammad; Manumachu, Ravi Reddy; Lastovetsky, Alexey";Energy of Computing on Multicore CPUs: Predictive Models and Energy Conservation Law;;;;;http://arxiv.org/abs/1907.02805;Energy is now a first-class design constraint along with performance in all computing settings. Energy predictive modelling based on performance monitoring counts (PMCs) is the leading method used for prediction of energy consumption during an application execution. We use a model-theoretic approach to formulate the assumed properties of existing models in a mathematical form. We extend the formalism by adding properties, heretofore unconsidered, that account for a limited form of energy conservation law. The extended formalism defines our theory of energy of computing. By applying the basic practical implications of the theory, we improve the prediction accuracy of state-of-the-art energy models from 31% to 18%. We also demonstrate that use of state-of-the-art measurement tools for energy optimisation may lead to significant losses of energy (ranging from 56% to 65% for applications used in experiments) since they do not take into account the energy conservation properties.;27/09/2021;18/04/2023 06:22;18/04/2023 06:22;10/04/2023 12:54;;;;;;;Energy of Computing on Multicore CPUs;;;;;arXiv;;;;;;;arXiv.org;;arXiv:1907.02805 [cs, eess];;C:\Users\charlotte.rodriguez\Zotero\storage\X4ET8VRU\1907.html;;;"Computer Science - Distributed, Parallel, and Cluster Computing; Computer Science - Performance; Electrical Engineering and Systems Science - Systems and Control";;;;;;;;;;;;;;;;;;;arXiv:1907.02805;;;;;;;;;;;;;;;;;;;;;;;;;;;
QT24GU2Y;journalArticle;2021;"Shahid, Arsalan; Fahad, Muhammad; Manumachu, Ravi Reddy; Lastovetsky, Alexey";Energy Predictive Models of Computing: Theory, Practical Implications and Experimental Analysis on Multicore Processors;IEEE Access;;2169-3536;10.1109/ACCESS.2021.3075139;;The energy efficiency in ICT is becoming a grand technological challenge and is now a first-class design constraint in all computing settings. Energy predictive modelling based on performance monitoring counters (PMCs) is the leading method for application-level energy optimization. However, a sound theoretical framework to understand the fundamental significance of the PMCs to the energy consumption and the causes of the inaccuracy of the models is lacking. In this work, we propose a small but insightful theory of energy predictive models of computing, which formalizes both the assumptions behind the existing PMC-based energy predictive models and properties, heretofore unconsidered, that are basic implications of the universal energy conservation law. The theory's basic practical implications include selection criteria for model variables, model intercept, and model coefficients. The experiments on two modern Intel multicore servers show that applying the proposed selection criteria improves the prediction accuracy of state-of-the-art linear regression models from 31.2% to 18%. Finally, we demonstrate that employing energy models constructed using the proposed theory for energy optimization can save a significant amount of energy (up to 80% for applications used in experiments) compared to state-of-the-art energy measurement tools.;2021;18/04/2023 06:22;18/04/2023 06:22;;63149-63172;;;9;;;Energy Predictive Models of Computing;;;;;;;;;;;;IEEE Xplore;;Conference Name: IEEE Access;;"C:\Users\charlotte.rodriguez\Zotero\storage\DK663KHD\Shahid et al. - 2021 - Energy Predictive Models of Computing Theory, Pra.pdf; C:\Users\charlotte.rodriguez\Zotero\storage\PJ8888P9\9410601.html";;;"Biological system modeling; Computational modeling; energy conservation; Energy consumption; Energy measurement; energy optimization; energy predictive modeling; linear regression; Multicore processing; Multicore processor; performance monitoring counters; Predictive models; Program processors";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
GBVCNZCP;conferencePaper;2013;"Singh, Vivek Kumar; Dutta, Kaushik; VanderMeer, Debra";Estimating the Energy Consumption of Executing Software Processes;2013 IEEE International Conference on Green Computing and Communications and IEEE Internet of Things and IEEE Cyber, Physical and Social Computing;;;10.1109/GreenCom-iThings-CPSCom.2013.40;;Power consumption in data centers is significant across the globe. The use of cloud-based services, e.g., infrastructure as a service and software as a service (such as Google Docs, Microsoft Office 365, Salesforce.com), is becoming a standard practice in modern IT frameworks. This paradigm shift in the IT industry indicates that the demand for data-center-based services will continue to increase in the future, with concomitant increases in power consumption. In such a scenario, optimizing the IT resources to improve energy efficiency is a necessity. The first step of such an optimization at the application level is knowing how much energy an application is consuming. One of the main challenges in this domain is developing a software-based energy metering tool that can measure an OS processes' energy consumption. Many existing solutions depend on an external watt-meter or other hardware-based enhancements, these are not practical for real-world use in data centers. To overcome the limitations of existing solutions, we have developed an OS process-level power metering tool that can accurately estimate the energy usage of each OS process running on a Linux server without an online watt-meter. Based on a set of experiments, we demonstrated that our method and implementation provides energy consumption estimation for complex e-business applications with above 95% accuracy.;2013-08;18/04/2023 06:22;18/04/2023 06:22;;94-101;;;;;;;;;;;;;;;;;;IEEE Xplore;;;;C:\Users\charlotte.rodriguez\Zotero\storage\RXYJFISY\6682054.html;;;"Measurement; Software; Biological system modeling; Energy consumption; Energy measurement; Energy; Equations; Green IT; Hardware; Mathematical model; Modeling; Power Meter";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;2013 IEEE International Conference on Green Computing and Communications and IEEE Internet of Things and IEEE Cyber, Physical and Social Computing;;;;;;;;;;;;;;;
MBU5BBSV;conferencePaper;2019;"Strubell, Emma; Ganesh, Ananya; McCallum, Andrew";Energy and Policy Considerations for Deep Learning in NLP;Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics;;;10.18653/v1/P19-1355;https://aclanthology.org/P19-1355;Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.;2019-07;18/04/2023 06:22;18/04/2023 06:22;10/04/2023 13:04;3645–3650;;;;;;;;;;;Association for Computational Linguistics;Florence, Italy;;;;;;ACLWeb;;;;C:\Users\charlotte.rodriguez\Zotero\storage\5L8JF7FL\Strubell et al. - 2019 - Energy and Policy Considerations for Deep Learning.pdf;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;ACL 2019;;;;;;;;;;;;;;;
Q77CKL49;conferencePaper;2017;"Yang, Tien-Ju; Chen, Yu-Hsin; Emer, Joel; Sze, Vivienne";A method to estimate the energy consumption of deep neural networks;2017 51st Asilomar Conference on Signals, Systems, and Computers;;;10.1109/ACSSC.2017.8335698;;Deep Neural Networks (DNNs) have enabled state-of-the-art accuracy on many challenging artificial intelligence tasks. While most of the computation currently resides in the cloud, it is desirable to embed DNN processing locally near the sensor due to privacy, security, and latency concerns or limitations in communication bandwidth. Accordingly, there has been increasing interest in the research community to design energy-efficient DNNs. However, estimating energy consumption from the DNN model is much more difficult than other metrics such as storage cost (model size) and throughput (number of operations). This is due to the fact that a significant portion of the energy is consumed by data movement, which is difficult to extract directly from the DNN model. This work proposes an energy estimation methodology that can estimate the energy consumption of a DNN based on its architecture, sparsity, and bitwidth. This methodology can be used to evaluate the various DNN architectures and energy-efficient techniques that are currently being proposed in the field and guide the design of energy-efficient DNNs. We have released an online version of the energy estimation tool at energyestimation.mit.edu. We believe that this method will play a critical role in bridging the gap between algorithm and hardware design and provide useful insights for the development of energy-efficient DNNs.;2017-10;18/04/2023 06:22;18/04/2023 06:22;;1916-1920;;;;;;;;;;;;;;;;;;IEEE Xplore;;ISSN: 2576-2303;;"C:\Users\charlotte.rodriguez\Zotero\storage\3Q2AJGCH\8335698.html; C:\Users\charlotte.rodriguez\Zotero\storage\ZA78ILG7\Yang et al. - 2017 - A method to estimate the energy consumption of dee.pdf";;;"Measurement; Energy consumption; Hardware; Deep learning; deep neural network; energy estimation; energy metric; Estimation; machine learning; Memory management; Neural networks; Optimization";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;2017 51st Asilomar Conference on Signals, Systems, and Computers;;;;;;;;;;;;;;;
7KRHM3PF;conferencePaper;2010;"Treibig, Jan; Hager, Georg; Wellein, Gerhard";LIKWID: A Lightweight Performance-Oriented Tool Suite for x86 Multicore Environments;Proceedings of the 2010 39th International Conference on Parallel Processing Workshops;978-0-7695-4157-0;;10.1109/ICPPW.2010.38;https://doi.org/10.1109/ICPPW.2010.38;Exploiting the performance of today's processors requires intimate knowledge of the microarchitecture as well as an awareness of the ever-growing complexity in thread and cache topology. LIKWID is a set of command-line utilities that addresses four key problems: Probing the thread and cache topology of a shared-memory node, enforcing thread-core affinity on a program, measuring performance counter metrics, and toggling hardware prefetchers. An API for using the performance counting features from user code is also included. We clearly state the differences to the widely used PAPI interface. To demonstrate the capabilities of the tool set we show the influence of thread pinning on performance using the well-known OpenMP STREAM triad benchmark, and use the affinity and hardware counter tools to study the performance of a stencil code specifically optimized to utilize shared caches on multicore chips.;13/09/2010;21/04/2023 14:41;21/04/2023 14:41;21/04/2023;207–216;;;;;;LIKWID;ICPPW '10;;;;IEEE Computer Society;USA;;;;;;ACM Digital Library;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
T5WF393N;journalArticle;2020;"Qiu, Xinchi; Parcollet, Titouan; Beutel, Daniel J.; Topal, Taner; Mathur, Akhil; Lane, N.";A first look into the carbon footprint of federated learning;ArXiv;;;;https://www.semanticscholar.org/paper/A-first-look-into-the-carbon-footprint-of-federated-Qiu-Parcollet/ea50fdb7f3b071f3d19b7bc8a0df95cb80f15e06;Despite impressive results, deep learning-based technologies also raise severe privacy and environmental concerns induced by the training procedure often conducted in data centers. In response, alternatives to centralized training such as Federated Learning (FL) have emerged. Perhaps unexpectedly, FL in particular is starting to be deployed at a global scale by companies that must adhere to new legal demands and policies originating from governments and the civil society for privacy protection. However, the potential environmental impact related to FL remains unclear and unexplored. This paper offers the first-ever systematic study of the carbon footprint of FL. First, we propose a rigorous model to quantify the carbon footprint, hence facilitating the investigation of the relationship between FL design and carbon emissions. Then, we compare the carbon footprint of FL to traditional centralized learning. We also formalize an early-stage FL optimization problem enabling the community to consider the importance of optimizing the rate of CO2 emissions jointly to the accuracy of neural networks. Finally, we highlight and connect the reported results to the future challenges and trends in FL to reduce its environmental impact, including algorithms efficiency, hardware capabilities, and stronger industry transparency.;13/10/2020;27/04/2023 13:35;27/04/2023 13:35;27/04/2023 13:35;;;;;;;;;;;;;;;;;;;Semantic Scholar;;;;"C:\Users\charlotte.rodriguez\Zotero\storage\LWN6837Z\Qiu et al. - 2020 - A first look into the carbon footprint of federate.pdf; ";https://www.semanticscholar.org/paper/A-first-look-into-the-carbon-footprint-of-federated-Qiu-Parcollet/ea50fdb7f3b071f3d19b7bc8a0df95cb80f15e06;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
IJ2Z4VLE;conferencePaper;2022;"Corda, Stefano; Veenboer, Bram; Tolley, Emma";PMT: Power Measurement Toolkit;2022 IEEE/ACM International Workshop on HPC User Support Tools (HUST);;;10.1109/HUST56722.2022.00011;;Efficient use of energy is essential for today's super-computing systems, as energy cost is generally a major component of their operational cost. Research into “green computing” is needed to reduce the environmental impact of running these systems. As such, several scientific communities are evaluating the trade-off between time-to-solution and energy-to-solution. While the runtime of an application is typically easy to measure, power consumption is not. Therefore, we present the Power Measurement Toolkit (PMT), a high-level software library capable of collecting power consumption measurements on various hardware. The library provides a standard interface to easily measure the energy use of devices such as CPUs and GPUs in critical application sections.;2022-11;27/04/2023 15:55;27/04/2023 15:55;;44-47;;;;;;PMT;;;;;;;;;;;;IEEE Xplore;;;;"C:\Users\charlotte.rodriguez\Zotero\storage\ARFK74M3\10027520.html; C:\Users\charlotte.rodriguez\Zotero\storage\AB7BLCM5\Corda et al. - 2022 - PMT Power Measurement Toolkit.pdf";;;"Power demand; Energy measurement; Hardware; GPU; HPC; Costs; CPU; Efficiency; Performance; Power measurement; Runtime; Software libraries";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;2022 IEEE/ACM International Workshop on HPC User Support Tools (HUST);;;;;;;;;;;;;;;
8DUB84ZH;journalArticle;2023;"Desislavov, Radosvet; Martínez-Plumed, Fernando; Hernández-Orallo, José";Trends in AI inference energy consumption: Beyond the performance-vs-parameter laws of deep learning;Sustainable Computing: Informatics and Systems;;22105379;10.1016/j.suscom.2023.100857;https://linkinghub.elsevier.com/retrieve/pii/S2210537923000124;"Semantic Scholar extracted view of ""Compute and Energy Consumption Trends in Deep Learning Inference"" by Radosvet Desislavov et al.";2023-04;27/04/2023 16:38;27/04/2023 16:38;27/04/2023 16:38;100857;;;38;;Sustainable Computing: Informatics and Systems;Trends in AI inference energy consumption;;;;;;;en;;;;;Semantic Scholar;;;;"C:\Users\charlotte.rodriguez\Zotero\storage\UHNNSJQL\Desislavov et al. - 2023 - Trends in AI inference energy consumption Beyond .pdf; ";https://www.semanticscholar.org/paper/Compute-and-Energy-Consumption-Trends-in-Deep-Desislavov-Mart'inez-Plumed/ff69d758764157e612f92f97a987838312c568a9;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
4S25S936;report;2020;Trébaol, Tristan;CUMULATOR — a tool to quantify and report the carbon footprint of machine learning computations and communication in academia and healthcare;;;;;;"""The cost of training machines is becoming a problem"". This is the title of an article from The Economist published in June 2020 that highlights the staggeringly unappreciated the financial impact of AI [38]. However, these costs on are not limited to monetary concerns but also accrue a concerning environmental toll. One round of training for some of the most complex machine learning models can emit millions of kilograms of carbon dioxide due to the electricity consumed . With the growing popularity of ML and the digitization across all sectors, there should be a growing awareness of the potential impact of these technologies on the environment and their potential contribution to climate change. Then, their use could be rationalized and steps can be taken to responsibilize the offset of their impact. We hereby propose CUMULATOR: an open-source API to quantify and report the carbon footprint of machine learning methods. As a demonstration, we integrated this API within an ML based medical research platform called Alg-E, which will be used in a large scale medical research project in Tanzania and Rwanda. We use CUMULATOR to analyse the trade-off between accuracy and carbon footprint within Alg-E and extend it with simple visualisations. In parallel, we also propose a Carbon Statement Protocol to quantify and report the carbon footprint of individual work, which uses this project as a proof-of-concept. This protocol and CUMULATOR thus comprise a great set of tools to report the carbon footprint of a large-scale medical research trial and EPFL research projects in the future";2020;18/04/2023 06:22;18/04/2023 06:22;;;;;;;;;;;;;;;;;;;;Infoscience;;;;;;;"carbon footprint; machine learning; sustainable AI";;;;Trébaol, Tristan;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
IEW52W45;thesis;2020;Rodrigues, Crefeda;Efficient Execution of Convolutional Neural Networks on Low Powered Heterogeneous Systems;;;;;;Energy-efficient machine learning has been gaining interest due to the increase use of of machine learning, in particular deep learning, in applications that run on mobile and embedded devices. These devices are constrained in terms of resources in computation, memory and power, which limit the adoption of deep learning-based solutions, which are known to be power hungry. Research efforts that focus on reducing the energy consumption of machine learning is often referred to as Machine Learning on the Edge, the area of research to which the work in this thesis contributes. In this thesis, we identity and address three main issues related to enabling machine learning on the edge: the lack of software support to procure energy measurements, the lack of experimental evaluations based on energy use, and finally, the need for tools to rapidly explore neural network implementations in the context of emerging hardware. To address the first two issues, we first present, SyNERGY, a framework integrated in current deep learning software frameworks, that allows researchers to evaluate deep learning models and their optimizations on the metrics of both execution time and energy use on existing mobile platforms at different levels of granularity. To address the last issue, to explore efficient deep neural network implementations and hardware designs, a second tool, NNTaskSim, is proposed which supports the expression of neural network computations in a task-parallel framework and can be used to explore the execution of the resulting task graphs in the context of emerging hardware designs. The result of using SyNERGY is empirically gathered energy use and execution time data of existing deep learning models on current mobile devices. Based on the experimental data gathered, new, relatively low cost and accurate predictive models are explored and provided to estimate the energy use of new deep learning models. The experimental evaluation based on NNTaskSim shows that rapid exploration of neural network task graph executions can be done for prototype hardware, evaluated on the metrics of time and memory use, to reveal insights into the software and hardware design choices that lead to efficient solutions for deep learning systems.;2020;18/04/2023 06:22;18/04/2023 06:22;;;;;;;;;;;;;The University of Manchester (United Kingdom);;;;phd;;;ACM Digital Library;;AAI28130258;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
9FEAR5V4;conferencePaper;2021;"Bannour, Nesrine; Ghannay, Sahar; Névéol, Aurélie; Ligozat, Anne-Laure";Evaluating the carbon footprint of NLP methods: a survey and analysis of existing tools;Proceedings of the Second Workshop on Simple and Efficient Natural Language Processing;;;10.18653/v1/2021.sustainlp-1.2;https://aclanthology.org/2021.sustainlp-1.2;Modern Natural Language Processing (NLP) makes intensive use of deep learning methods because of the accuracy they offer for a variety of applications. Due to the significant environmental impact of deep learning, cost-benefit analysis including carbon footprint as well as accuracy measures has been suggested to better document the use of NLP methods for research or deployment. In this paper, we review the tools that are available to measure energy use and CO2 emissions of NLP methods. We describe the scope of the measures provided and compare the use of six tools (carbon tracker, experiment impact tracker, green algorithms, ML CO2 impact, energy usage and cumulator) on named entity recognition experiments performed on different computational set-ups (local server vs. computing facility). Based on these findings, we propose actionable recommendations to accurately measure the environmental impact of NLP experiments.;01/11/2021;18/04/2023 06:22;18/04/2023 06:22;10/04/2023 13:20;11–21;;;;;;Evaluating the carbon footprint of NLP methods;;;;;Association for Computational Linguistics;Virtual;;;;;;ACLWeb;;;;C:\Users\charlotte.rodriguez\Zotero\storage\3RVRJCKW\Bannour et al. - 2021 - Evaluating the carbon footprint of NLP methods a .pdf;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;sustainlp 2021;;;;;;;;;;;;;;;
HV5LFT7D;journalArticle;2019;"García-Martín, Eva; Rodrigues, Crefeda Faviola; Riley, Graham; Grahn, Håkan";Estimation of energy consumption in machine learning;Journal of Parallel and Distributed Computing;;0743-7315;10.1016/j.jpdc.2019.07.007;https://www.sciencedirect.com/science/article/pii/S0743731518308773;Energy consumption has been widely studied in the computer architecture field for decades. While the adoption of energy as a metric in machine learning is emerging, the majority of research is still primarily focused on obtaining high levels of accuracy without any computational constraint. We believe that one of the reasons for this lack of interest is due to their lack of familiarity with approaches to evaluate energy consumption. To address this challenge, we present a review of the different approaches to estimate energy consumption in general and machine learning applications in particular. Our goal is to provide useful guidelines to the machine learning community giving them the fundamental knowledge to use and build specific energy estimation methods for machine learning algorithms. We also present the latest software tools that give energy estimation values, together with two use cases that enhance the study of energy consumption in machine learning.;01/12/2019;18/04/2023 06:22;18/04/2023 06:22;10/04/2023 13:21;75-88;;;134;;Journal of Parallel and Distributed Computing;;;;;;;;en;;;;;ScienceDirect;;;;"C:\Users\charlotte.rodriguez\Zotero\storage\9E5KUEW9\García-Martín et al. - 2019 - Estimation of energy consumption in machine learni.pdf; C:\Users\charlotte.rodriguez\Zotero\storage\PLBZDGKB\S0743731518308773.html";;;"Energy consumption; Deep learning; GreenAI; High performance computing; Machine learning";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
IXLZC2RG;journalArticle;2019;"Fahad, Muhammad; Shahid, Arsalan; Manumachu, Ravi Reddy; Lastovetsky, Alexey";A Comparative Study of Methods for Measurement of Energy of Computing;Energies;;1996-1073;10.3390/en12112204;https://www.mdpi.com/1996-1073/12/11/2204;"Energy of computing is a serious environmental concern and mitigating it is an important technological challenge. Accurate measurement of energy consumption during an application execution is key to application-level energy minimization techniques. There are three popular approaches to providing it: (a) System-level physical measurements using external power meters; (b) Measurements using on-chip power sensors and (c) Energy predictive models. In this work, we present a comprehensive study comparing the accuracy of state-of-the-art on-chip power sensors and energy predictive models against system-level physical measurements using external power meters, which we consider to be the ground truth. We show that the average error of the dynamic energy profiles obtained using on-chip power sensors can be as high as 73% and the maximum reaches 300% for two scientific applications, matrix-matrix multiplication and 2D fast Fourier transform for a wide range of problem sizes. The applications are executed on three modern Intel multicore CPUs, two Nvidia GPUs and an Intel Xeon Phi accelerator. The average error of the energy predictive models employing performance monitoring counters (PMCs) as predictor variables can be as high as 32% and the maximum reaches 100% for a diverse set of seventeen benchmarks executed on two Intel multicore CPUs (one Haswell and the other Skylake). We also demonstrate that using inaccurate energy measurements provided by on-chip sensors for dynamic energy optimization can result in significant energy losses up to 84%. We show that, owing to the nature of the deviations of the energy measurements provided by on-chip sensors from the ground truth, calibration can not improve the accuracy of the on-chip sensors to an extent that can allow them to be used in optimization of applications for dynamic energy. Finally, we present the lessons learned, our recommendations for the use of on-chip sensors and energy predictive models and future directions.";01/01/2019;18/04/2023 06:22;18/04/2023 06:22;10/04/2023 13:21;2204;;11;12;;;;;;;;;;en;http://creativecommons.org/licenses/by/3.0/;;;;www.mdpi.com;;Number: 11 Publisher: Multidisciplinary Digital Publishing Institute;;C:\Users\charlotte.rodriguez\Zotero\storage\RHFLNHKK\Fahad et al. - 2019 - A Comparative Study of Methods for Measurement of .pdf;;;"performance monitoring counters; energy efficiency; energy predictive models; GPU; multicore CPU; NVML; power aensors; power meters; RAPL; Xeon Phi";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
QD75WZHR;journalArticle;2013;"Noureddine, Adel; Rouvoy, Romain; Seinturier, Lionel";A review of energy measurement approaches;ACM SIGOPS Operating Systems Review;;0163-5980;10.1145/2553070.2553077;https://dl.acm.org/doi/10.1145/2553070.2553077;Reducing the energy footprint of digital devices and software is a task challenging the research in Green IT. Researches have proposed approaches for energy management, ranging from reducing usage of software and hardware, compilators optimization, to server consolidation and software migration. However, optimizing the energy consumption requires knowledge of that said consumption. In particular, measuring the energy consumption of hardware and software is an important requirement for efficient energy strategies. In this review, we outline the different categories of approaches in energy measurements, and provide insights into example of each category. We draw recommendations from our review on requirements on how to efficiently measure energy consumption of devices and software.;26/11/2013;18/04/2023 06:28;18/04/2023 06:28;18/04/2023 06:28;42–49;;3;47;;SIGOPS Oper. Syst. Rev.;;;;;;;;;;;;;ACM Digital Library;;;;C:\Users\charlotte.rodriguez\Zotero\storage\YPMUVVZC\Noureddine et al. - 2013 - A review of energy measurement approaches.pdf;;;"energy management; energy measurement; energy metrics; review; survey";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
8GUQFSZ4;conferencePaper;2023;"Jay, Mathilde; Ostapenco, Vladimir; Lefevre, Laurent; Trystram, Denis; Orgerie, Anne-Cécile; Fichel, Benjamin";An experimental comparison of software-based power meters: focus on CPU and GPU;2023 IEEE/ACM 23rd International Symposium on Cluster, Cloud and Internet Computing (CCGrid);9,79835E+12;;10.1109/CCGrid57682.2023.00020;https://ieeexplore.ieee.org/document/10171575/;;2023-05;03/09/2023 16:53;03/09/2023 16:53;03/09/2023 16:53;106-118;;;;;;An experimental comparison of software-based power meters;;;;;IEEE;Bangalore, India;;;;;;DOI.org (Crossref);;;;C:\Users\charlotte.rodriguez\Zotero\storage\8ZYKV46T\Jay et al. - 2023 - An experimental comparison of software-based power.pdf;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;2023 IEEE/ACM 23rd International Symposium on Cluster, Cloud and Internet Computing (CCGrid);;;;;;;;;;;;;;;
