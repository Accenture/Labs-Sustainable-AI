,original_title,ISBN,ISSN,DOI,URLs,Proceedings title,Abstract,Authors,Journal,Publication year,Date published,URLs,title,keep_title
0,Quantifying the Energy Consumption of a Pocket Computer and a Java Virtual Machine,"9,78158E+12",,10.1145/339331.339421,https://doi.org/10.1145/339331.339421;http://dx.doi.org/10.1145/339331.339421,Proceedings of the 2000 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems,"In this paper, we examine the energy consumption of a state-of-the-art pocket computer. Using a data acquisition system, we measure the energy consumption of the Itsy Pocket Computer, developed by Compaq Computer Corporation's Palo Alto Research Labs. We begin by showing that the energy usage characteristics of the Itsy differ markedly from that of a notebook computer. Then, since we expect that flexible software environments will become increasingly prevalent on pocket computers, we consider applications running in a Java environment. In particular, we explain some of the Java design tradeoffs applicable to pocket computers, and quantify their energy costs. For the design options we considered and the three workloads we studied, we find a maximum change in energy use of 25%.","Farkas KI,Flinn J,Back G,Grunwald D,Anderson JM",,2000,2000,https://doi.org/10.1145/339331.339421;http://dx.doi.org/10.1145/339331.339421,quantifying the energy consumption of a pocket computer and a java virtual machine,1
1,Quantifying the Energy Consumption of a Pocket Computer and a Java Virtual Machine,,0163-5999,10.1145/345063.339421,https://doi.org/10.1145/345063.339421;http://dx.doi.org/10.1145/345063.339421,,"In this paper, we examine the energy consumption of a state-of-the-art pocket computer. Using a data acquisition system, we measure the energy consumption of the Itsy Pocket Computer, developed by Compaq Computer Corporation's Palo Alto Research Labs. We begin by showing that the energy usage characteristics of the Itsy differ markedly from that of a notebook computer. Then, since we expect that flexible software environments will become increasingly prevalent on pocket computers, we consider applications running in a Java environment. In particular, we explain some of the Java design tradeoffs applicable to pocket computers, and quantify their energy costs. For the design options we considered and the three workloads we studied, we find a maximum change in energy use of 25%.","Farkas KI,Flinn J,Back G,Grunwald D,Anderson JM",SIGMETRICS Perform. Eval. Rev.,2000,2000-06,https://doi.org/10.1145/345063.339421;http://dx.doi.org/10.1145/345063.339421,quantifying the energy consumption of a pocket computer and a java virtual machine,1
3,Cloud Computing Virtual Machine Migration Energy Measuring Research,"9,78145E+12",,10.1145/3387168.3387192,https://doi.org/10.1145/3387168.3387192;http://dx.doi.org/10.1145/3387168.3387192,"Proceedings of the 3rd International Conference on Vision, Image and Signal Processing","This paper research virtual machine migration energy measuring on IPv4/IPv6 network based on cloud computing infrastructure platform, it conducts a research on the energy measuring in IPv4/IPv6 cloud computing platform, and presents a dynamic energy measuring mathematical model based on analyzing CPU energy consumption changes brought by random assignment works. The research determines mathematical model parameter values and it completes the IPv4/IPv6 cloud computing platform virtual machine migration experimentally. This paper achieves the energy consumption of IPv4/IPv6 transition prior-period, mid-period and last-period cloud computing platform virtual machine migration, the conclusions in line with the cloud energy measurement needs, it builds the theoretical foundation for cloud computing platform energy consumption optimize.","Jun L,Jie Z,DingHong P",,2020,2020,https://doi.org/10.1145/3387168.3387192;http://dx.doi.org/10.1145/3387168.3387192,cloud computing virtual machine migration energy measuring research,1
4,ESAVE: Estimating Server and Virtual Machine Energy,"9,78145E+12",,10.1145/3551349.3561170,https://doi.org/10.1145/3551349.3561170;http://dx.doi.org/10.1145/3551349.3561170,Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering,"Sustainable software engineering has received a lot of attention in recent times, as we witness an ever-growing slice of energy use, for example, at data centers, as software systems utilize the underlying infrastructure. Characterizing servers for their energy use accurately without being intrusive, is therefore important to make sustainable software deployment choices. In this paper, we introduce ESAVE which is a machine learning-based approach that leverages a small set of hardware attributes to characterize a server or virtual machineÂ’s energy usage across different levels of utilization. This is based upon an extensive exploration of multiple ML approaches, with a focus on a minimal set of required attributes, while showcasing good accuracy. Early validations show that ESAVE has only around 12% average prediction error, despite being non-intrusive.","Pathania P,Mehra R,Sharma VS,Kaulgud V,Podder S,Burden AP",,2023,2023,https://doi.org/10.1145/3551349.3561170;http://dx.doi.org/10.1145/3551349.3561170,esave estimating server and virtual machine energy,1
6,Program Analysis and Machine Learning Based Approach to Predict Power Consumption of CUDA Kernel,,2376-3639,10.1145/3603533,https://doi.org/10.1145/3603533;http://dx.doi.org/10.1145/3603533,,"General Purpose Graphics Processing Unit (GPGPU) has secured a prominent position in the High-Performance Computing (HPC) world due to its performance gain and programmability. Understanding the relationship between GPU power consumption and program features can aid developers in building energy-efficient sustainable applications. In this work, we propose a static analysis based power model built using machine learning techniques. We have investigated six machine learning models across three NVIDIA GPU architectures: Kepler, Maxwell, and Volta with Random Forest, Extra Trees, Gradient Boosting, CatBoost, and XGBoost, reporting favorable results. We observed that the XGBoost technique based prediction model is the most efficient technique with an R-square value of 0.9646 on Volta Architecture. The dataset used for these techniques includes kernels from different benchmarks suits, sizes, nature (e.g., compute-bound, memory-bound), and complexity (e.g., control divergence, memory access patterns). Experimental results suggest that the proposed solution can help developers precisely predict GPU applications power consumption using program analysis across GPU architectures. Developers can use this approach to refactor their code to build energy-efficient GPU applications.","Alavani G,Desai J,Saha S,Sarkar S",ACM Trans. Model. Perform. Eval. Comput. Syst.,2023,2023-06,https://doi.org/10.1145/3603533;http://dx.doi.org/10.1145/3603533,program analysis and machine learning based approach to predict power consumption of cuda kernel,1
7,Process-Level Power Estimation in VM-Based Systems,"9,78145E+12",,10.1145/2741948.2741971,https://doi.org/10.1145/2741948.2741971;http://dx.doi.org/10.1145/2741948.2741971,Proceedings of the Tenth European Conference on Computer Systems,"Power estimation of software processes provides critical indicators to drive scheduling or power capping heuristics. State-of-the-art solutions can perform coarse-grained power estimation in virtualized environments, typically treating virtual machines (VMs) as a black box. Yet, VM-based systems are nowadays commonly used to host multiple applications for cost savings and better use of energy by sharing common resources and assets.In this paper, we propose a fine-grained monitoring middleware providing real-time and accurate power estimation of software processes running at any level of virtualization in a system. In particular, our solution automatically learns an application-agnostic power model, which can be used to estimate the power consumption of applications.Our middleware implementation, named BitWatts, builds on a distributed actor implementation to collect process usage and infer fine-grained power consumption without imposing any hardware investment (e.g., power meters). BitWatts instances use high-throughput communication channels to spread the power consumption across the VM levels and between machines. Our experiments, based on CPU- and memory-intensive benchmarks running on different hardware setups, demonstrate that BitWatts scales both in number of monitored processes and virtualization levels. This non-invasive monitoring solution therefore paves the way for scalable energy accounting that takes into account the dynamic nature of virtualized environments.","Colmant M,Kurpicz M,Felber P,Huertas L,Rouvoy R,Sobe A",,2015,2015,https://doi.org/10.1145/2741948.2741971;http://dx.doi.org/10.1145/2741948.2741971,process level power estimation in vm based systems,1
8,A Cost Model for IaaS Clouds Based on Virtual Machine Energy Consumption,"9,78858E+12",,,,Proceedings of the XII Brazilian Symposium on Information Systems on Brazilian Symposium on Information Systems: Information Systems in the Cloud Computing Era - Volume 1,"Reduction in data center energy consumption is a constant motivation for IaaS providers. Among all components, CPU appears as a main energy consumer. Although there is a strong relationship between CPU load and its energy consumption, pricing models of popular IaaS providers do not consider this information as a primary and variable element. This paper quantifies the relationship by identifying the individual consumption of virtual CPUs, which form the basis for an allocation cost model. The proposed model, termed Virtual Power, is faced with Amazon EC2 pricing model pointing a cost reduction for IaaS provider and a proportional sharing between users.","Hinz M,Miers CC,Pillon MA,Koslovski GP",,2016,2016,,a cost model for iaas clouds based on virtual machine energy consumption,1
10,Worst-Case Power Integrity Prediction Using Convolutional Neural Network,,1084-4309,10.1145/3564932,https://doi.org/10.1145/3564932;http://dx.doi.org/10.1145/3564932,,"Power integrity analysis is an essential step in power distribution network (PDN) sign-off to ensure the performance and reliability of chips. However, with the growing PDN size and increasing scenarios to be validated, it becomes very time- and resource-consuming to conduct full-stack PDN simulation to check the power integrity for different test vectors. Recently, various works have proposed machine learningÂ–based methods for PDN power integrity prediction, many of which still suffer from large training overhead, inefficiency, or non-scalability. Thus, this article proposed an efficient and scalable framework for the worst-case power integrity prediction, which can handle general tasks including dynamic noise prediction and bump current prediction. The framework first reduces the spatial and temporal redundancy in the PDN and input current vector and then employs efficient feature extraction as well as a novel convolutional neural network architecture to predict the worst-case power integrity. Experimental results show that the proposed framework consistently outperforms the commercial tool and the state-of-the-art machine learning method with only 0.63Â–1.02% mean relative error and 25Â–69Ã— speedup for noise prediction and 0.22Â–1.06% mean relative error and 24Â–64Ã— speedup for bump current prediction.","Dong X,Chen Y,Chen J,Wang Y,Li J,Ni T,Shi Z,Yin X,Zhuo C",ACM Trans. Des. Autom. Electron. Syst.,2023,2023-05,https://doi.org/10.1145/3564932;http://dx.doi.org/10.1145/3564932,worst case power integrity prediction using convolutional neural network,1
11,Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning,,1532-4435,,,,"Accurate reporting of energy and carbon usage is essential for understanding the potential climate impacts of machine learning research. We introduce a framework that makes this easier by providing a simple interface for tracking realtime energy consumption and carbon emissions, as well as generating standardized online appendices. Utilizing this framework, we create a leaderboard for energy efficient reinforcement learning algorithms to incentivize responsible research in this area as an example for other areas of machine learning. Finally, based on case studies using our framework, we propose strategies for mitigation of carbon emissions and reduction of energy consumption. By making accounting easier, we hope to further the sustainable development of machine learning experiments and spur more research into energy efficient algorithms.","Henderson P,Hu J,Romoff J,Brunskill E,Jurafsky D,Pineau J",J. Mach. Learn. Res.,2020,2020-01,,towards the systematic reporting of the energy and carbon footprints of machine learning,1
15,Efficient Power Adaptation against Deep Learning Based Predictive Adversaries,"9,78145E+12",,10.1145/3324921.3328787,https://doi.org/10.1145/3324921.3328787;http://dx.doi.org/10.1145/3324921.3328787,Proceedings of the ACM Workshop on Wireless Security and Machine Learning,"Wireless communication networks are subject to various types of adversarial attacks, which might be passive in the form of eavesdropping, or active in the form of jamming. For the former category, even if the traffic is encrypted, an adversary performing analysis on observed traffic signatures may lead to leakage of the so called contextual information regarding the traffic. New advances in the field of machine learning also result in significantly more complex adversarial units, which may deduce different forms and uses of such contextual information. In this work, we are interested in power adaptation against an intelligent adversary which utilizes deep learning and attempts to perform predictions and time forecasting on the observed traffic traces to estimate the imminent traffic intensities. Based on its traffic predictions, the adversary might possibly activate its jamming mode and utilize its limited power more efficiently to inflict maximal damage. As a method of mitigation, the transmitter may want to increase transmitter power if it expects a higher probability of jamming, and it has a significant amount of upcoming data to transmit. We leverage Lyapunov optimization and virtual queues to meet a certain level of data transmission reliability while also minimizing power consumption.","Ciftcioglu E,Ricos M",,2019,2019,https://doi.org/10.1145/3324921.3328787;http://dx.doi.org/10.1145/3324921.3328787,efficient power adaptation against deep learning based predictive adversaries,1
18,Customization of Virtual Machine Allocation Policy Using K-Means Clustering Algorithm to Minimize Power Consumption in Data Centers,"9,78145E+12",,10.1145/3018896.3018947,https://doi.org/10.1145/3018896.3018947;http://dx.doi.org/10.1145/3018896.3018947,"Proceedings of the Second International Conference on Internet of Things, Data and Cloud Computing","Cloud Computing provides rapid provision of computing resources like processing power, memory, network resources, storage, etc. Running computing resources for longer time, leads energy consumption, increase the emission of Carbon Dioxide (CO2) and increase the expenditure cost for the resources usage. Hence there is a necessity to minimize the execution time to reduce energy consumption in the cloud environment. One of the existing approaches to reducing energy consumption is based on Migration and Placement Policy for Virtual Machine, but still improving placement technique we can further minimize power consumption. In our proposed architecture for cloud resource allocation based on Clustering method, we do map a group of tasks to virtual machines. For clustering, we work on task usage of CPU, memory, and bandwidth. This proposed clustering technique further decreases energy consumption by efficient resource allocation.","Rugwiro U,Chunhua G",,2017,2017,https://doi.org/10.1145/3018896.3018947;http://dx.doi.org/10.1145/3018896.3018947,customization of virtual machine allocation policy using k means clustering algorithm to minimize power consumption in data centers,1
19,Predictive Maintenance Powered by Machine Learning and Simulation,,,,,Proceedings of the Winter Simulation Conference,"To optimize the balance between costs and reliability of cranes, it is important to perform maintenance when the risk of failures becomes high while possibly delaying planned maintenance when the crane shows no signs of possible problems. To accomplish this, we investigate the possibility of applying predictive maintenance for container-handling cranes. The application of predictive maintenance requires historical data collection and preprocessing of equipment sensor and maintenance data. To get a feeling of the possibilities and limitations of predictive maintenance for container-handling cranes, before investing time and money to collect operational data, we have used simulations to generate synthetic data for a few components of the cranes. Using the simulated crane data, a prediction model was trained to predict upcoming component failures. The results show that using simulation we can identify the possibilities and limitations of machine learning for predicting failures of components of the crane.","Burger M,Boer CA,Straub E,Saanen YA",,2023,2023,,predictive maintenance powered by machine learning and simulation,1
20,Perspectives on Predictive Power of Multimodal Deep Learning: Surprises and Future Directions,"9,78197E+12",,10.1145/3107990.3108006,https://doi.org/10.1145/3107990.3108006;http://dx.doi.org/10.1145/3107990.3108006,,,"Bengio S,Deng L,Morency LP,Schuller B",,2018,2018,https://doi.org/10.1145/3107990.3108006;http://dx.doi.org/10.1145/3107990.3108006,perspectives on predictive power of multimodal deep learning surprises and future directions,1
21,Machine Learning GPU Power Measurement on Chameleon Cloud,"9,78145E+12",,10.1145/3147213.3149450,https://doi.org/10.1145/3147213.3149450;http://dx.doi.org/10.1145/3147213.3149450,Proceedings of The10th International Conference on Utility and Cloud Computing,"Machine Learning (ML) is becoming critical for many industrial and scientific endeavors, and has a growing presence in High Performance Computing (HPC) environments. Neural network training requires long execution times for large data sets, and libraries like TensorFlow implement GPU acceleration to reduce the total runtime for each calculation. This tutorial demonstrates how to 1) use Chameleon Cloud to perform comparative studies of ML training performance across different hardware configurations; and 2) run and monitor power utilization of TensorFlow on NVIDIA GPUs.",Chuah JY,,2017,2017,https://doi.org/10.1145/3147213.3149450;http://dx.doi.org/10.1145/3147213.3149450,machine learning gpu power measurement on chameleon cloud,1
24,A Deep Learning Prediction Process Based on Low-Power Heterogeneous Multi Core Architecture,"9,78145E+12",,10.1145/3239576.3239609,https://doi.org/10.1145/3239576.3239609;http://dx.doi.org/10.1145/3239576.3239609,Proceedings of the 2nd International Conference on Advances in Image Processing,"With the rapid development of machine learning both in theory and practice in the past decade. And recently, it is widely used in applications and cloud services. As the emerging field of machine learning, deep learning shows excellent ability in solving complex learning problems. In this paper, we designed a deep learning prediction process based on low-power heterogeneous multi core architecture. Firstly, the fundamental principle of image recognition method based on deep learning reviewed as the basis of the research. Secondly, a set of key algorithm design to parallel access and process image for object detection based on Parallella multi core platform was proposed to improve the detection speed and the computational resource efficiency on single node. Thirdly, Rockchip RK3288 SoC with 4 Arm Cortex-A17 cores hardware platform, Xilinx Zynq and Adapteva Epiphany combined heterogeneous multi core hardware platform was introduced. Some key designs based on Parallella board's architecture to achieve image recognition was proposed to improve the recognition speed and the computational resource efficiency. Finally, The experimental results that based on Parallella board indicate that the proposed image recognition system can achieve nearly 14.8 times speedup than dual-core Arm which was integrated in Parallella board with similar accuracy and achieve 8.6 times speedup than RK3288 board which has the newest series of high-performance Arm core CPU as the control included 4 Arm Cortex-A17 cores.","Li R,Li,Zhang S",,2018,2018,https://doi.org/10.1145/3239576.3239609;http://dx.doi.org/10.1145/3239576.3239609,a deep learning prediction process based on low power heterogeneous multi core architecture,1
25,Learning-Based Power Prediction for Data Centre Operations via Deep Neural Networks,"9,78145E+12",,10.1145/2940679.2940685,https://doi.org/10.1145/2940679.2940685;http://dx.doi.org/10.1145/2940679.2940685,Proceedings of the 5th International Workshop on Energy Efficient Data Centres,"Modelling and analyzing power consumption for data centres can diagnose potential energy-hungry components and applications, and facilitate in-time control, benefiting the energy efficiency of data centers. However, solutions to this problem, including static power models and canonical prediction models, either aim to build a static relationship between power consumption and hardware/application configurations without considering the dynamic fluctuation of power; or simply treat it as time series, ignoring the inherit power data characteristics. To tackle these issues, in this paper, we present a systematic power prediction framework based on extensive power dynamic profiling and deep learning models. In particular, we first analyse different power series samples to illustrate their noise patterns; accordingly we propose a power data de-noising method, which lowers noise interference to the modelling. With the pretreated data, we propose two deep learning based prediction models, including a fine-grained model and a coarse-grained model, which are suitable for different time scales. In the fine-grained prediction model, a recursive autoencoder (AE) is employed for short-duration prediction; in the coarse-grained model, an AE is used to encode massive fine-grained historical data as a further data pretreatment for long-duration prediction. Experimental results show that our proposed models outperform canonical prediction methods with higher accuracy, up to 79% error reduction for certain cases.","Li Y,Hu H,Wen Y,Zhang J",,2016,2016,https://doi.org/10.1145/2940679.2940685;http://dx.doi.org/10.1145/2940679.2940685,learning based power prediction for data centre operations via deep neural networks,1
26,ECO-GNN: Signoff Power Prediction Using Graph Neural Networks with Subgraph Approximation,,1084-4309,10.1145/3569942,https://doi.org/10.1145/3569942;http://dx.doi.org/10.1145/3569942,,"Modern electronic design automation flows depend on both implementation and signoff tools to perform timing-constrained power optimization through Engineering Change Orders (ECOs), which involve gate sizing and threshold-voltage (Vth)-assignment of standard cells. However, the signoff ECO optimization is highly time-consuming, and the power improvement is hard to predict in advance. Ever since the industrial benchmarks released by the ISPD-2012 gate-sizing contest, active research has been conducted extensively to improve the optimization process. Nonetheless, previous works were mostly based on heuristics or analytical methods whose timing models were oversimplified and lacked of formal validations from commercial signoff tools. In this article, we propose ECO-graph neural networks (GNN), a transferable graph-learning-based framework, which harnesses GNNs to perform commercial-quality signoff power optimization through discrete (Vth-assignment. One of the highlights of our framework is that it generates tool-accurate optimization results instantly on unseen netlists that are not utilized in the training process. Furthermore, we propose a subgraph approximation technique to improve training and inferencing time of the proposed GNN model. We show that design instances with non-overlapping subgraphs can be optimized in parallel so as to improve the inference time of the learning-based model. Finally, we implement a GNN-based explanation method to interpret the optimization results achieved by our framework. Experimental results on 14 industrial designs, including a RISC-V-based multi-core system and the renowned ISPD-2012 benchmarks, demonstrate that our framework achieves up to 14Ã— runtime improvement with similar signoff power optimization quality compared with Synopsys PrimeTime, an industry-leading signoff tool.","Lu YC,Nath S,Pentapati S,Lim SK",ACM Trans. Des. Autom. Electron. Syst.,2023,2023-05,https://doi.org/10.1145/3569942;http://dx.doi.org/10.1145/3569942,eco gnn signoff power prediction using graph neural networks with subgraph approximation,1
27,Energy-Demand Estimation of Embedded Devices Using Deep Artificial Neural Networks,"9,78145E+12",,10.1145/3297280.3297338,https://doi.org/10.1145/3297280.3297338;http://dx.doi.org/10.1145/3297280.3297338,Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing,"The need for high performance in embedded devices grows at a breathtaking pace. Embedded processors that satisfy the hunger for superlative processing power share a common issue: the increasing performance leads to growing energy demands during operation. As energy remains a limited resource to embedded devices, it is critical to optimise software components for low power. Low-power software needs energy models which, however, are increasingly difficult to create as to the complexity of today's devices.In this paper we present a black-box approach to construct precise energy models for complex hardware devices. We apply machine-learning techniques in combination with fully automatic energy measurements and evaluate our approach with an ARM Cortex platform. We show that our system estimates the energy demand of program code with a mean percentage error of 1.8% compared to the results of energy measurements.","HÃ¶nig T,Herzog B,SchrÃ¶der-Preikschat W",,2019,2019,https://doi.org/10.1145/3297280.3297338;http://dx.doi.org/10.1145/3297280.3297338,energy demand estimation of embedded devices using deep artificial neural networks,1
28,Uninterruptible Power Supply State of Charge Estimation Based on BP Neural Network,"9,78145E+12",,10.1145/3446999.3447639,https://doi.org/10.1145/3446999.3447639;http://dx.doi.org/10.1145/3446999.3447639,Proceedings of the 2020 8th International Conference on Information Technology: IoT and Smart City,"Objetive: Display console is the software and hardware platform of warship and submarine control system. It's very important to strengthen the performance and improve the life of the Uninterruptible power supply (UPS) in display console . The residual capacity of UPS is a nonlinear function of voltage, discharge current, temperature and other variables. At present, there are some problems such as large measurement error and poor state prediction, what influence the battery power management system's management effectiveness. Therefore, this paper studies the estimation method of battery residual capacity based on BP Algorithm according to the principle of neural network, so as to improve the estimation accuracy of UPS residual capacity. Method: Firstly, we establish the BP neural network model consists of three layers, secondly gather UPS experimental data set, then build, train and test BP network model with LM algorithm by Matlab program language, finally gather the state of charge(SOC) training results. Result: Experimental results indicate that the method in this paper can estimate UPS SOC accurately and efficiently. Conclusion: It can satisfy the system requirements of uninterruptible power supply SOC estimation.",Gong H,,2021,2021,https://doi.org/10.1145/3446999.3447639;http://dx.doi.org/10.1145/3446999.3447639,uninterruptible power supply state of charge estimation based on bp neural network,1
29,Continuous Low-Power Ammonia Monitoring Using Long Short-Term Memory Neural Networks,"9,78145E+12",,10.1145/3274783.3274836,https://doi.org/10.1145/3274783.3274836;http://dx.doi.org/10.1145/3274783.3274836,Proceedings of the 16th ACM Conference on Embedded Networked Sensor Systems,"Accurate and continuous ammonia monitoring is important for laboratory animal studies and many other applications. Existing solutions are often expensive, inaccurate, or unsuitable for long-term monitoring. In this work, we propose a new ammonia monitoring approach that is low-power, automatic, accurate, and wireless.Our system uses metal oxide sensors which change their electrical resistance due to an induced reduction reaction with ammonia at high temperatures. Traditional methods infer the ammonia level by measuring the sensor's electrical resistance after it reaches equilibrium. Such a system consumes a significant amount of energy because reaching equilibrium requires heating the sensor for minutes. Our proposed approach does not wait for equilibrium, but tries to predict the resistance at equilibrium using the sensor's initial resistance response curve in a very short heating pulse (as short as 200ms). The prediction model is built on long short-term memory (LSTM) neural networks.We built 38 prototype sensors and a home-grown gas flow system. In a 3-month in-lab testing period, we conducted extensive experiments and collected 13,770 measurements. Our model accurately predicts the equilibrium state resistance value, with an average error rate of 0.12%. The final average estimation error for the ammonia concentration level is 9.38ppm. Given the ultra low power consumption and accurate measurements, we have partnered with cage vendors and deployed our system at two animal research facilities (NIH and Cornell University) for month-long medical trials.","Jia Z,Lyu X,Zhang W,Martin RP,Howard RE,Zhang Y",,2018,2018,https://doi.org/10.1145/3274783.3274836;http://dx.doi.org/10.1145/3274783.3274836,continuous low power ammonia monitoring using long short term memory neural networks,1
31,Improving BLE Distance Estimation and Classification Using TX Power and Machine Learning: A Comparative Analysis,"9,78145E+12",,10.1145/3127540.3127577,https://doi.org/10.1145/3127540.3127577;http://dx.doi.org/10.1145/3127540.3127577,"Proceedings of the 20th ACM International Conference on Modelling, Analysis and Simulation of Wireless and Mobile Systems","Distance estimation and proximity classification techniques are essential for numerous IoT applications and in providing efficient services in smart cities. Bluetooth Low Energy (BLE) is designed for IoT devices, and its received signal strength indicator (RSSI) has been used in distance and proximity estimation, though they are noisy and unreliable. In this study, we leverage the BLE TX power level in BLE models.We adopt a comparative analysis framework that utilizes our extensive data library of measurements. It considers commonly used state-of-the-art model, in addition to our data-driven proposed approach. The RSSI and TX power are integrated into several parametric models such as log shadowing and Android Beacon library models, and machine learning models such as linear regression, decision trees, random forests and neural networks. Specific mobile apps are developed for the study experiment. We have collected more than 1.8 millions of BLE records between encounters with various distances that range from 0.5 to 22 meters in an indoor environment. Interestingly, considering TX power when estimating the distance reduced the mean errors by up to 46% in parametric models and by up to 35% in machine learning models. Also, the proximity classification accuracy increased by up to 103% and 70% in parametric and machine learning models, respectively. This work is one of the first studies (if not the first) that analyze in depth the TX power variations in improving the distance estimation and classification.","Al Qathrady M,Helmy A",,2017,2017,https://doi.org/10.1145/3127540.3127577;http://dx.doi.org/10.1145/3127540.3127577,improving ble distance estimation and classification using tx power and machine learning a comparative analysis,1
32,Towards Software-Adaptive Green Computing Based on Server Power Consumption,"9,78145E+12",,10.1145/2593743.2593745,https://doi.org/10.1145/2593743.2593745;http://dx.doi.org/10.1145/2593743.2593745,Proceedings of the 3rd International Workshop on Green and Sustainable Software,"With the proliferation of virtualization and cloud comput- ing, optimizing the power usage effectiveness of enterprise data centers has become a laudable goal and a critical re- quirement in IT operations all over the world. While a sig- nificant body of research exists to measure, monitor, and control the greenness level of hardware components, signif- icant research efforts are needed to relate hardware energy consumption to energy consumption due to program exe- cution. In this paper we report on our investigations to characterize power consumption profiles for different types of compute and memory intensive software applications. In particular, we focus on studying the effects of CPU loads on the power consumption of compute servers by monitoring rack power consumption in a data center. We conducted a series of experiments with a variety of processes of differ- ent complexity to understand and characterize the effect on power consumption. Combining processes of varying com- plexity with varying resource allocations produces different energy consumption levels. The challenge is to optimize pro- cess orchestration based on a power consumption framework to accrue energy savings. Our ultimate goal is to develop smart adaptive green computing techniques, such as adap- tive job scheduling and resource provisioning, to reduce over- all power consumption in data centers or clouds.","Bergen A,Desmarais R,Ganti S,Stege U",,2014,2014,https://doi.org/10.1145/2593743.2593745;http://dx.doi.org/10.1145/2593743.2593745,towards software adaptive green computing based on server power consumption,1
36,A Methodology to Predict the Power Consumption of Servers in Data Centres,"9,78145E+12",,10.1145/2318716.2318718,https://doi.org/10.1145/2318716.2318718;http://dx.doi.org/10.1145/2318716.2318718,Proceedings of the 2nd International Conference on Energy-Efficient Computing and Networking,"Until recently, there have been relatively few studies exploring the power consumption of ICT resources in data centres. In this paper, we propose a methodology to capture the behaviour of most relevant energy-related ICT resources in data centres and present a generic model for them. This is achieved by decomposing the design process into four modelling phases. Furthermore, unlike the state-of-the-art approaches, we provide detailed power consumption models at server and storage levels. We evaluate our model for different types of servers and show that it suffers from an error rate of 2% in the best case, and less than 10% in the worst case.","Basmadjian R,Ali N,Niedermeier F,de Meer H,Giuliani G",,2011,2011,https://doi.org/10.1145/2318716.2318718;http://dx.doi.org/10.1145/2318716.2318718,a methodology to predict the power consumption of servers in data centres,1
37,Predicting Server Power Consumption from Standard Rating Results,"9,78145E+12",,10.1145/3297663.3310298,https://doi.org/10.1145/3297663.3310298;http://dx.doi.org/10.1145/3297663.3310298,Proceedings of the 2019 ACM/SPEC International Conference on Performance Engineering,"Data center providers and server operators try to reduce the power consumption of their servers. Finding an energy efficient server for a specific target application is a first step in this regard. Estimating the power consumption of an application on an unavailable server is difficult, as nameplate power values are generally overestimations. Offline power models are able to predict the consumption accurately, but are usually intended for system design, requiring very specific and detailed knowledge about the system under consideration.In this paper, we introduce an offline power prediction method that uses the results of standard power rating tools. The method predicts the power consumption of a specific application for multiple load levels on a target server that is otherwise unavailable for testing. We evaluate our approach by predicting the power consumption of three applications on different physical servers. Our method is able to achieve an average prediction error of 9.49% for three workloads running on real-world, physical servers.","von Kistowski J,Grohmann J,Schmitt N,Kounev S",,2019,2019,https://doi.org/10.1145/3297663.3310298;http://dx.doi.org/10.1145/3297663.3310298,predicting server power consumption from standard rating results,1
40,Fast Yet Accurate Timing and Power Prediction of Artificial Neural Networks Deployed on Clock-Gated Multi-Core Platforms,,,10.1145/3579170.3579263,https://doi.org/10.1145/3579170.3579263;http://dx.doi.org/10.1145/3579170.3579263,Proceedings of the DroneSE and RAPIDO: System Engineering for Constrained Embedded Systems,"When deploying Artificial Neural Networks (ANNs) onto multi-core embedded platforms, an intensive evaluation flow is necessary to find implementations that optimize resource usage, timing and power. ANNs require indeed significant amounts of computational and memory resources to execute, while embedded execution platforms offer limited resources with strict power budget. Concurrent accesses from processors to shared resources on multi-core platforms can lead to bottlenecks with impact on performance and power. Existing approaches show limitations to deliver fast yet accurate evaluation ahead of ANN deployment on the targeted hardware. In this paper, we present a modeling flow for timing and power prediction in early design stage of fully-connected ANNs on multi-core platforms. Our flow offers fast yet accurate predictions with consideration of shared communication resources and scalability in regards of the number of cores used. The flow is evaluated on real measurements for 42 mappings of 3 fully-connected ANNs executed on a clock-gated multi-core platform featuring two different communication modes: polling or interrupt-based. Our modeling flow predicts timing with accuracy and power with accuracy on the tested mappings for an average simulation time of 0.23 s for 100 iterations. We then illustrate the application of our approach for efficient design space exploration of ANN implementations.","Dariol Q,Le Nours S,Helms D,Stemmer R,Pillement S,GrÃ¼ttner K",,2023,2023,https://doi.org/10.1145/3579170.3579263;http://dx.doi.org/10.1145/3579170.3579263,fast yet accurate timing and power prediction of artificial neural networks deployed on clock gated multi core platforms,1
44,Reducing Power Consumption during Server Maintenance on Edge Computing Infrastructures,"9,78145E+12",,10.1145/3555776.3577739,https://doi.org/10.1145/3555776.3577739;http://dx.doi.org/10.1145/3555776.3577739,Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing,"Edge servers must routinely undergo maintenance to ensure the environment's performance and security. During maintenance, applications hosted by outdated servers must be relocated to alternative servers to avoid downtime. In distributed edges with servers spread across large regions, ensuring that applications are not migrated to servers too far away from their users to avoid high latency hardens the maintenance planning. In addition, the limited power supply of edge sites restricts the list of suitable alternative hosts for the applications even further. Past work has focused on optimizing maintenance or increasing the power efficiency of edge computing infrastructures. However, no work addresses both objectives together. This paper presents Emma, a maintenance strategy that reduces power consumption during edge server maintenance without excessively extending maintenance time or increasing application latency. Experiments show that Emma can minimize power consumption during maintenance by up to 26.48% compared to strategies from the literature.","Rubin F,Souza P,Ferreto T",,2023,2023,https://doi.org/10.1145/3555776.3577739;http://dx.doi.org/10.1145/3555776.3577739,reducing power consumption during server maintenance on edge computing infrastructures,1
45,Runtime Energy Consumption Estimation for Server Workloads Based on Chaotic Time-Series Approximation,,1544-3566,10.1145/2355585.2355588,https://doi.org/10.1145/2355585.2355588;http://dx.doi.org/10.1145/2355585.2355588,,"This article proposes a runtime model that relates server energy consumption to its overall thermal envelope, using hardware performance counters and experimental measurements. While previous studies have attempted system-wide modeling of server power consumption through subsystem models, our approach is different in that it links system energy input to subsystem energy consumption based on a small set of tightly correlated parameters. The proposed model takes into account processor power, bus activities, and system ambient temperature for real-time prediction on the power consumption of long running jobs. Using the HyperTransport and QuickPath Link structures as case studies and through electrical measurements on example server subsystems, we develop a chaotic time-series approximation for runtime power consumption, arriving at the Chaotic Attractor Predictor (CAP). With polynomial time complexity, CAP exhibits high prediction accuracy, having the prediction errors within 1.6% (or 3.3%) for servers based on the HyperTransport bus (or the QuickPath Links), as verified by a set of common processor benchmarks. Our CAP is a superior predictive mechanism over existing linear auto-regressive methods, which require expensive and complex corrective steps to address the nonlinear and chaotic aspects of the underlying physical system.","Lewis AW,Tzeng NF,Ghosh S",ACM Trans. Archit. Code Optim.,2012,2012-10,https://doi.org/10.1145/2355585.2355588;http://dx.doi.org/10.1145/2355585.2355588,runtime energy consumption estimation for server workloads based on chaotic time series approximation,1
47,Totally Green: Evaluating and Designing Servers for Lifecycle Environmental Impact,"9,78145E+12",,10.1145/2150976.2150980,https://doi.org/10.1145/2150976.2150980;http://dx.doi.org/10.1145/2150976.2150980,Proceedings of the Seventeenth International Conference on Architectural Support for Programming Languages and Operating Systems,"The environmental impact of servers and datacenters is an important future challenge. System architects have traditionally focused on operational energy as a proxy for designing green servers, but this ignores important environmental implications from server production (materials, manufacturing, etc.). In contrast, this paper argues for a lifecycle focus on the environmental impact of future server designs, to include both operation and production. We present a new methodology to quantify the total environmental impact of system design decisions. Our approach uses the thermodynamic metric of exergy consumption, adapted and validated for use by system architects. Using this methodology, we evaluate the lifecycle impact of several example system designs with environment-friendly optimizations. Our results show that environmental impact from production can be important (around 20% on current servers and growing) and system design choices can reduce this component (by 30--40%). Our results also highlight several, sometimes unexpected, cross-interactions between the environmental impact of production and operation that further motivate a total lifecycle emphasis for future green server designs.","Chang J,Meza J,Ranganathan P,Shah A,Shih R,Bash C",,2012,2012,https://doi.org/10.1145/2150976.2150980;http://dx.doi.org/10.1145/2150976.2150980,totally green evaluating and designing servers for lifecycle environmental impact,1
48,Totally Green: Evaluating and Designing Servers for Lifecycle Environmental Impact,,0163-5964,10.1145/2189750.2150980,https://doi.org/10.1145/2189750.2150980;http://dx.doi.org/10.1145/2189750.2150980,,"The environmental impact of servers and datacenters is an important future challenge. System architects have traditionally focused on operational energy as a proxy for designing green servers, but this ignores important environmental implications from server production (materials, manufacturing, etc.). In contrast, this paper argues for a lifecycle focus on the environmental impact of future server designs, to include both operation and production. We present a new methodology to quantify the total environmental impact of system design decisions. Our approach uses the thermodynamic metric of exergy consumption, adapted and validated for use by system architects. Using this methodology, we evaluate the lifecycle impact of several example system designs with environment-friendly optimizations. Our results show that environmental impact from production can be important (around 20% on current servers and growing) and system design choices can reduce this component (by 30--40%). Our results also highlight several, sometimes unexpected, cross-interactions between the environmental impact of production and operation that further motivate a total lifecycle emphasis for future green server designs.","Chang J,Meza J,Ranganathan P,Shah A,Shih R,Bash C",SIGARCH Comput. Archit. News,2012,2012-03,https://doi.org/10.1145/2189750.2150980;http://dx.doi.org/10.1145/2189750.2150980,totally green evaluating and designing servers for lifecycle environmental impact,1
49,Totally Green: Evaluating and Designing Servers for Lifecycle Environmental Impact,,0362-1340,10.1145/2248487.2150980,https://doi.org/10.1145/2248487.2150980;http://dx.doi.org/10.1145/2248487.2150980,,"The environmental impact of servers and datacenters is an important future challenge. System architects have traditionally focused on operational energy as a proxy for designing green servers, but this ignores important environmental implications from server production (materials, manufacturing, etc.). In contrast, this paper argues for a lifecycle focus on the environmental impact of future server designs, to include both operation and production. We present a new methodology to quantify the total environmental impact of system design decisions. Our approach uses the thermodynamic metric of exergy consumption, adapted and validated for use by system architects. Using this methodology, we evaluate the lifecycle impact of several example system designs with environment-friendly optimizations. Our results show that environmental impact from production can be important (around 20% on current servers and growing) and system design choices can reduce this component (by 30--40%). Our results also highlight several, sometimes unexpected, cross-interactions between the environmental impact of production and operation that further motivate a total lifecycle emphasis for future green server designs.","Chang J,Meza J,Ranganathan P,Shah A,Shih R,Bash C",SIGPLAN Not.,2012,2012-03,https://doi.org/10.1145/2248487.2150980;http://dx.doi.org/10.1145/2248487.2150980,totally green evaluating and designing servers for lifecycle environmental impact,1
51,I've Got the Power's Value! A Computational Model to Evaluate the Interlocutor's Behaviors in Collaborative Negotiation: Socially Interactive Agents Track,,,,,Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems,"We present in this paper a simulation-oriented theory of mind model for interpreting behaviors of power during a collaborative negotiation. This model relies on a model of negotiation that allows an agent to express behaviors of power through its strategy of negotiation. Based on the simulation theory, we adapted the decision model of the agent to reason about its interlocutor's behavior. A preliminary evaluation in the context of agent-agent interaction shows that the system correctly predicts the interlocutor's power.","OuldOuali L,Sabouret N,Rich C",,2018,2018,,i ve got the power s value a computational model to evaluate the interlocutor s behaviors in collaborative negotiation socially interactive agents track,1
52,Optimizing Power Consumption in Cloud Computing Based on Optimization and Predictive Analysis,"9,78145E+12",,10.1145/3164541.3164608,https://doi.org/10.1145/3164541.3164608;http://dx.doi.org/10.1145/3164541.3164608,Proceedings of the 12th International Conference on Ubiquitous Information Management and Communication,"Due to the budget and the environmental issues, achieving energy efficiency gradually receives a lot of attentions these days. In our previous research, a prediction technique has been developed to improve the monitoring statistics. In this research, by adopting the predictive monitoring information, our new proposal can perform the optimization to solve the energy issue of cloud computing. Actually, the optimization technique, which is convex optimization, is coupled with the proposed prediction method to produce a near-optimal set of hosting physical machines. After that, a corresponding migrating instruction can be created eventually. Based on this instruction, the cloud orchestrator can suitably relocate virtual machines to a designed subset of infrastructure. Subsequently, the idle physical servers can be turned off in an appropriate manner to save the power as well as maintain the system performance. For the purpose of evaluation, an experiment is conducted based on 29-day period of Google traces. By utilizing this evaluation, the proposed approach shows the potential to significantly reduce the power consumption without affecting the quality of services.","Bui DM,Huh EN,Lee S",,2018,2018,https://doi.org/10.1145/3164541.3164608;http://dx.doi.org/10.1145/3164541.3164608,optimizing power consumption in cloud computing based on optimization and predictive analysis,1
53,Quantifying the Energy Efficiency Challenges of Achieving Exascale Computing,"9,78148E+12",,10.1109/CCGrid.2015.130,https://doi.org/10.1109/CCGrid.2015.130;http://dx.doi.org/10.1109/CCGrid.2015.130,"Proceedings of the 15th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing","Power and performance are two potentially opposing objectives in the design of a supercomputer, where increases in performance often come at the cost of increased power consumption and vice versa. The task of simultaneously maximising both objectives is becoming an increasingly prominent challenge in the development of future exascale supercomputers. To gain some perspective on the scale of the challenge, we analyse the power and performance trends for the Top500 and Green500 supercomputer lists. We then present the Pa PW metric, which we use to evaluate the scalability of power efficiency, projecting the development of an exascale system. From this analysis, we found that when both power and performance are considered, the projected date of achieving an exascale system falls far beyond the current target of 2020.","Mair J,Huang Z,Eyers D,Chen Y",,2015,2015,https://doi.org/10.1109/CCGrid.2015.130;http://dx.doi.org/10.1109/CCGrid.2015.130,quantifying the energy efficiency challenges of achieving exascale computing,1
54,Artificial Neural Network Performance Evaluation for a Hybrid Power Domain Orthogonal / Non-Orthogonal Multiple Access (OMA / NOMA) System,"9,78145E+12",,10.1145/3416011.3424760,https://doi.org/10.1145/3416011.3424760;http://dx.doi.org/10.1145/3416011.3424760,"Proceedings of the 17th ACM Symposium on Performance Evaluation of Wireless Ad Hoc, Sensor, & Ubiquitous Networks","Next-generation wireless technologies face considerable challenges in terms of providing the required latency and connectivity for new heterogeneous mobile networks. Driven by these problems, this study focuses on increasing user connectivity together with system throughput. For doing so, we propose and evaluate a hybrid machine learning-driven orthogonal/non-orthogonal multiple access (OMA/NOMA) system. In this work, we use an artificial neural network (ANN) to assign an OMA or NOMA access method to each user equipment (UE). As part of this research we also evaluate the accuracy and training time of the three most relevant learning algorithms of ANN (L-M, BFGS, and OSS). The main objective is to increase the sum-rate of the mobile network in the introduced beamforming and mmWave channel environment.Simulation results show up to a $20%$ sum-rate average performance increase of the system using the ANN management in contrast to a random non-ANN managed system. The Leveberg-Marquard (L-M) training algorithm is the best overall algorithm for this proposed application as presents the highest accuracy of around $77%$ despite 37 minutes of training and lower accuracy of $73%$ with approximately 28 seconds of training time.","Belesaca JD,Avila-Campos P,Vazquez-Rodas A",,2020,2020,https://doi.org/10.1145/3416011.3424760;http://dx.doi.org/10.1145/3416011.3424760,artificial neural network performance evaluation for a hybrid power domain orthogonal non orthogonal multiple access oma noma system,1
55,BitWatts: A Process-Level Power Monitoring Middleware,"9,78145E+12",,10.1145/2678508.2678529,https://doi.org/10.1145/2678508.2678529;http://dx.doi.org/10.1145/2678508.2678529,Proceedings of the Posters and Demos Session of the 15th International Middleware Conference,"Power estimation of software processes provides critical indicators to drive scheduling or power capping heuristics. State-of-the-art power estimation solutions only provide coarse-grained support for power estimation. In this paper, we therefore propose a middleware for assembling and configuring software-defined power meters. Software-defined power meters provide real-time and accurate power estimation of software processes. In particular, our solution automatically learns an application-agnostic power model, which can be used to estimate the power consumption of applications.Our approach, named BitWatts, builds on a distributed actor middleware to collect process usage and infer fine-grained power consumption without imposing any hardware investment (e.g., power meters).","Colmant M,Kurpicz M,Felber P,Huertas L,Rouvoy R,Sobe A",,2014,2014,https://doi.org/10.1145/2678508.2678529;http://dx.doi.org/10.1145/2678508.2678529,bitwatts a process level power monitoring middleware,1
56,Application Power Consumption Estimation Considering Software Dependency in Android,"9,78145E+12",,10.1145/3022227.3022312,https://doi.org/10.1145/3022227.3022312;http://dx.doi.org/10.1145/3022227.3022312,Proceedings of the 11th International Conference on Ubiquitous Information Management and Communication,"Android operating system has become one of the most popular smartphone platforms. One report stated that the most important issue of smartphones was its power consumption. Android has a function with which an application can be invoked in screen-off state without user's operation. Some applications frequently work in screen-off state, heavily and consume battery. For saving power consumption in the state, accurate estimation of power consumption of each application is important. However, estimating power consumption cannot be easily achieved because of its dependency on device. That is, application's power consumption varies on installed application and type of hardware module in the device, which can be called software and hardware dependency, respectively. In this paper, we discuss estimation of power consumption in screen-off state considering software dependency. First, we explain software dependency of power consumption. Second, we propose a method, which takes account of software dependency, for estimating power consumption due to GPS. The proposed method monitors GPS utilization individually. Third, we evaluate our method with a benchmark and practical applications using GPS. We then demonstrate that our method can estimate power consumption of each application and suitably predict consumption after uninstalling an application without uninstallation.","Kurihara S,Fukuda S,Hamanaka S,Oguchi M,Yamaguchi S",,2017,2017,https://doi.org/10.1145/3022227.3022312;http://dx.doi.org/10.1145/3022227.3022312,application power consumption estimation considering software dependency in android,1
57,Modeling the Power Consumption of Audio Signal Processing Computations Using Customized Numerical Representations,"9,78077E+12",,,,Proceedings of the 36th Annual Symposium on Simulation,"This paper explores the impact that numericalrepresentation has on the power consumption of audiosignal processing applications. The motivation is digitalhearing aids, for which minimizing the powerconsumption is a critical design goal. We investigate twoaspects of this problem. First, we evaluate the validity ofusing signal transition counts to model actual powerconsumption within this problem domain, and second, wecompare the relative power consumption of multiply-accumulateoperations for several customized numericalrepresentations.","Chamberlain R,Hemmeter E,Morley R,White J",,2003,2003,,modeling the power consumption of audio signal processing computations using customized numerical representations,1
58,Evaluation of Energy Consumption of Replicated Tasks in a Volunteer Computing Environment,"9,78145E+12",,10.1145/3185768.3186313,https://doi.org/10.1145/3185768.3186313;http://dx.doi.org/10.1145/3185768.3186313,Companion of the 2018 ACM/SPEC International Conference on Performance Engineering,"High Throughput Computing allows workloads of many thousands of tasks to be performed efficiently over many distributed resources and frees the user from the laborious process of managing task deployment, execution and result collection. However, in many cases the High Throughput Computing system is comprised from volunteer computational resources where tasks may be evicted by the owner of the resource. This has two main disadvantages. First, tasks may take longer to run as they may require multiple deployments before finally obtaining enough time on a resource to complete. Second, the wasted computation time will lead to wasted energy. We may be able to reduce the effect of the first disadvantage here by submitting multiple replicas of the task and take the results from the first one to complete. This, though, could lead to a significant increase in energy consumption. Thus we desire to only ever submit the minimum number of replicas required to run the task in the allocated time whilst simultaneously minimising energy. In this work we evaluate the use of fixed replica counts and Reinforcement Learning on the proportion of task which fail to finish in a given time-frame and the energy consumed by the system.","McGough AS,Forshaw M",,2018,2018,https://doi.org/10.1145/3185768.3186313;http://dx.doi.org/10.1145/3185768.3186313,evaluation of energy consumption of replicated tasks in a volunteer computing environment,1
59,Accounting for the Energy Consumption of Personal Computing Including Portable Devices,"9,78145E+12",,10.1145/1791314.1791337,https://doi.org/10.1145/1791314.1791337;http://dx.doi.org/10.1145/1791314.1791337,Proceedings of the 1st International Conference on Energy-Efficient Computing and Networking,"In light of the increased awareness of global energy consumption, questions are also being asked about the contribution of computing equipment. Though studies have documented the share of energy consumption due to these equipment over the years, these have rarely characterized the increasing share contributed by the rapidly growing portable computing segment. The portable computing device segment is widely predicted to be a dominant mode of computing and communication, and accounting for its energy consumption is necessary for energy-efficient computing in the future. This work takes a fresh and updated look at the energy consumption due to computing devices in perspective of global consumption, and pays special attention to the contribution of portable computing devices. We further quantify the impact of energy consumed by the computing sector on the environment, as well as on the electricity cost to an average residential consumer. Finally, based on the results of the study, recommendations targeted at the computer networking community are made.","Somavat P,Jadhav S,Namboodiri V",,2010,2010,https://doi.org/10.1145/1791314.1791337;http://dx.doi.org/10.1145/1791314.1791337,accounting for the energy consumption of personal computing including portable devices,1
60,Fine-Grain Compensation Method with Consideration of Trade-Offs between Computation and Data Transfer for Power Consumption,,0163-5964,10.1145/1360464.1360475,https://doi.org/10.1145/1360464.1360475;http://dx.doi.org/10.1145/1360464.1360475,,"Fine-grain parallelizing method with consideration of the number of data transfers for low power consumption is proposed. In the proposed method, power consumption by data transfers between processor elements in a multiprocessor is focused on, and the number of data transfers is reduced.In this paper, a measure based on the relationship between variables in a given program is defined to evaluate the number of data transfers, firstly. And then a proposed compensation method by use of the evaluation of power consumption based on the measure is explained. Finally, the result of applying proposed compensation method implemented on COINS framework to several example programs is shown.","Miyoshi T,Sugino N",SIGARCH Comput. Archit. News,2007,2007-12,https://doi.org/10.1145/1360464.1360475;http://dx.doi.org/10.1145/1360464.1360475,fine grain compensation method with consideration of trade offs between computation and data transfer for power consumption,1
61,Analysis of the Influences on Server Power Consumption and Energy Efficiency for CPU-Intensive Workloads,"9,78145E+12",,10.1145/2668930.2688057,https://doi.org/10.1145/2668930.2688057;http://dx.doi.org/10.1145/2668930.2688057,Proceedings of the 6th ACM/SPEC International Conference on Performance Engineering,"Energy efficiency of servers has become a significant research topic over the last years, as server energy consumption varies depending on multiple factors, such as server utilization and workload type. Server energy analysis and estimation must take all relevant factors into account to ensure reliable estimates and conclusions. Thorough system analysis requires benchmarks capable of testing different system resources at different load levels using multiple workload types. Server energy estimation approaches, on the other hand, require knowledge about the interactions of these factors for the creation of accurate power models. Common approaches to energy-aware workload classification categorize workloads depending on the resource types used by the different workloads. However, they rarely take into account differences in workloads targeting the same resources. Industrial energy-efficiency benchmarks typically do not evaluate the system's energy consumption at different resource load levels, and they only provide data for system analysis at maximum system load.In this paper, we benchmark multiple server configurations using the CPU worklets included in SPEC's Server Efficiency Rating Tool (SERT). We evaluate the impact of load levels and different CPU workloads on power consumption and energy efficiency. We analyze how functions approximating the measured power consumption differ over multiple server configurations and architectures. We show that workloads targeting the same resource can differ significantly in their power draw and energy efficiency. The power consumption of a given workload type varies depending on utilization, hardware and software configuration. The power consumption of CPU-intensive workloads does not scale uniformly with increased load, nor do hardware or software configuration changes affect it in a uniform manner.","v. Kistowski J,Block H,Beckett J,Lange KD,Arnold JA,Kounev S",,2015,2015,https://doi.org/10.1145/2668930.2688057;http://dx.doi.org/10.1145/2668930.2688057,analysis of the influences on server power consumption and energy efficiency for cpu intensive workloads,1
62,A Measurement-Based Analysis of the Energy Consumption of Data Center Servers,"9,78145E+12",,10.1145/2602044.2602061,https://doi.org/10.1145/2602044.2602061;http://dx.doi.org/10.1145/2602044.2602061,Proceedings of the 5th International Conference on Future Energy Systems,"Energy consumption is a growing issue in data centers, impacting their economic viability and their public image. In this work we empirically characterize the power and energy consumed by different types of servers. In particular, in order to understand the behavior of their energy and power consumption, we perform measurements in different servers. In each of them, we exhaustively measure the power consumed by the CPU, the disk, and the network interface under different configurations, identifying the optimal operational levels. One interesting conclusion of our study is that the curve that defines the minimal CPU power as a function of the load is neither linear nor purely convex as has been previously assumed. Moreover, we find that the efficiency of the various server components can be maximized by tuning the CPU frequency and the number of active cores as a function of the system and network load, while the block size of I/O operations should be always maximized by applications. We also show how to estimate the energy consumed by an application as a function of some simple parameters, like the CPU load, and the disk and network activity. We validate the proposed approach by accurately estimating the energy of a map-reduce computation in a Hadoop platform.","Arjona Aroca J,Chatzipapas A,FernÃ¡ndez Anta A,Mancuso V",,2014,2014,https://doi.org/10.1145/2602044.2602061;http://dx.doi.org/10.1145/2602044.2602061,a measurement based analysis of the energy consumption of data center servers,1
64,Power Consumption Models for Multi-Tenant Server Infrastructures,,1544-3566,10.1145/3148965,https://doi.org/10.1145/3148965;http://dx.doi.org/10.1145/3148965,,"Multi-tenant virtualized infrastructures allow cloud providers to minimize costs through workload consolidation. One of the largest costs is power consumption, which is challenging to understand in heterogeneous environments. We propose a power modeling methodology that tackles this complexity using a divide-and-conquer approach. Our results outperform previous research work, achieving a relative error of 2% on average and under 4% in almost all cases. Models are portable across similar architectures, enabling predictions of power consumption before migrating a tenant to a different hardware platform. Moreover, we show the models allow us to evaluate colocations of tenants to reduce overall consumption.","Ferroni M,Corna A,Damiani A,Brondolin R,Colmenares JA,Hofmeyr S,Kubiatowicz JD,Santambrogio MD",ACM Trans. Archit. Code Optim.,2017,2017-11,https://doi.org/10.1145/3148965;http://dx.doi.org/10.1145/3148965,power consumption models for multi tenant server infrastructures,1
65,Quantifying the Impact of Frequency Scaling on the Energy Efficiency of the Single-Chip Cloud Computer,"9,78398E+12",,,,"Proceedings of the Conference on Design, Automation and Test in Europe","Dynamic frequency and voltage scaling (DVFS) techniques have been widely used for meeting energy constraints. Single-chip many-core systems bring new challenges owing to the large number of operating points and the shift to message passing interface (MPI) from shared memory communication. DVFS, however, has been mostly studied on single-chip systems with one or few cores, without considering the impact of the communication among cores. This paper evaluates the impact of frequency scaling on the performance and power of many-core systems with MPI. We conduct experiments on the Single-Chip Cloud Computer (SCC), an experimental many-core processor developed by Intel. The paper first introduces the run-time monitoring infrastructure and the application suite we have designed for an in-depth evaluation of the SCC. We provide an extensive analysis quantifying the effects of frequency perturbations on performance and energy efficiency. Experimental results show that run-time communication patterns lead to significant differences in power/performance tradeoffs in many-core systems with MPI.","Bartolini A,Sadri M,Furst JN,Coskun AK,Benini L",,2012,2012,,quantifying the impact of frequency scaling on the energy efficiency of the single chip cloud computer,1
67,Evaluating Performance and Energy in File System Server Workloads,,,,,Proceedings of the 8th USENIX Conference on File and Storage Technologies,"Recently, power has emerged as a critical factor in designing components of storage systems, especially for power-hungry data centers. While there is some research into power-aware storage stack components, there are no systematic studies evaluating each component's impact separately. This paper evaluates the file system's impact on energy consumption and performance. We studied several popular Linux file systems, with various mount and format options, using the FileBench workload generator to emulate four server workloads: Web, database, mail, and file server. In case of a server node consisting of a single disk, CPU power generally exceeds disk-power consumption. However, file system design, implementation, and available features have a significant effect on CPU/disk utilization, and hence on performance and power. We discovered that default file system options are often suboptimal, and even poor. We show that a carefulmatching of expectedworkloads to file system types and options can improve power-performance efficiency by a factor ranging from 1.05 to 9.4 times.","Sehgal P,Tarasov V,Zadok E",,2010,2010,,evaluating performance and energy in file system server workloads,1
68,Adaptive Power Panel of Cloud Computing Controlling Cloud Power Consumption,"9,78145E+12",,10.1145/2944165.2944167,https://doi.org/10.1145/2944165.2944167;http://dx.doi.org/10.1145/2944165.2944167,Proceedings of the 2nd Africa and Middle East Conference on Software Engineering,"Cloud computing had created a new era of network design, where end-users can get their required services without having to purchase expensive infrastructure or even to care about troubleshooting. Power consumption is a challenge facing the Cloud Providers to operate their Datacenters. One solution to overcome this is the Virtual Machine (VM) migration, which is a technique used to switch under-utilized hosts to sleep mode in order to save power, and to avoid over-utilized hosts from Service Level Agreement (SLA) violation. But still the problem is that the Cloud Service Provider apply a single policy on all nodes. Our proposed solution is an adaptive power panel where different policies can be applied based on both of the nature of the tasks running on hosts, and the Cloud Provider decision.","Azmy NM,El-Maddah IA,Mohamed HK",,2016,2016,https://doi.org/10.1145/2944165.2944167;http://dx.doi.org/10.1145/2944165.2944167,adaptive power panel of cloud computing controlling cloud power consumption,1
69,Towards Server-Level Power Monitoring in Data Centers Using Single-Point Voltage Measurement,"9,78145E+12",,10.1145/3560905.3568079,https://doi.org/10.1145/3560905.3568079;http://dx.doi.org/10.1145/3560905.3568079,Proceedings of the 20th ACM Conference on Embedded Networked Sensor Systems,"Server-level power monitoring in data centers can significantly contribute to its efficient management. Nevertheless, due to the cost of a dedicated power meter for each server, most data center power management only focuses on UPS or cluster-level power monitoring. In this paper, we propose a low-cost novel power monitoring approach that uses only one sensor to extract power consumption information of all servers. We utilize the conducted electromagnetic interference of server power supplies to measure its power consumption from non-intrusive single-point voltage measurement. Using a pair of commercial grade Dell PowerEdge servers, we demonstrate that our approach can estimate each server's power consumption with 3% mean absolute percentage error.","Gupta P,Talukder Z,Islam MA,Nguyen P",,2023,2023,https://doi.org/10.1145/3560905.3568079;http://dx.doi.org/10.1145/3560905.3568079,towards server level power monitoring in data centers using single point voltage measurement,1
70,Optimizing Energy Consumption for Cloud Computing: A Cluster and Migration Based Approach (CMBA),"9,78145E+12",,10.1145/3374587.3374594,https://doi.org/10.1145/3374587.3374594;http://dx.doi.org/10.1145/3374587.3374594,Proceedings of the 2019 3rd International Conference on Computer Science and Artificial Intelligence,"The increased use of IT technologies and number of IT users have triggered cloud computing resource demand including the need for more data centers. Each data center consumes electricity for its un-interrupted operations and maintenance, therefore responsible for the emissions of carbon dioxide, a potent greenhouse gas causing climate change. Hence, there is a necessity to provide a solution through which energy consumption for cloud data centers can be reduced. As virtual machine located in data center are run under loaded to maintain higher performance but it causes wastage of resources and power. While, task overloading severally reduce the performance of data center. To address this issue, we propose CMBA (Cluster and Migration Based Approach) for cloud resource allocation that maps groups of tasks to customized virtual machine types based on processing, memory and network requirements. Proper placement of workload with specific VMs and dynamic migration concept reduce energy consumption for running physical machine and its respective host or data centers. Taking altogether, intelligent customization of virtual machines by adopting CMBA approach will maintain high efficiency of datacenters with reduced energy consumption.","Singh J,Chen J",,2020,2020,https://doi.org/10.1145/3374587.3374594;http://dx.doi.org/10.1145/3374587.3374594,optimizing energy consumption for cloud computing a cluster and migration based approach cmba,1
71,Measuring Server Energy Proportionality,"9,78145E+12",,10.1145/2668930.2688049,https://doi.org/10.1145/2668930.2688049;http://dx.doi.org/10.1145/2668930.2688049,Proceedings of the 6th ACM/SPEC International Conference on Performance Engineering,"In performance engineering, metrics are often used to track the progress over time. Concerning the potential bias of using a single metric, performance engineers tend to use multiple metrics for reasoning. However, this approach has its own challenges. In this work we study one of the challenges in the context of analyzing trends in server energy proportionality. We examine a wide range of metrics for measuring energy proportionality, trying to determine which metrics are essential and which are redundant. We do this by comparing the trend curves of the metrics for the published results of the SPECpower_ssj2008 benchmark. While the context is specific, the proposed analysis method is quite general. We hope that this method would help us do performance engineering more effectively.","Hsu CH,Poole SW",,2015,2015,https://doi.org/10.1145/2668930.2688049;http://dx.doi.org/10.1145/2668930.2688049,measuring server energy proportionality,1
72,Impact of Virtualization on Cloud Computing Energy Consumption: Empirical Study,"9,78145E+12",,10.1145/3284557.3284738,https://doi.org/10.1145/3284557.3284738;http://dx.doi.org/10.1145/3284557.3284738,Proceedings of the 2nd International Symposium on Computer Science and Intelligent Control,"Global warming, which is currently one of the greatest environmental challenges, is caused by carbon emissions. A report from the Energy Information Administration indicates that approximately 98% of CO2 emissions can be attributed to energy consumption. The trade-off between efficient and ecologically sound operation represents a major challenge faced by many organizations at present. In addition, numerous companies are currently compelled to pay a carbon tax for the resources they use and the environmental impact of their products and services. Therefore, an energy consumption system can generate actual financial payback. Green information technology involves various approaches, including power management, recycling, telecommunications, and virtualization. This paper focuses on comparing and evaluating techniques used for reducing energy consumption in virtualized environments. We first highlight the impact of virtualization techniques on minimizing energy consumption in cloud computing. Then we present an experimental comparative study between two common energy-efficient task scheduling algorithms in cloud computing (i.e., the green scheduler, the power saver scheduler). These algorithms are discussed briefly and analyzed. The three metrics used to evaluate the task scheduling algorithms are (1) total power consumption, (2) data center load, and (3) virtual machine load. This work aims to gauge and subsequently improve energy consumption efficiency in virtualized environments.","Atiewi S,Abuhussein A,Saleh MA",,2018,2018,https://doi.org/10.1145/3284557.3284738;http://dx.doi.org/10.1145/3284557.3284738,impact of virtualization on cloud computing energy consumption empirical study,1
73,A Predictive System Shutdown Method for Energy Saving of Event-Driven Computation,,1084-4309,10.1145/335043.335046,https://doi.org/10.1145/335043.335046;http://dx.doi.org/10.1145/335043.335046,,"This paper presents a system-level power management technique for energy savings of event-driven application. We present a new predictive system-shutdown method to exploit sleep mode operations for energy saving. We use an exponential-average approach to predict the upcoming idle period. We introduce two mechanisms, prediction-miss correction and prewake-up, to improve the hit ratio and to reduce the delay overhead. Experiments on four different event-driven applications show that our proposed method achieves high hit ratios in a wide range of delay overheads, which results in a high degree of energy with low delay penaties.","Hwang CH,Wu AC",ACM Trans. Des. Autom. Electron. Syst.,2000,2000-04,https://doi.org/10.1145/335043.335046;http://dx.doi.org/10.1145/335043.335046,a predictive system shutdown method for energy saving of event driven computation,1
74,A Performance-Conserving Approach for Reducing Peak Power Consumption in Server Systems,"9,7816E+12",,10.1145/1088149.1088188,https://doi.org/10.1145/1088149.1088188;http://dx.doi.org/10.1145/1088149.1088188,Proceedings of the 19th Annual International Conference on Supercomputing,"The combination of increasing component power consumption, a desire for denser systems, and the required performance growth in the face of technology-scaling issues are posing enormous challenges for powering and cooling of server systems. The challenges are directly linked to the peak power consumption of servers.Our solution, Power Shifting, reduces the peak power consumption of servers minimizing the impact on performance. We reduce peak power consumption by using workload-guided dynamic allocation of power among components incorporating real-time performance feedback, activity-related power estimation techniques, and performance-sensitive activity-regulation mechanisms to enforce power budgets.We apply our techniques to a computer system with a single processor and memory. Power shifting adds a system power manager with a dynamic, global view of the system's power consumption to continuously re-budget the available power amongst the two components. Our contributions include:Â• Demonstration of the greater effectiveness of dynamic power allocation over static budgeting,Â• Evaluation of different power shifting policies,Â• Analysis of system and workload factors critical to successful power shifting, andÂ• Proposal of performance-sensitive power budget enforcement mechanisms that ensure system reliability.","Felter W,Rajamani K,Keller T,Rusu C",,2005,2005,https://doi.org/10.1145/1088149.1088188;http://dx.doi.org/10.1145/1088149.1088188,a performance conserving approach for reducing peak power consumption in server systems,1
75,A Predictive System Shutdown Method for Energy Saving of Event-Driven Computation,"9,78082E+12",,,,Proceedings of the 1997 IEEE/ACM International Conference on Computer-Aided Design,"We present a system-level power management technique for power saving of event-driven applications. We present a new predictive system shutdown method to exploit sleep mode operations for power saving. We use an exponential-average approach to predict the upcoming idle period. We introduce two mechanisms, prediction-miss correction and pre-wakeup, to improve the hit ratio and to reduce the delay overhead. Experiments on four different event-driven applications show that our proposed method achieves high hit ratios in a wide range of delay overheads, which results in a high degree of power saving with low delay penalties.","Hwang CH,Wu AC",,1997,1997,,a predictive system shutdown method for energy saving of event driven computation,1
76,Creating and Evaluating a Software Power Model for Linux Single Board Computers,"9,78145E+12",,10.1145/3194078.3194079,https://doi.org/10.1145/3194078.3194079;http://dx.doi.org/10.1145/3194078.3194079,Proceedings of the 6th International Workshop on Green and Sustainable Software,"The number of Single Board Computers (SBCs) is increasing, and so is the cumulative energy consumed by this category of device. Moreover, such devices are often always-on or running on batteries. Therefore, it is worth investigating their energy consumption to provide software developers and users with indicators for understanding how much energy the device is consuming while running a software application. In this paper, we explain a procedure for the creation of an energy consumption model of SBCs based on the usage of its components. We apply the procedure on a Raspberry PI 2 model B to test the model with a set of real applications. The results demonstrate the practical feasibility of the approach and show that estimated consumption values on our device have an average error of 2.2%, which is a good approximation without using external and expensive measuring devices.","Ardito L,Torchiano M",,2018,2018,https://doi.org/10.1145/3194078.3194079;http://dx.doi.org/10.1145/3194078.3194079,creating and evaluating a software power model for linux single board computers,1
77,Demystifying Energy Consumption Dynamics in Transiently Powered Computers,,1539-9087,10.1145/3391893,https://doi.org/10.1145/3391893;http://dx.doi.org/10.1145/3391893,,"Transiently powered computers (TPCs) form the foundation of the battery-less Internet of Things, using energy harvesting and small capacitors to power their operation. This kind of power supply is characterized by extreme variations in supply voltage, as capacitors charge when harvesting energy and discharge when computing. We experimentally find that these variations cause marked fluctuations in clock speed and power consumption. Such a deceptively minor observation is overlooked in existing literature. Systems are thus designed and parameterized in overly conservative ways, missing on a number of optimizations.We rather demonstrate that it is possible to accurately model and concretely capitalize on these fluctuations. We derive an energy model as a function of supply voltage and prove its use in two settings. First, we develop EPIC, a compile-time energy analysis tool. We use it to substitute for the constant power assumption in existing analysis techniques, giving programmers accurate information on worst-case energy consumption of programs. When using EPIC with existing TPC system support, run-time energy efficiency drastically improves, eventually leading up to a 350% speedup in the time to complete a fixed workload. Further, when using EPIC with existing debugging tools, it avoids unnecessary program changes that hurt energy efficiency. Next, we extend the MSPsim emulator and explore its use in parameterizing a different TPC system support. The improvements in energy efficiency yield up to more than 1000% time speedup to complete a fixed workload.","Ahmed S,Nawaz M,Bakar A,Bhatti NA,Alizai MH,Siddiqui JH,Mottola L",ACM Trans. Embed. Comput. Syst.,2020,2020-09,https://doi.org/10.1145/3391893;http://dx.doi.org/10.1145/3391893,demystifying energy consumption dynamics in transiently powered computers,1
78,"Estimating the Energy Consumption of Software Components from Size, Complexity and Code Smells Metrics","9,78145E+12",,10.1145/3477314.3507353,https://doi.org/10.1145/3477314.3507353;http://dx.doi.org/10.1145/3477314.3507353,Proceedings of the 37th ACM/SIGAPP Symposium on Applied Computing,"Software quality may influence energy consumption. This work presents the CCsEM green model based on Size, Complexity and Code Smells to estimate energy consumption of architecture software components without being executed. This paper details the definition of this multiple linear regression model constructed from 42 applications, and its results.","GuamÃ¡n D,PÃ©rez J,Valdiviezo-Diaz P,Canas N",,2022,2022,https://doi.org/10.1145/3477314.3507353;http://dx.doi.org/10.1145/3477314.3507353,estimating the energy consumption of software components from size complexity and code smells metrics,1
79,An Energy Consumption Model and Analysis Tool for Cloud Computing Environments,"9,78147E+12",,,,Proceedings of the First International Workshop on Green and Sustainable Software,"Cloud computing delivers computing as a utility to users worldwide. A consequence of this model is that cloud data centres have high deployment and operational costs, as well as significant carbon footprints for the environment. We need to develop Green Cloud Computing (GCC) solutions that reduce these deployment and operational costs and thus save energy and reduce adverse environmental impacts. In order to achieve this objective, a thorough understanding of the energy consumption patterns in complex Cloud environments is needed. We present a new energy consumption model and associated analysis tool for Cloud computing environments. We measure energy consumption in Cloud environments based on different runtime tasks. Empirical analysis of the correlation of energy consumption and Cloud data and computational tasks, as well as system performance, will be investigated based on our energy consumption model and analysis tool. Our research results can be integrated into Cloud systems to monitor energy consumption and support static or dynamic system-level optimisation.","Chen F,Schneider JG,Yang Y,Grundy J,He Q",,2012,2012,,an energy consumption model and analysis tool for cloud computing environments,1
80,Trace-Driven Simulation for Energy Consumption in High Throughput Computing Systems,"9,78148E+12",,10.1109/DS-RT.2014.12,https://doi.org/10.1109/DS-RT.2014.12;http://dx.doi.org/10.1109/DS-RT.2014.12,Proceedings of the 2014 IEEE/ACM 18th International Symposium on Distributed Simulation and Real Time Applications,"High Throughput Computing (HTC) is a powerful paradigm allowing vast quantities of independent work to be performed simultaneously. However, until recently little evaluation has been performed on the energy impact of HTC. Many organisations now seek to minimise energy consumption across their IT infrastructure though it is unclear how this will affect the usability of HTC systems. We present here HTC-Sim, a simulation system which allows the evaluation of different energy reduction policies across an HTC system comprising a collection of computational resources dedicated to HTC work and resources provided through cycle scavenging--a Desktop Grid. We demonstrate that our simulation software scales linearly with increasing HTC workload.","Forshaw M,Thomas N,McGough AS",,2014,2014,https://doi.org/10.1109/DS-RT.2014.12;http://dx.doi.org/10.1109/DS-RT.2014.12,trace driven simulation for energy consumption in high throughput computing systems,1
81,Experimental Analysis of Task-Based Energy Consumption in Cloud Computing Systems,"9,78145E+12",,10.1145/2479871.2479911,https://doi.org/10.1145/2479871.2479911;http://dx.doi.org/10.1145/2479871.2479911,Proceedings of the 4th ACM/SPEC International Conference on Performance Engineering,"Cloud computing delivers IT solutions as a utility to users. One consequence of this model is that large cloud data centres consume large amounts of energy and produce significant carbon footprints. A common objective of cloud providers is to develop resource provisioning and management solutions that minimise energy consumption while guaranteeing Service Level Agreements (SLAs). In order to achieve this objective, a thorough understanding of energy consumption patterns in complex cloud systems is imperative. We have developed an energy consumption model for cloud computing systems. To operationalise this model, we have conducted extensive experiments to profile the energy consumption in cloud computing systems based on three types of tasks: computation-intensive, data-intensive and communication-intensive tasks. We collected fine-grained energy consumption and performance data with varying system configurations and workloads. Our experimental results show the correlation coefficients of energy consumption, system configuration and workload, as well as system performance in cloud systems. These results can be used for designing energy consumption monitors, and static or dynamic system-level energy consumption optimisation strategies for green cloud computing systems.","Chen F,Grundy J,Yang Y,Schneider JG,He Q",,2013,2013,https://doi.org/10.1145/2479871.2479911;http://dx.doi.org/10.1145/2479871.2479911,experimental analysis of task based energy consumption in cloud computing systems,1
82,An Optimized VM Placement Approach to Reduce Energy Consumption in Green Cloud Computing,"9,78145E+12",,10.1145/3484824.3484894,https://doi.org/10.1145/3484824.3484894;http://dx.doi.org/10.1145/3484824.3484894,"Proceedings of the International Conference on Data Science, Machine Learning and Artificial Intelligence","As a global digitization advancement, there is a massive need of cloud-based solutions and data centers. Another reason behind excessive need of data centers is because of increasing number of internet users. Increasing demand of data centers simultaneously need huge amount of energy for data center operation and on other end emit enormous amount of CO2. Several approaches have been proposed to reduce energy consumption, but major concern is by looking at one parameter or criteria they must compromise on other. Our proposed approach MIPS-Aware VM Placement in combination with searching of best capable host helps to reduce VM migration and increase mean time for better performance and save energy. Proposed approach identifies overloaded and underloaded hosts and to improve system performance algorithm does not allow to allocate additional workload, which will also help to reduce energy and get better QoS. Proposed approach significantly decreases VM migration and increase mean time before VM migration which in turns helps to reduce energy and associated cost. By using proposed MIPS-Aware VM Placement approach, we can reduce upto 25% more energy consumption compared to traditional approaches.","Bheda H,Thaker C,Shah S",,2022,2022,https://doi.org/10.1145/3484824.3484894;http://dx.doi.org/10.1145/3484824.3484894,an optimized vm placement approach to reduce energy consumption in green cloud computing,1
85,Towards Power Consumption Modeling for Servers at Scale,"9,78077E+12",,,,Proceedings of the 8th International Conference on Utility and Cloud Computing,"As of 2010 data centers use 1.5% of global electricity production and this is expected to keep growing [1]. There is a need for a near real-time power consumption modeling/monitoring system that could be used at scale within a Software Defined Data Center (SDDC). The power consumption models and information they provide can then be used to make better decisions for data center orchestration, e.g., whether to migrate virtual machines to reduce power consumption. We propose a scalable system that would 1) create initial power consumption models, as needed, for data center components, and 2) could be continually refined while the components are in use. The models will be used for the near real-time monitoring of power consumption, as well as predicting power consumption before and after potential orchestration decisions. The first step towards this goal of whole data center power modeling and prediction is to be able to predict the power consumption of one server effectively, based on high level utilization statistics from that server. In this paper we present a novel method for modeling whole system power consumption for a server, under varying random levels of CPU utilization, with a scalable random forest based model, that utilizes statistics available at the data center management level.","Harton TW,Walker C,O'Sullivan M",,2015,2015,,towards power consumption modeling for servers at scale,1
86,On Understanding the Energy Consumption of ARM-Based Multicore Servers,"9,78145E+12",,10.1145/2465529.2465553,https://doi.org/10.1145/2465529.2465553;http://dx.doi.org/10.1145/2465529.2465553,Proceedings of the ACM SIGMETRICS/International Conference on Measurement and Modeling of Computer Systems,"There is growing interest to replace traditional servers with low-power multicore systems such as ARM Cortex-A9. However, such systems are typically provisioned for mobile applications that have lower memory and I/O requirements than server application. Thus, the impact and extent of the imbalance between application and system resources in exploiting energy efficient execution of server workloads is unclear. This paper proposes a trace-driven analytical model for understanding the energy performance of server workloads on ARM Cortex-A9 multicore systems. Key to our approach is the modeling of the degrees of CPU core, memory and I/O resource overlap, and in estimating the number of cores and clock frequency that optimizes energy performance without compromising execution time. Since energy usage is the product of utilized power and execution time, the model first estimates the execution time of a program. CPU time, which accounts for both cores and memory response time, is modeled as an M/G/1 queuing system. Workload characterization of high performance computing, web hosting and financial computing applications shows that bursty memory traffic fits a Pareto distribution, and non-bursty memory traffic is exponentially distributed. Our analysis using these server workloads reveals that not all server workloads might benefit from higher number of cores or clock frequencies. Applying our model, we predict the configurations that increase energy efficiency by 10% without turning off cores, and up to one third with shutting down unutilized cores. For memory-bounded programs, we show that the limited memory bandwidth might increase both execution time and energy usage, to the point where energy cost might be higher than on a typical x64 multicore system. Lastly, we show that increasing memory and I/O bandwidth can improve both the execution time and the energy usage of server workloads on ARM Cortex-A9 systems.","Tudor BM,Teo YM",,2013,2013,https://doi.org/10.1145/2465529.2465553;http://dx.doi.org/10.1145/2465529.2465553,on understanding the energy consumption of arm based multicore servers,1
87,On Understanding the Energy Consumption of ARM-Based Multicore Servers,,0163-5999,10.1145/2494232.2465553,https://doi.org/10.1145/2494232.2465553;http://dx.doi.org/10.1145/2494232.2465553,,"There is growing interest to replace traditional servers with low-power multicore systems such as ARM Cortex-A9. However, such systems are typically provisioned for mobile applications that have lower memory and I/O requirements than server application. Thus, the impact and extent of the imbalance between application and system resources in exploiting energy efficient execution of server workloads is unclear. This paper proposes a trace-driven analytical model for understanding the energy performance of server workloads on ARM Cortex-A9 multicore systems. Key to our approach is the modeling of the degrees of CPU core, memory and I/O resource overlap, and in estimating the number of cores and clock frequency that optimizes energy performance without compromising execution time. Since energy usage is the product of utilized power and execution time, the model first estimates the execution time of a program. CPU time, which accounts for both cores and memory response time, is modeled as an M/G/1 queuing system. Workload characterization of high performance computing, web hosting and financial computing applications shows that bursty memory traffic fits a Pareto distribution, and non-bursty memory traffic is exponentially distributed. Our analysis using these server workloads reveals that not all server workloads might benefit from higher number of cores or clock frequencies. Applying our model, we predict the configurations that increase energy efficiency by 10% without turning off cores, and up to one third with shutting down unutilized cores. For memory-bounded programs, we show that the limited memory bandwidth might increase both execution time and energy usage, to the point where energy cost might be higher than on a typical x64 multicore system. Lastly, we show that increasing memory and I/O bandwidth can improve both the execution time and the energy usage of server workloads on ARM Cortex-A9 systems.","Tudor BM,Teo YM",SIGMETRICS Perform. Eval. Rev.,2013,2013-06,https://doi.org/10.1145/2494232.2465553;http://dx.doi.org/10.1145/2494232.2465553,on understanding the energy consumption of arm based multicore servers,1
88,GRANNITE: Graph Neural Network Inference for Transferable Power Estimation,"9,78145E+12",,,,Proceedings of the 57th ACM/EDAC/IEEE Design Automation Conference,"This paper introduces GRANNITE, a GPU-accelerated novel graph neural network (GNN) model for fast, accurate, and transferable vector-based average power estimation. During training, GRANNITE learns how to propagate average toggle rates through combinational logic: a netlist is represented as a graph, register states and unit inputs from RTL simulation are used as features, and combinational gate toggle rates are used as labels. A trained GNN model can then infer average toggle rates on a new workload of interest or new netlists from RTL simulation results in a few seconds. Compared to traditional power analysis using gate-level simulations, GRANNITE achieves >18.7X speedup with an error of only <5.5% across a diverse set of benchmark circuits. Compared to a GPU-accelerated conventional probabilistic switching activity estimation approach, GRANNITE achieves much better accuracy (on average 25.9% lower error) at similar runtimes.","Zhang Y,Ren H,Khailany B",,2020,2020,,grannite graph neural network inference for transferable power estimation,1
89,How to Measure Energy-Efficiency of Software: Metrics and Measurement Results,"9,78147E+12",,,,Proceedings of the First International Workshop on Green and Sustainable Software,"In the field of information and computer technology (ICT), saving energy has its focus set on energy efficient hardware and its operation. Recently, efforts have also been made in the area of computer software. However, the development of energy efficient software requires metrics, which measure the software's energy consumption as well as models to monitor and minimize it. In software and software development processes they hardly exist. In this work we present a generic metric to measure software and a method to apply it in a software engineering process.","Johann T,Dick M,Naumann S,Kern E",,2012,2012,,how to measure energy efficiency of software metrics and measurement results,1
90,Reducing Power Consumption Using Approximate Encoding for CNN Accelerators at the Edge,"9,78145E+12",,10.1145/3526241.3530315,https://doi.org/10.1145/3526241.3530315;http://dx.doi.org/10.1145/3526241.3530315,Proceedings of the Great Lakes Symposium on VLSI 2022,"Convolutional neural networks (CNNs) have demonstrated significant potential across a range of applications due to their superior accuracy. Edge inference, in which inference is performed locally in embedded systems with limited power resources, is researched for its energy efficiency. An approximate encoder is proposed in this study for decreasing switching activity, which minimizes power consumption in CNN accelerators at the edge. The proposed encoder performs approximate encoding based on a pattern matching of a comparison pattern and current data. Software determines the value of the comparison pattern and the availability of the recommended encoder. Experiments with a CIFAR-10 dataset utilizing LeNet5 show that using the suggested encoder, depending upon the comparison pattern, power consumption of a CNN accelerator can be reduced by 21.5% with 1.59% degradation on inference quality.","Yang T,Ukezono T,Sato T",,2022,2022,https://doi.org/10.1145/3526241.3530315;http://dx.doi.org/10.1145/3526241.3530315,reducing power consumption using approximate encoding for cnn accelerators at the edge,1
91,Estimating Energy Parameters for RNA Secondary Structure Predictions Using Both Experimental and Computational Data,,1545-5963,10.1109/TCBB.2018.2813388,https://doi.org/10.1109/TCBB.2018.2813388;http://dx.doi.org/10.1109/TCBB.2018.2813388,,"Computational RNA secondary structure prediction depends on a large number of nearest-neighbor free-energy parameters, including 10 parameters for Watson-Crick stacked base pairs that were estimated from experimental measurements of the free energies of 90 RNA duplexes. These experimental data are provided by time-consuming and cost-intensive experiments. In contrast, various modified nucleotides in RNAs, which would affect not only their structures but also functions, have been found, and rapid determination of energy parameters for a such modified nucleotides is needed. To reduce the high cost of determining energy parameters, we propose a novel method to estimate energy parameters from both experimental and computational data, where the computational data are provided by a recently developed molecular dynamics simulation protocol. We evaluate our method for Watson-Crick stacked base pairs, and show that parameters estimated from 10 experimental data items and 10 computational data items can predict RNA secondary structures with accuracy comparable to that using conventional parameters. The results indicate that the combination of experimental free-energy measurements and molecular dynamics simulations is capable of estimating the thermodynamic properties of RNA secondary structures at lower cost.","Nishida S,Sakuraba S,Asai K,Hamada M",IEEE/ACM Trans. Comput. Biol. Bioinformatics,2019,2019-09,https://doi.org/10.1109/TCBB.2018.2813388;http://dx.doi.org/10.1109/TCBB.2018.2813388,estimating energy parameters for rna secondary structure predictions using both experimental and computational data,1
93,MLStar: Machine Learning in Energy Profile Estimation of Android Apps,"9,78145E+12",,10.1145/3286978.3287011,https://doi.org/10.1145/3286978.3287011;http://dx.doi.org/10.1145/3286978.3287011,"Proceedings of the 15th EAI International Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services","Improving the energy efficiency of smartphones is critical for increasing the utility that they provide to the users. With most mobile operating systems, users are responsible for managing their phone's battery efficiency by utilizing the various settings provided by the operating system, as well as selecting energy-efficient apps. However, current app marketplaces do not provide users with information about app energy efficiency, which makes it challenging for the user to make informed decision when selecting an app. This paper presents a novel machine learning approach to estimate app energy efficiency by utilizing textual information available in the Google Play store such as an app's description, user reviews, as well as system permissions. Our detailed analysis of the resulting system shows that hardware permissions, app description, and user reviews correlate well with energy efficiency ratings. We evaluate five models that represent popular classes of machine learning algorithms in their ability to predict energy efficiency ratings. Finally, we compare our approach to gold truth ratings obtained by the actual energy profiling of the app, demonstrating that the proposed system is able to estimate an app's energy efficiency within less than 1 point on the 1-5 scale provided by the profiler, without requiring any kind of profiling.","Gaska B,Gniady C,Surdeanu M",,2018,2018,https://doi.org/10.1145/3286978.3287011;http://dx.doi.org/10.1145/3286978.3287011,mlstar machine learning in energy profile estimation of android apps,1
94,"Computing Server Power Modeling in a Data Center: Survey, Taxonomy, and Performance Evaluation",,0360-0300,10.1145/3390605,https://doi.org/10.1145/3390605;http://dx.doi.org/10.1145/3390605,,"Data centers are large-scale, energy-hungry infrastructure serving the increasing computational demands as the world is becoming more connected in smart cities. The emergence of advanced technologies such as cloud-based services, internet of things (IoT), and big data analytics has augmented the growth of global data centers, leading to high energy consumption. This upsurge in energy consumption of the data centers not only incurs the issue of surging high cost (operational and maintenance) but also has an adverse effect on the environment. Dynamic power management in a data center environment requires the cognizance of the correlation between the system and hardware-level performance counters and the power consumption. Power consumption modeling exhibits this correlation and is crucial in designing energy-efficient optimization strategies based on resource utilization. Several works in power modeling are proposed and used in the literature. However, these power models have been evaluated using different benchmarking applications, power-measurement techniques, and error-calculation formulas on different machines. In this work, we present a taxonomy and evaluation of 24 software-based power models using a unified environment, benchmarking applications, power-measurement techniques, and error formulas, with the aim of achieving an objective comparison. We use different server architectures to assess the impact of heterogeneity on the modelsÂ’ comparison. The performance analysis of these models is elaborated in the article.","Ismail L,Materwala H",ACM Comput. Surv.,2020,2020-06,https://doi.org/10.1145/3390605;http://dx.doi.org/10.1145/3390605,computing server power modeling in a data center survey taxonomy and performance evaluation,1
95,A New Method of Cloud-Based Computation Model for Mobile Devices: Energy Consumption Optimization in Mobile-to-Mobile Computation Offloading,"9,78145E+12",,10.1145/3193092.3193103,https://doi.org/10.1145/3193092.3193103;http://dx.doi.org/10.1145/3193092.3193103,Proceedings of the 6th International Conference on Communications and Broadband Networking,"Today, cell phones have great important role in everyday lives. They are the most effective and achievable communication and computation devices that necessitate no exact time or location. Fast development of movable calculation created an energetic power to develop a synchronous technology in the markets. Anyhow movable devices are encountering with various challenges concerning available resources such as: battery, storage, bandwidth, security, and mobility. On the other side, limitation of resources in these phones has a great effect on the quality of the services. The most important limitation is the energy consuming that leads us to use the device for calculation, in spite of the accessibility of other sources. The best suggested solutions are getting rid of computation on the cloud and use of portable cloud for computation. But If there was no accessible cloud or an accurace of disconnection with the cloud, cell phones are the best to use. According to a mathematical algorithm based on Lyapunov optimization and regarding the requirement time for suitable program, we will try to introduce a dynamic way which its limitation analysis reveals that offlooding the computation considering the limitation of time leads to a less spending of power and energy resembled the current algorithm.","Jamali H,Karimi A,Haghighizadeh M",,2018,2018,https://doi.org/10.1145/3193092.3193103;http://dx.doi.org/10.1145/3193092.3193103,a new method of cloud based computation model for mobile devices energy consumption optimization in mobile to mobile computation offloading,1
96,A Computing Profiling Procedure for Mobile Developers to Estimate Energy Cost,"9,78145E+12",,10.1145/2811587.2811627,https://doi.org/10.1145/2811587.2811627;http://dx.doi.org/10.1145/2811587.2811627,"Proceedings of the 18th ACM International Conference on Modeling, Analysis and Simulation of Wireless and Mobile Systems","Mobile devices are constrained by the limited capacities of their small batteries. However, profiling the energy consumed in the task execution is crucial to help the developers to build energy efficient applications. Therefore, the major challenge in the profiling approach is to accurately estimating the energy consumed for an application by the hardware components, such as CPU, memory, storage unit, and network interfaces. In this work, we develop and validate hardware and software profiling models and procedures. We profile smartphone CPU, where we consider multi-core CPUs and the impact of Dynamic Voltage and Frequency Scaling mechanism on the power consumption. In addition, we profile smartphone storage unit by taking into account the writing and reading rate to the unit. Moreover, we experimentally validated these profiles on two diverse smartphones with different versions of operating systems. The experimental results reveal that our profiles are able to estimate the application energy accurately.","Altamimi ML,Naik K",,2015,2015,https://doi.org/10.1145/2811587.2811627;http://dx.doi.org/10.1145/2811587.2811627,a computing profiling procedure for mobile developers to estimate energy cost,1
97,An Empirical Study on the Performance and Energy Consumption of AI Containerization Strategies for Computer-Vision Tasks on the Edge,"9,78145E+12",,10.1145/3530019.3530025,https://doi.org/10.1145/3530019.3530025;http://dx.doi.org/10.1145/3530019.3530025,Proceedings of the 26th International Conference on Evaluation and Assessment in Software Engineering,"Context. The rise of use cases of AI catered towards the Edge, where devices have limited computation power and storage capabilities, motivates the need for better understating of how AI performs and consumes energy. Goal. The aim of this paper is to empirically assess the impact of three different AI containerization strategies on the energy consumption, execution time, CPU, and memory usage for computer-vision tasks on the Edge. Method. In this paper we conduct an experiment with the used containerization strategy as main factor, with three treatments: ONNX Runtime, WebAssembly, and Docker. The subjects of the experiment are four widely-used computer-vision algorithms. We then orchestrate a series of runs where we deploy the four subjects on different generations of Raspberry Pi devices, with different hardware capabilities. A total of 120 runs (per device) are recorded to gather data on energy, execution time, CPU, and memory. Results. We found a statistically significant difference between the three containerization strategies on all dependent variables. Specifically, WebAssembly proves to be a valuable alternative for devices with reduced disk space and computation power. Conclusions. For computer-vision tasks with limited disk space and RAM memory requirements, developers should prefer WebAssembly for deployment. The (non-dockerized) ONNX Runtime resulted to be the best choice in terms of energy consumption and execution time.","Hampau RM,Kaptein M,van Emden R,Rost T,Malavolta I",,2022,2022,https://doi.org/10.1145/3530019.3530025;http://dx.doi.org/10.1145/3530019.3530025,an empirical study on the performance and energy consumption of ai containerization strategies for computer vision tasks on the edge,1
98,Computing the Entire Active Area/Power Consumption versus Delay Trade-off Curve for Gate Sizing with a Piecewise Linear Simulator,"9,7809E+12",,,,Proceedings of the 1994 IEEE/ACM International Conference on Computer-Aided Design,"The gate sizing problem is the problem of finding load drive capabilities for all gates in a given Boolean network such, that a given delay limit is kept, and the necessary cost in terms of active area usage and/or power consumption is minimal. This paper describes a way to obtain the entire cost versus delay trade-off curve of a combinational logic circuit in an efficient way. Every point on the resulting curve is the global optimum of the corresponding gate sizing problem. The problem is solved by mapping it onto piecewise linear models in such a way, that a piecewise linear (circuit) simulator can do the job. It is shown that this setup is very efficient, and can produce trade-off curves for large circuits (thousands of gates) in a few minutes. Benchmark results for the entire set of MCNC '91 two-level examples are given.","Berkelaar MR,Buurman PH,Jess JA",,1994,1994,,computing the entire active area power consumption versus delay trade off curve for gate sizing with a piecewise linear simulator,1
99,A Sequential DNN Based Baseline Energy Prediction Framework with Long Term Error Mitigation,"9,78145E+12",,10.1145/3307772.3331027,https://doi.org/10.1145/3307772.3331027;http://dx.doi.org/10.1145/3307772.3331027,Proceedings of the Tenth ACM International Conference on Future Energy Systems,"In this paper, we present a novel sequential framework containing two deep network architectures for baseline energy prediction of a building. The proposed framework utilizes convolution layers to extract features from the input data space without changing spatial relations between variables. These features are memorized by tensor train based gated recurrent units for an accurate long-term prediction. We observe that this sequential framework helps to improve long term prediction accuracy, thereby mitigating prediction error accumulation over time. Although architectures comprising the amalgamation of convolution layers and memory cell have shown promising results in domains such as social media analysis, language modeling, video frame prediction, and image recognition, this paper extends its scope to the context of energy applications. Furthermore, the addition of tensor train based gated recurrent units is motivated by the necessity of computational time reduction during the training process, thereby making this framework suitable for field deployment. Results on a commercial building dataset show that the current framework outperforms other existing machine learning based methods, in both short-term and long-term prediction categories.","Chakraborty I,Chandan V,Vrabie D",,2019,2019,https://doi.org/10.1145/3307772.3331027;http://dx.doi.org/10.1145/3307772.3331027,a sequential dnn based baseline energy prediction framework with long term error mitigation,1
100,Reducing Disk Power Consumption in a Portable Computer,,0163-5980,10.1145/202213.202218,https://doi.org/10.1145/202213.202218;http://dx.doi.org/10.1145/202213.202218,,The problem of minimizing disk power consumption in portable personal computers is studied. Two online algorithms for determining when to stop spinning a disk are presented and analyzed using competitive analysis techniques.,"Klostermeyer WF,Srinivas K",SIGOPS Oper. Syst. Rev.,1995,1995-04,https://doi.org/10.1145/202213.202218;http://dx.doi.org/10.1145/202213.202218,reducing disk power consumption in a portable computer,1
102,Evaluating the Efficiency of Energy-Scape Software,"9,78145E+12",,10.1145/3328833.3328868,https://doi.org/10.1145/3328833.3328868;http://dx.doi.org/10.1145/3328833.3328868,Proceedings of the 8th International Conference on Software and Information Engineering,"Recently, researchers were focusing on integrating renewable energy (RE) within urban environment instead of integrating renewables with buildings due to the large occupies of urban areas. Urban areas have a great potential in generating sufficient amount of energy that could satisfy the needs of urban neighborhoods. Energy-scape elements are sustainable elements that integrates RE devices with landscape elements. This research focuses on analyzing the importance and efficiency of Energy-scape software through a qualitative method. The efficiency of Energy-scape web-based application will be tested using qualitative method and a site survey. The research concludes that Energy-scape software application is an effective tool for landscape designers in using Energy-scape elements, it identifies the optimum type and location of Energy-scape elements within their projects, and it calculates the impact of using Energy-scape elements in term of energy-savings and carbon emission (CO2) reduction.","Moussa RR,Dewidar KM",,2019,2019,https://doi.org/10.1145/3328833.3328868;http://dx.doi.org/10.1145/3328833.3328868,evaluating the efficiency of energy scape software,1
103,Analysing Energy Consumption of Systems Controlled by Software,"9,78145E+12",,10.1145/3079368.3079396,https://doi.org/10.1145/3079368.3079396;http://dx.doi.org/10.1145/3079368.3079396,"Companion Proceedings of the 1st International Conference on the Art, Science, and Engineering of Programming","Energy consumption analysis of IT-controlled systems can play a major role in minimising the overall energy consumption of such IT systems, during the development phase, or for optimisation in the field. As software is increasingly embedded in our daily life, with IT using more and more energy, the software industry should become aware of their energy footprint, and methods must be developed to assist in reducing this footprint.Recently, we developed a precise energy analysis, to analyse software in conjunction with hardware. It has the property of being parametric with regard to the hardware. In principle, this creates the opportunity to investigate which is the best software implementation for given hardware, or the other way around: choose the best hardware for a given algorithm.",van Gastel B,,2017,2017,https://doi.org/10.1145/3079368.3079396;http://dx.doi.org/10.1145/3079368.3079396,analysing energy consumption of systems controlled by software,1
105,Survey of Approaches for Assessing Software Energy Consumption,"9,78145E+12",,10.1145/3141842.3141846,https://doi.org/10.1145/3141842.3141846;http://dx.doi.org/10.1145/3141842.3141846,Proceedings of the 2nd ACM SIGPLAN International Workshop on Comprehension of Complex Systems,"Though the energy consumption of software-controlled ICT systems ranging from mobile devices to data centers is increasingly gaining attention, energy optimization is still far from an established task in the software development process. Therefore, we have surveyed the available research on assessing the energy consumption of software systems, which showed a lack of development tools, but several approaches exist for measuring the energy consumption. We group these approaches according to how measurement data is made available and compare several characteristics of the collected data. The survey shows that not only development tools for software energy optimization are still missing, but there is also a lack of fine-grained measurement approaches as well as approaches for general-purpose platforms.","Rieger F,Bockisch C",,2017,2017,https://doi.org/10.1145/3141842.3141846;http://dx.doi.org/10.1145/3141842.3141846,survey of approaches for assessing software energy consumption,1
106,Unit Testing of Energy Consumption of Software Libraries,"9,78145E+12",,10.1145/2554850.2554932,https://doi.org/10.1145/2554850.2554932;http://dx.doi.org/10.1145/2554850.2554932,Proceedings of the 29th Annual ACM Symposium on Applied Computing,"The development of energy-efficient software has become a key requirement for a large number of devices, from smartphones to data centers. However, measuring accurately this consumption is a major challenge that state-of-the-art approaches have tried to tackle with a limited success. While monitoring applications' consumption offers a clear insight on where the energy is being spent, it does not help in understanding how the energy is consumed. In this paper, we therefore introduce JalenUnit, a software framework that infers the energy consumption model of software libraries from execution traces. This model can then be used to diagnose application code for detecting energy bugs, understanding energy distribution, establishing energy profiles and classifications, and comparing software libraries against their energy consumption.","Noureddine A,Rouvoy R,Seinturier L",,2014,2014,https://doi.org/10.1145/2554850.2554932;http://dx.doi.org/10.1145/2554850.2554932,unit testing of energy consumption of software libraries,1
107,Awakening Awareness on Energy Consumption in Software Engineering,"9,78154E+12",,10.1109/ICSE-SEIS.2017.10,https://doi.org/10.1109/ICSE-SEIS.2017.10;http://dx.doi.org/10.1109/ICSE-SEIS.2017.10,Proceedings of the 39th International Conference on Software Engineering: Software Engineering in Society Track,"Software producing organizations have the ability to address the energy impact of their ICT solutions during the development process. However, while industry is convinced of the energy impact of hardware, the role of software has mostly been acknowledged by researchers in software engineering. Strengthened by the limited practical knowledge to reduce the energy consumption, organizations have less control over the energy impact of their products and lose the contribution of software towards energy related strategies. Consequently, industry risks not being able to meet customer requirements or even fulfill corporate sustainability goals.In this paper we perform an exploratory case study on how to create and maintain awareness on an energy consumption perspective for software among stakeholders involved with the development of software products. During the study, we followed the development process of two commercial software products and provided direct feedback to the stakeholders on the effects of their development efforts, specifically concerning energy consumption and performance, using an energy dashboard. Multiple awareness measurements allowed us to keep track of changes over time on specific aspects affecting software development. Our results show that, despite a mixed sentiment towards the dashboard, changed awareness has triggered discussion on the energy consumption of software.","Jagroep E,Broekman J,van der Werf JM,Brinkkemper S,Lago P,Blom L,van Vliet R",,2017,2017,https://doi.org/10.1109/ICSE-SEIS.2017.10;http://dx.doi.org/10.1109/ICSE-SEIS.2017.10,awakening awareness on energy consumption in software engineering,1
108,On Reducing the Energy Consumption of Software: From Hurdles to Requirements,"9,78145E+12",,10.1145/3382494.3410678,https://doi.org/10.1145/3382494.3410678;http://dx.doi.org/10.1145/3382494.3410678,Proceedings of the 14th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM),"Background. As software took control over hardware in many domains, the question of the energy footprint induced by the software is becoming critical for our society, as the resources powering the underlying infrastructure are finite. Yet, beyond this growing interest, energy consumption remains a difficult concept to master for a developer.Aims. The purpose of this study is to better understand the root causes that prevent the issue of software energy consumption to be more widely considered by developers and companies.Method. To investigate this issue, this paper reports on a qualitative study we conducted in an industrial context. We applied an in-depth analysis of the interviews of 10 experienced developers and summarized a set of implications.Results. We argue that our study delivers i) insightful feedback on how green software design is considered among the interviewed developers and ii) a set of findings to build helpful tools, motivate further research, and establish better development strategies to promote green software design.Conclusion. This paper covers an industrial case study of developers' awareness of green software design and how to promote it within the company. While it might not be generalizable for any company, we believe our results deliver a common body of knowledge with implications to be considered for similar cases and further researches.","Ournani Z,Rouvoy R,Rust P,Penhoat J",,2020,2020,https://doi.org/10.1145/3382494.3410678;http://dx.doi.org/10.1145/3382494.3410678,on reducing the energy consumption of software from hurdles to requirements,1
110,On the Energy Consumption and Performance of Systems Software,"9,78145E+12",,10.1145/1987816.1987827,https://doi.org/10.1145/1987816.1987827;http://dx.doi.org/10.1145/1987816.1987827,Proceedings of the 4th Annual International Conference on Systems and Storage,"Models of energy consumption and performance are necessary to understand and identify system behavior, prior to designing advanced controls that can balance out performance and energy use. This paper considers the energy consumption and performance of servers running a relatively simple file-compression workload. We found that standard techniques for system identification do not produce acceptable models of energy consumption and performance, due to the intricate interplay between the discrete nature of software and the continuous nature of energy and performance. This motivated us to perform a detailed empirical study of the energy consumption and performance of this system with varying compression algorithms and compression levels, file types, persistent storage media, CPU DVFS levels, and disk I/O schedulers. Our results identify and illustrate factors that complicate the system's energy consumption and performance, including nonlinearity, instability, and multi-dimensionality. Our results provide a basis for future work on modeling energy consumption and performance to support principled design of controllable energy-aware systems.","Li Z,Grosu R,Sehgal P,Smolka SA,Stoller SD,Zadok E",,2011,2011,https://doi.org/10.1145/1987816.1987827;http://dx.doi.org/10.1145/1987816.1987827,on the energy consumption and performance of systems software,1
111,On Reducing the Energy Consumption of Software Product Lines,"9,78145E+12",,10.1145/3461001.3471142,https://doi.org/10.1145/3461001.3471142;http://dx.doi.org/10.1145/3461001.3471142,Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A,"Along the last decade, several studies considered green software design as a key development concern to improve the energy efficiency of software. Yet, few techniques address this concern for Software Product Lines (SPL). In this paper, we therefore introduce two approaches to measure and reduce the energy consumption of a SPL by analyzing a limited set of products sampled from this SPL. While the first approach relies on the analysis of individual feature consumptions, the second one takes feature interactions into account to better mitigate energy consumption of resulting products.Our experimental results on a real-world SPL indicate that both approaches succeed to produce significant energy improvements on a large number of products, while consumption data was modeled from a small set of sampled products. Furthermore, we show that taking feature interactions into account leads to more products improved with higher energy savings per product.","GuÃ©gain Ã‰,Quinton C,Rouvoy R",,2021,2021,https://doi.org/10.1145/3461001.3471142;http://dx.doi.org/10.1145/3461001.3471142,on reducing the energy consumption of software product lines,1
117,Software-Directed Data Access Scheduling for Reducing Disk Energy Consumption,"9,78145E+12",,10.1145/1996130.1996175,https://doi.org/10.1145/1996130.1996175;http://dx.doi.org/10.1145/1996130.1996175,Proceedings of the 20th International Symposium on High Performance Distributed Computing,,"Zhang Y,Liu J,Wilson E,Kandemir M",,2011,2011,https://doi.org/10.1145/1996130.1996175;http://dx.doi.org/10.1145/1996130.1996175,software directed data access scheduling for reducing disk energy consumption,1
118,Using Grammatical Evolution for Modelling Energy Consumption on a Computer Numerical Control Machine,"9,78145E+12",,10.1145/3449726.3463185,https://doi.org/10.1145/3449726.3463185;http://dx.doi.org/10.1145/3449726.3463185,Proceedings of the Genetic and Evolutionary Computation Conference Companion,"Discrete manufacturing is known to be a high consumer of energy and much work has been done in continuous improvement and energy saving methods addressing this issue. Computer Numerical Control (CNC) machines, commonly used in the manufacturing of metal parts, are highly energy-demanding because of many required sub-systems, such as cooling, lubrication, logical interfaces and electric motors. For this reason, there is a large body of work focusing on modelling the energy needs of this class of machine.This paper applies Grammatical Evolution (GE) for developing auto-regressive models for the energy consumption of a CNC machine. Empirical data from three 24-hour work shifts comprising three different types of products are used as inputs. We also introduce an autocorrelation-informed approach for the grammar, which benefits from a prior analysis of the training data for better capturing periodic or close to periodic behaviour. Finally, we compare the outcomes from real and predicted energy profiles through the use of an existing analysis tool, which is capable of extracting production-related information such as total and average KW consumption, number of parts produced and breakdown of production and idle hours. Results show that GE yields accurate and explainable models for the analysed scenario.","Carvalho S,Sullivan J,Dias DM,Naredo E,Ryan C",,2021,2021,https://doi.org/10.1145/3449726.3463185;http://dx.doi.org/10.1145/3449726.3463185,using grammatical evolution for modelling energy consumption on a computer numerical control machine,1
119,On Achieving Balanced Power Consumption in Software Pipelined Loops,"9,78158E+12",,10.1145/581630.581663,https://doi.org/10.1145/581630.581663;http://dx.doi.org/10.1145/581630.581663,"Proceedings of the 2002 International Conference on Compilers, Architecture, and Synthesis for Embedded Systems","While a significant body of work in compilers has been devoted to reducing energy consumption in embedded systems, the role of a compiler in harnessing the power variation has not been widely explored. Since sharp power variations across time steps cause power supply noises and degrade reliability of functional blocks, power variation is a design constraint in embedded systems. With the advent of high performance embedded systems and extensive deployment of fine grain clock-gating, reducing variations in power is becoming increasingly important.This paper studies how compilation techniques, more specifically instruction scheduling, can ameliorate variations in power due to functional units during program execution. By extending our previous work on rate-optimal software pipelining, this paper formulates the problem of constructing a performance-optimal schedule that minimizes power variations as an integer linear programming (ILP) problem. The formulation can be solved using an ILP solver. We applied our approach on SPEC NAS benchmarks to construct software pipelined schedules that have minimum power variations. The benchmarks are executed on the Wattch power simulator. In comparison to the original (power-unaware) scheduler implemented in the MIPSpro compiler, our power-aware approach generates schedules which have significantly lower power variations while maintaining the same performance. Such schedules have the potential to reduce hardware cost on power delivery in designing embedded systems.","Yang H,Gao GR,Leung C",,2002,2002,https://doi.org/10.1145/581630.581663;http://dx.doi.org/10.1145/581630.581663,on achieving balanced power consumption in software pipelined loops,1
121,A Model-Based Framework for the Analysis of Software Energy Consumption,"9,78145E+12",,10.1145/3350768.3353813,https://doi.org/10.1145/3350768.3353813;http://dx.doi.org/10.1145/3350768.3353813,Proceedings of the XXXIII Brazilian Symposium on Software Engineering,"Software is present in all types of devices, some of them with restrictions as to the amount of energy they can spend to execute software applications. For this reason, energy costs are becoming an important factor during software development and evolution. However, there is still little support for creating energy-efficient software. In this work, we introduce a possible framework for software energy costs evaluation based on model analysis. We model software as Labelled Transitions Systems (LTS) and annotate these models with energy costs, which can be obtained using existing tools. We can then apply graph-based algorithms to traverse the models to obtain information about energy consumption related to software behaviour, such as its most/least costly execution, the cost of a specific execution, and the average cost of executing the software. No existing tool currently provides all the necessary analyses, even though they are essential for energy-consumption evaluation. We have conducted a small experiment with our framework where we employed jRAPL to measure energy costs. We annotated the models with the collected energy costs using an extended version of the LoTuS tool, where we have also implemented some of the desired analyses. Based on this support and on our initial results, we believe developers could create software more energy-efficient and consider possible trade-offs related to time, space, and energy costs when producing new versions of their systems.","Duarte LM,da Silva Alves D,Toresan BR,Maia PH,Silva D",,2019,2019,https://doi.org/10.1145/3350768.3353813;http://dx.doi.org/10.1145/3350768.3353813,a model based framework for the analysis of software energy consumption,1
122,P4: Phase-Based Power/Performance Prediction of Heterogeneous Systems via Neural Networks,,,,,Proceedings of the 36th International Conference on Computer-Aided Design,"The emergence of Internet of Things increases the complexity and the heterogeneity of computing platforms. Migrating workload between various platforms is one way to improve both energy efficiency and performance. Effective migration decisions require accurate estimates of its costs and benefits. To date, these estimates were done by either instrumenting the source code/binaries, thus causing high overhead, or by using power estimates from hardware performance counters, which work well for individual machines, but until now have not been accurate for predicting across different architectures. In this paper, we propose P4, a new Phase-based Power and Performance Prediction framework which identifies cross-platform application power and performance at runtime for heterogeneous computing systems. P4 analyzes and detects machine-independent application phases by characterizing computing platforms offline with a set of benchmarks, and then builds neural network-based models to automatically identify and generalize the complex cross-platform relationships for each benchmark phase. It then leverages these models along with performance counter measurements collected at runtime to estimate performance and power consumption if it were running on a completely different computing platform, including a different CPU architecture, without ever having to run it on there. We evaluate the proposed framework on four commercial heterogeneous platforms, ranging from X86 servers to mobile ARM-based architecture, with 129 industry-standard benchmarks. Our experimental results show that P4 can predict the power and performance changes with only 6.8% and 5.6% error, respectively, even for completely different architectures from the ones applications ran on.","Kim Y,Mercati P,More A,Shriver E,Rosing T",,2017,2017,,p phase based power performance prediction of heterogeneous systems via neural networks,1
124,Using Mathematical Forecasting Methods to Estimate the Load on the Computing Power of the IoT Network,"9,78145E+12",,10.1145/3440749.3442605,https://doi.org/10.1145/3440749.3442605;http://dx.doi.org/10.1145/3440749.3442605,Proceedings of the 4th International Conference on Future Networks and Distributed Systems,"The size of the network, the number of nodes and connected devices are exponentially increasing due to the development of the Internet of Things (IoT). It becomes difficult to administer the monitoring of heterogeneous networks. It is necessary to use predictive models (Model Predictive Control) to deploy decision support systems related to the IoT network security. The article examines three popular mathematical forecasting methods, evaluates their accuracy and their using possibility in predictive models to solve the problem of assessing the load on the computing power of IoT devices, including servers and services.","Krasov A,Pestov I,Gelfand A,Kazantsev A,Polyanicheva A",,2021,2021,https://doi.org/10.1145/3440749.3442605;http://dx.doi.org/10.1145/3440749.3442605,using mathematical forecasting methods to estimate the load on the computing power of the iot network,1
126,Empirically Modeling How a Multicore Software ICN Router and an ICN Network Consume Power,"9,78145E+12",,10.1145/2660129.2660142,https://doi.org/10.1145/2660129.2660142;http://dx.doi.org/10.1145/2660129.2660142,Proceedings of the 1st ACM Conference on Information-Centric Networking,"ICN (Information Centric Networking) has received much attention due to its built-in functionalities such as caching and mobility-support. One of the important research challenges is to reduce the power consumed by ICN networks because ICN's packet forwarding and packet-level caching are power-hungry. As the first step to achieve power-efficient ICN networks, this paper develops a power consumption model of a multicore software ICN router while taking into account the power consumed by power-hungry computation. This paper makes the following three contributions: First, the model is one of the first realistic models which consider ICN packet forwarding and packet-level caching. Second, the model is represented as a concise set of equations with just a few parameters. Third, we apply the model to estimate power consumed by simple networks.","Hasegawa T,Nakai Y,Ohsugi K,Takemasa J,Koizumi Y,Psaras I",,2014,2014,https://doi.org/10.1145/2660129.2660142;http://dx.doi.org/10.1145/2660129.2660142,empirically modeling how a multicore software icn router and an icn network consume power,1
128,Evaluating Energy Efficiency of Gigabit Ethernet and Infiniband Software Stacks in Data Centres,"9,78148E+12",,10.1109/UCC.2014.10,https://doi.org/10.1109/UCC.2014.10;http://dx.doi.org/10.1109/UCC.2014.10,Proceedings of the 2014 IEEE/ACM 7th International Conference on Utility and Cloud Computing,"Reducing energy consumption has become a key issue for data centres, not only because of economical benefits but also for environmental and marketing reasons. Many approaches tackle this problem from the point of view of different hardware components, such as CPUs, storage and network interface cards (NIC). To this date, few works focused on the energy consumption of network transfers at the software level comprising their complete stacks with different energy characteristics, and the way the NIC selection impacts the energy consumption of applications. Since data centres often install multiple NICs on each node, investigating and comparing them at the software level has high potential to enhance the energy efficiency of applications on Cloud infrastructures. We present a comparative analysis of the energy consumption of the software stack of two of today's most used NICs in data centres, Ethernet and Infiniband. We carefully design for this purpose a set of benchmark experiments to assess the impact of different traffic patterns and interface settings on energy consumption. Using our benchmark results, we derive an energy consumption model for network transfers and evaluate its accuracy for a virtual machine migration scenario. Finally, we propose guidelines for NIC selection from an energy efficiency perspective for different application classes.","Maio V,Nae V,Prodan R",,2014,2014,https://doi.org/10.1109/UCC.2014.10;http://dx.doi.org/10.1109/UCC.2014.10,evaluating energy efficiency of gigabit ethernet and infiniband software stacks in data centres,1
129,DRAT - A Dynamic Resource Allocation Tool for Estimating Compute Power in a Cybersecurity Engineering Learning Facility,"9,78145E+12",,10.1145/3437914.3437980,https://doi.org/10.1145/3437914.3437980;http://dx.doi.org/10.1145/3437914.3437980,Proceedings of 5th Conference on Computing Education Practice,"Cybersecurity laboratory infrastructure has direct impact on the quality of student learning experiences. Because of this, the computing education field has developed a variety of approaches to designing and implementing these learning facilities. Yet, little work has gone into how to properly size cybersecurity laboratory infrastructure relative to student population and curricular compute power demands. The result has been laboratory infrastructures that do not scale with degree programs. Consequently, laboratories are either underpowered, thus limiting learning experiences, or overpowered which wastes financial resources. Accordingly, this work presents DRAT, an open-source software tool, for estimating necessary compute power in a cybersecurity engineering learning facility. More specifically, DRAT is designed to estimate the required discrete compute power on a per exercise basis in a cybersecurity engineering learning facility operating in a private cloud model. Such discrete estimations are intended to communicate physical host hardware requirements such as physical CPU core count, virtual RAM, and total Hard Disk space. The first step in designing DRAT was to forge a model estimator function. Then, we identified a series of scalar abstractions representing learning facility hardware infrastructure and behaving as conversion factors between the model function and output. Because the goal of this work was to provide estimates for cloud compute power requirements, DRAT outputs the number of physical cores, total RAM, total Disk, and total (virtual or physical) Network interfaces required to run the indicated scenario. The implication is that such estimates can inform purchasing and configuration decisions which directly impact student learning outcomes.",M. Pittman J,,2021,2021,https://doi.org/10.1145/3437914.3437980;http://dx.doi.org/10.1145/3437914.3437980,drat a dynamic resource allocation tool for estimating compute power in a cybersecurity engineering learning facility,1
130,NeuPow: A CAD Methodology for High-Level Power Estimation Based on Machine Learning,,1084-4309,10.1145/3388141,https://doi.org/10.1145/3388141;http://dx.doi.org/10.1145/3388141,,"In this article, we present a new, simple, accurate, and fast power estimation technique that can be used to explore the power consumption of digital system designs at an early design stage. We exploit the machine learning techniques to aid the designers in exploring the design space of possible architectural solutions, and more specifically, their dynamic power consumption, which is application-, technology-, frequency-, and data-stimuli dependent. To model the power and the behavior of digital components, we adopt the Artificial Neural Networks (ANNs), while the final target technology is Application Specific Integrated Circuit (ASIC). The main characteristic of the proposed method, called NeuPow, is that it relies on propagating the signals throughout connected ANN models to predict the power consumption of a composite system. Besides a baseline version of the NeuPow methodology that works for a given predefined operating frequency, we also derive an upgraded version that is frequency-aware, where the same operating frequency is taken as additional input by the ANN models. To prove the effectiveness of the proposed methodology, we perform different assessments at different levels. Moreover, technology and scalability studies have been conducted, proving the NeuPow robustness in terms of these design parameters. Results show a very good estimation accuracy with less than 9% of relative error independently from the technology and the size/layers of the design. NeuPow is also delivering a speed-up factor of about 84Ã— with respect to the classical power estimation flow.","Nasser Y,Sau C,PrÃ©votet JC,Fanni T,Palumbo F,HÃ©lard M,Raffo L",ACM Trans. Des. Autom. Electron. Syst.,2020,2020-08,https://doi.org/10.1145/3388141;http://dx.doi.org/10.1145/3388141,neupow a cad methodology for high level power estimation based on machine learning,1
131,"Collecting, Monitoring, and Analyzing Facility and Systems Data at the National Energy Research Scientific Computing Center","9,78145E+12",,10.1145/3339186.3339213,https://doi.org/10.1145/3339186.3339213;http://dx.doi.org/10.1145/3339186.3339213,Workshop Proceedings of the 48th International Conference on Parallel Processing,"As high-performance computing (HPC) resources continue to grow in size and complexity, so too does the volume and velocity of the operational data that is associated with them. At such scales, new mechanisms and technologies are required to continuously gather, store, and analyze this data in near-real time from heterogeneous and distributed sources without impacting the underlying data center operations or HPC resource utilization. In this paper, we describe our experiences in designing and implementing an infrastructure for extreme-scale operational data collection, known as the Operations Monitoring and Notification Infrastructure (OMNI) at the National Energy Research Scientific Computing (NERSC) center at Lawrence Berkeley National Laboratory. OMNI currently holds over 522 billion records of online operational data (totaling over 125TB) and can ingest new data points at an average rate of 25,000 data points per second. Using OMNI as a central repository, facilities and environmental data can be seamlessly integrated and correlated with machine metrics, job scheduler information, network errors, and more, providing a holistic view of data center operations. To demonstrate the value of real-time operational data collection, we present a number of real-world case studies for which having OMNI data readily available led to key operational insights at NERSC. The case results include a reduction in the downtime of an HPC system during a facility transition, as well as a $2.5 million electrical substation savings for the next-generation Perlmutter HPC system.","Bautista E,Romanus M,Davis T,Whitney C,Kubaska T",,2019,2019,https://doi.org/10.1145/3339186.3339213;http://dx.doi.org/10.1145/3339186.3339213,collecting monitoring and analyzing facility and systems data at the national energy research scientific computing center,1
133,Runtime Monitoring of Software Energy Hotspots,"9,78145E+12",,10.1145/2351676.2351699,https://doi.org/10.1145/2351676.2351699;http://dx.doi.org/10.1145/2351676.2351699,Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering,"GreenIT has emerged as a discipline concerned with the optimization of software solutions with regards to their energy consumption. In this domain, most of the state-of-the-art solutions concentrate on coarse-grained approaches to monitor the energy consumption of a device or a process. However, none of the existing solutions addresses in-process energy monitoring to provide in-depth analysis of a process energy consumption. In this paper, we therefore report on a fine-grained runtime energy monitoring framework we developed to help developers to diagnose energy hotspots with a better accuracy than the state-of-the-art. Concretely, our approach adopts a 2-layer architecture including OS-level and process-level energy monitoring. OS-level energy monitoring estimates the energy consumption of processes according to different hardware devices (CPU, network card). Process-level energy monitoring focuses on Java-based applications and builds on OS-level energy monitoring to provide an estimation of energy consumption at the granularity of classes and methods. We argue that this per-method analysis of energy consumption provides better insights to the application in order to identify potential energy hotspots. In particular, our preliminary validation demonstrates that we can monitor energy hotspots of Jetty web servers and monitor their variations under stress scenarios.","Noureddine A,Bourdon A,Rouvoy R,Seinturier L",,2012,2012,https://doi.org/10.1145/2351676.2351699;http://dx.doi.org/10.1145/2351676.2351699,runtime monitoring of software energy hotspots,1
137,Software-Based Sensor Node Energy Estimation,"9,7816E+12",,10.1145/1322263.1322319,https://doi.org/10.1145/1322263.1322319;http://dx.doi.org/10.1145/1322263.1322319,Proceedings of the 5th International Conference on Embedded Networked Sensor Systems,"Being able to estimate the energy consumption of sensor nodes is essential both for evaluating existing sensor network mechanisms and for constructing new energy-aware mechanisms. We present a software-based mechanism for estimating the energy consumption of sensor node at run-time. Unlike previous energy estimation mechanisms, our mechanism does not require any additional hardware components or add-ons.Our demonstration shows the energy estimation in practice on a small network of Tmote Sky motes running the Contiki operating system. A PC connected to one of the motes shows the real-time energy estimation of the network nodes and where the energy is spent: CPU active, CPU sleeping, radio transmitting, radio listening, and LEDs.","Dunkels A,Ã–sterlind F,Tsiftes N,He Z",,2007,2007,https://doi.org/10.1145/1322263.1322319;http://dx.doi.org/10.1145/1322263.1322319,software based sensor node energy estimation,1
138,Live Demonstration: Deep Learning-Based Multiple Object Detection and Tracking on a Low-Power Embedded System,"9,78145E+12",,10.1145/3243394.3243712,https://doi.org/10.1145/3243394.3243712;http://dx.doi.org/10.1145/3243394.3243712,Proceedings of the 12th International Conference on Distributed Smart Cameras,"Despite the hopeful prospect for image processing using Convolutional Neural Netwoks, CNNs, the gap between software and hardware solutions is already considerable for embedded applications due to their high power consumption. This demo performs low-power and real time deep learning-based multiple object tracking implemented on an NVIDIA Jetson TX2 development kit. The performance of the proposed algorithm is exemplified under challenging real scenarios, demonstrating the feasibility of deep learning algorithms on embedded platforms.","Blanco-Filgueira B,GarcÃ­a-Lesta D,FernÃ¡ndez-Sanjurjo M,Brea VM,LÃ³pez P",,2018,2018,https://doi.org/10.1145/3243394.3243712;http://dx.doi.org/10.1145/3243394.3243712,live demonstration deep learning based multiple object detection and tracking on a low power embedded system,1
139,Hybrid Simulation for Embedded Software Energy Estimation,"9,7816E+12",,10.1145/1065579.1065590,https://doi.org/10.1145/1065579.1065590;http://dx.doi.org/10.1145/1065579.1065590,Proceedings of the 42nd Annual Design Automation Conference,"Software energy estimation is a critical step in the design of energy-efficient embedded systems. Instruction-level simulation techniques, despite several advances, remain too slow for iterative use in system-level exploration. In this paper, we propose a methodology called hybrid simulation, which combines instruction set simulation with selective native execution (execution of some parts of the program directly on the simulation host computer), thereby overcoming the disadvantages of instruction-level simulation (low speed) and pure native execution (estimation accuracy, inapplicability to target-dependent code), while exploiting their advantages. Previously developed techniques for software energy macromodeling are utilized to estimate energy consumption for natively executed sub-programs. We identify and address the main challenges involved in hybrid simulation, and present an automatic tool flow for it, which analyzes a given program and selects functions for native execution in order to achieve maximum estimation efficiency while limiting estimation error. We have applied the proposed hybrid simulation methodology to a variety of embedded software programs, resulting in an average speed-up of 70% and estimation error of at most 6%, compared to one of the fastest publicly-available instruction set simulators.","Muttreja A,Raghunathan A,Ravi S,Jha NK",,2005,2005,https://doi.org/10.1145/1065579.1065590;http://dx.doi.org/10.1145/1065579.1065590,hybrid simulation for embedded software energy estimation,1
140,Speeding up Power Estimation of Embedded Software,"9,78158E+12",,10.1145/344166.344580,https://doi.org/10.1145/344166.344580;http://dx.doi.org/10.1145/344166.344580,Proceedings of the 2000 International Symposium on Low Power Electronics and Design,Power is increasingly becoming a design constraint for embedded systems. A processor is responsible for energy consumption on account of the software component of the embedded system. The power estimation of this component is a major concern due to the rising complexities of processors and the slow estimation tools. This work attempts to estimate the energy dissipation of the PR1900 processor based on instruction set model with improved accuracy. The model is integrated in a simulation framework and validated. Over 200 times speedup has been obtained with average 1.4% loss in accuracy over gate level estimation. Analysis of the energy dissipated by the instruction vis a vis the processor architecture has been carried out and a substantial reduction in the measurement effort to build the processor energy model has been achieved.,"Sama A,Theeuwen JF,Balakrishnan M",,2000,2000,https://doi.org/10.1145/344166.344580;http://dx.doi.org/10.1145/344166.344580,speeding up power estimation of embedded software,1
141,UNet-NILM: A Deep Neural Network for Multi-Tasks Appliances State Detection and Power Estimation in NILM,"9,78145E+12",,10.1145/3427771.3427859,https://doi.org/10.1145/3427771.3427859;http://dx.doi.org/10.1145/3427771.3427859,Proceedings of the 5th International Workshop on Non-Intrusive Load Monitoring,"Over the years, an enormous amount of research has been exploring Deep Neural Networks (DNN), particularly Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) for estimating the energy consumption of appliances from a single point source such as smart meters - Non-Intrusive Load Monitoring (NILM). However, most of the existing DNNs models for NILM use a single-task learning approach in which a neural network is trained exclusively for each appliance. This strategy is computationally expensive and ignores the fact that multiple appliances can be active simultaneously and dependencies between them. In this work, we propose UNet-NILM for multi-task appliances' state detection and power estimation, applying a multi-label learning strategy and multi-target quantile regression. The UNet-NILM is a one-dimensional CNN based on the U-Net architecture initially proposed for image segmentation. Empirical evaluation on the UK-DALE dataset suggests promising performance against traditional single-task learning.","Faustine A,Pereira L,Bousbiat H,Kulkarni S",,2020,2020,https://doi.org/10.1145/3427771.3427859;http://dx.doi.org/10.1145/3427771.3427859,unet nilm a deep neural network for multi tasks appliances state detection and power estimation in nilm,1
142,Energy and Performance Evaluation of Lossless File Data Compression on Server Systems,"9,78161E+12",,10.1145/1534530.1534536,https://doi.org/10.1145/1534530.1534536;http://dx.doi.org/10.1145/1534530.1534536,Proceedings of SYSTOR 2009: The Israeli Experimental Systems Conference,"Data compression has been claimed to be an attractive solution to save energy consumption in high-end servers and data centers. However, there has not been a study to explore this. In this paper, we present a comprehensive evaluation of energy consumption for various file compression techniques implemented in software. We apply various compression tools available on Linux to a variety of data files, and we try them on server class and workstation class systems. We compare their energy and performance results against raw reads and writes. Our results reveal that software based data compression cannot be considered as a universal solution to reduce energy consumption. Various factors like the type of the data file, the compression tool being used, the read-to-write ratio of the workload, and the hardware configuration of the system impact the efficacy of this technique. In some cases, however, we found compression to save substantial energy and improve performance.","Kothiyal R,Tarasov V,Sehgal P,Zadok E",,2009,2009,https://doi.org/10.1145/1534530.1534536;http://dx.doi.org/10.1145/1534530.1534536,energy and performance evaluation of lossless file data compression on server systems,1
144,A Multi-Level Strategy for Software Power Estimation,"9,78158E+12",,,,Proceedings of the 13th International Symposium on System Synthesis,In this paper a comprehensive methodology for software power estimation is presented. The methodology is supported by rigorous mathematical models of power consumption at three different levels of abstraction. The methodology has been validated in a complete framework developed within the TOSCA co-design environment.,"Brandolese C,Fornaciari W,Pomante L,Salice F,Sciuto D",,2000,2000,,a multi level strategy for software power estimation,1
145,Software-Based on-Line Energy Estimation for Sensor Nodes,"9,7816E+12",,10.1145/1278972.1278979,https://doi.org/10.1145/1278972.1278979;http://dx.doi.org/10.1145/1278972.1278979,Proceedings of the 4th Workshop on Embedded Networked Sensors,"Energy is of primary importance in wireless sensor networks. By being able to estimate the energy consumption of the sensor nodes, applications and routing protocols are able to make informed decisions that increase the lifetime of the sensor network. However, it is in general not possible to measure the energy consumption on popular sensor node platforms. In this paper, we present and evaluate a software-based on-line energy estimation mechanism that estimates the energy consumption of a sensor node. We evaluate the mechanism by comparing the estimated energy consumption with the lifetime of capacitor-powered sensor nodes. By implementing and evaluating the X-MAC protocol, we show how software-based on-line energy estimation can be used to empirically evaluate the energy efficiency of sensor network protocols.","Dunkels A,Osterlind F,Tsiftes N,He Z",,2007,2007,https://doi.org/10.1145/1278972.1278979;http://dx.doi.org/10.1145/1278972.1278979,software based on line energy estimation for sensor nodes,1
146,AI for Audience Prediction and Profiling to Power Innovative TV Content Recommendation Services,"9,78145E+12",,10.1145/3347449.3357485,https://doi.org/10.1145/3347449.3357485;http://dx.doi.org/10.1145/3347449.3357485,"Proceedings of the 1st International Workshop on AI for Smart TV Content Production, Access and Delivery","In contemporary TV audience prediction, outliers are considered mere anomalies in the otherwise cyclical trend and seasonality components that can be used to make predictions. In the ReTV project, we want to provide more accurate audience predictions in order to enable innovative services for TV content recommendation. This paper presents a concept for identifying the source of outliers and factoring TV content categories and the occurrence of events as additional features for training TV audience prediction. We show how this can improve the accuracy of the audience prediction. Finally, we outline how this work could also be combined with AI-enabled audience profiling to power new content recommendation services.","Nixon L,Ciesielski K,Philipp B",,2019,2019,https://doi.org/10.1145/3347449.3357485;http://dx.doi.org/10.1145/3347449.3357485,ai for audience prediction and profiling to power innovative tv content recommendation services,1
149,Software Energy Estimation Based on Statistical Characterization of Intermediate Compilation Code,"9,78161E+12",,,,Proceedings of the 17th IEEE/ACM International Symposium on Low-Power Electronics and Design,"Early estimation of embedded software power consumption is a critical issue that can determine the quality and, sometimes, the feasibility of a system. Architecture-specific, cycle-accurate simulators are valuable tools for fine-tuning performance of critical sections of the application but are often too slow for the simulation of entire systems. This paper proposes a fast and statistically accurate methodology to evaluate the energy performance of embedded software and describes the associated toolchain. The methodology is based on a static characterization of the target instruction set to allow estimation on an equivalent, target-independent intermediate code representation.","Brandolese C,Corbetta S,Fornaciari W",,2011,2011,,software energy estimation based on statistical characterization of intermediate compilation code,1
150,"A Power-Measurement Methodology for Large-Scale, High-Performance Computing","9,78145E+12",,10.1145/2568088.2576795,https://doi.org/10.1145/2568088.2576795;http://dx.doi.org/10.1145/2568088.2576795,Proceedings of the 5th ACM/SPEC International Conference on Performance Engineering,"Improvement in the energy efficiency of supercomputers can be accelerated by improving the quality and comparability of efficiency measurements. The ability to generate accurate measurements at extreme scale are just now emerging. The realization of system-level measurement capabilities can be accelerated with a commonly adopted and high quality measurement methodology for use while running a workload, typically a benchmark. This paper describes a methodology that has been developed collaboratively through the Energy Efficient HPC Working Group to support architectural analysis and comparative measurements for rankings, such as the Top500 and Green500. To support measurements with varying amounts of effort and equipment required we present three distinct levels of measurement, which provide increasing levels of accuracy. Level 1 is similar to the Green500 run rules today, a single average power measurement extrapolated from a subset of a machine. Level 2 is more comprehensive, but still widely achievable. Level 3 is the most rigorous of the three methodologies but is only possible at a few sites. However, the Level 3 methodology generates a high quality result that exposes details that the other methodologies may miss. In addition, we present case studies from the Leibniz Supercomputing Centre (LRZ), Argonne National Laboratory (ANL) and Calcul QuÃ©bec UniversitÃ© Laval that explore the benefits and difficulties of gathering high quality, system-level measurements on large-scale machines.","Scogland TR,Steffen CP,Wilde T,Parent F,Coghlan S,Bates N,Feng WC,Strohmaier E",,2014,2014,https://doi.org/10.1145/2568088.2576795;http://dx.doi.org/10.1145/2568088.2576795,a power measurement methodology for large scale high performance computing,1
151,Software Power Estimation Using IPI(Inter-Prefetch Interval) Power Model for Advanced off-the-Shelf Processor,"9,7816E+12",,10.1145/1228784.1228923,https://doi.org/10.1145/1228784.1228923;http://dx.doi.org/10.1145/1228784.1228923,Proceedings of the 17th ACM Great Lakes Symposium on VLSI,"This paper addresses a problem of modeling the power consumption of advanced off-the-shelf processors. Unlike existing methods for processor power estimation, where the internal information of processor architecture such as activation of specfic modules such as pipeline stages, etc.) is available via simulation or runtime counters, power modeling method presented in this paper is to estimate the power consumption of complex off-the-shelf RISC processor without such a detailed information, only based on the information of the processor I/O signals (i.e memory access). To tackle this problem, we propose a new power model, called IPI(Instruction-Prefetch Interval) power model. The IPI represents the time interval between two consecutive instruction prefetchs. Our model has two major advantages. First, this model can consider prefetch mechanism. Most of advanced RISC processors have prefetch mechanism which makes processor power estimation difficult. IPI model is the first approach to model prefetch mechanism in processor power estimation. Second, this model can provide power variation in time and therefore it overcomes the limitation of previous work, such as instruction-level energy model. Experiments show that the proposed model yields 96% accuracy on theaverage in case of ARM1136JF-S test chip.","Kang K,Kim J,Shim H,Kyung CM",,2007,2007,https://doi.org/10.1145/1228784.1228923;http://dx.doi.org/10.1145/1228784.1228923,software power estimation using ipi inter prefetch interval power model for advanced off the shelf processor,1
152,An Environment for Automated Power Measurements on Mobile Computing Platforms,"9,78145E+12",,10.1145/2498328.2500064,https://doi.org/10.1145/2498328.2500064;http://dx.doi.org/10.1145/2498328.2500064,Proceedings of the 51st ACM Southeast Conference,"Mobile computing devices such as smartphones, tablet computers, and e-readers have become the dominant personal computing platforms. Energy efficiency is a prime design requirement for mobile device manufacturers and smart application developers alike. Runtime power measurements on mobile platforms provide insights that can eventually lead to more energy-efficient operation. In this paper we describe mPowerProfile - an environment for automated power measurements of programs running on a mobile development platform. We discuss mPowerProfile's main functions and its utilization in several example studies based on the Pandaboard and Raspberry Pi platforms.","Milosevic M,Dzhagaryan A,Jovanov E,Milenkovi? A",,2013,2013,https://doi.org/10.1145/2498328.2500064;http://dx.doi.org/10.1145/2498328.2500064,an environment for automated power measurements on mobile computing platforms,1
153,Computational and Storage Power Optimizations for the O-GEHL Branch Predictor,"9,7816E+12",,10.1145/1242531.1242549,https://doi.org/10.1145/1242531.1242549;http://dx.doi.org/10.1145/1242531.1242549,Proceedings of the 4th International Conference on Computing Frontiers,"In recent years, highly accurate branch predictors have been proposed primarily for high performance processors. Unfortunately such predictors are extremely energy consuming and in some cases not practical as they come with excessive prediction latency. One example of such predictors is the O-GEHL predictor. To achieve high accuracy, O-GEHL relies on large tables and extensive computations and requires high energy and long prediction delay.In this work we propose power optimization techniques that aim at reducing both computational complexity and storage size for the O-GEHL predictor. We show that by eliminating unnecessary data from computations, we can reduce both predictor's energy consumption and delay. Moreover, we apply information theory findings to remove redundant storage, without any significant accuracy penalty. We reduce the dynamic and static power dissipated in the computational parts of the predictor by up to 74% and 65% respectively. Meantime we improve performance by up to 12% as we make faster prediction possible.","Aasaraai K,Baniasadi A,Atoofian E",,2007,2007,https://doi.org/10.1145/1242531.1242549;http://dx.doi.org/10.1145/1242531.1242549,computational and storage power optimizations for the o gehl branch predictor,1
154,Early Power Estimation of CUDA-Based CNNs on GPGPUs: Work-in-Progress,"9,78145E+12",,10.1145/3478684.3479255,https://doi.org/10.1145/3478684.3479255;http://dx.doi.org/10.1145/3478684.3479255,Proceedings of the 2021 International Conference on Hardware/Software Codesign and System Synthesis,"The increasing application of Machine Learning (ML) techniques in the Internet of Things (IoT) devices has led designers to leverage ML accelerators like GPGPUs in such devices. However, choosing the most appropriate accelerator for such IoT devices is very challenging as they commonly should adhere to tight constraints e.g., low power consumption, long battery lifetime, and low cost of the final products. As a consequence, designing such application-specific IoT devices becomes a non-trivial and difficult task. In this paper, we present a novel approach to estimate power consumption of CUDA-based Convolutional Neural Networks (CNNs) on GPGPUs in the design phase. Our approach is able to provide designers with an early prediction of CNNs power consumption up to an absolute error of less than 2% in comparison to the real hardware execution.","Metz CA,Goli M,Drechsler R",,2021,2021,https://doi.org/10.1145/3478684.3479255;http://dx.doi.org/10.1145/3478684.3479255,early power estimation of cuda based cnns on gpgpus work in progress,1
155,The Power of System Call Traces: Predicting the Software Energy Consumption Impact of Changes,,,,,Proceedings of 24th Annual International Conference on Computer Science and Software Engineering,"Battery is a critical resource for smartphones. Software developers as the builders and maintainers of applications, are responsible for updating and deploying energy efficient applications to end users. Unfortunately, the impact of software change on energy consumption is still unclear. Estimation based on software metrics has proved difficult. As energy consumption profiling requires special infrastructure, developers have difficulty assessing the impact of their actions on energy consumption. System calls are the interface between applications and the OS kernel and provide insight into how software utilizes hardware and software resources. As profiling system calls requires no specialized infrastructure, unlike energy consumption, it is much easier for the developers to track changes to system calls. Thus we relate software change to energy consumption by tracing the changes in an application's pattern of system call invocations. We find that significant changes to system call profiles often induce significant changes in energy consumption.","Aggarwal K,Zhang C,Campbell JC,Hindle A,Stroulia E",,2014,2014,,the power of system call traces predicting the software energy consumption impact of changes,1
156,Towards Neural Hardware Search: Power Estimation of CNNs for GPGPUs with Dynamic Frequency Scaling,"9,78145E+12",,10.1145/3551901.3556481,https://doi.org/10.1145/3551901.3556481;http://dx.doi.org/10.1145/3551901.3556481,Proceedings of the 2022 ACM/IEEE Workshop on Machine Learning for CAD,"Machine Learning (ML) algorithms are essential for emerging technologies such as autonomous driving and application-specific Internet of Things(IoT) devices. Convolutional Neural Network(CNN) is one of the major techniques used in such systems. This leads to leveraging ML accelerators like GPGPUs to meet the design constraints. However, GPGPUs have high power consumption, and selecting the most appropriate accelerator requires Design Space Exploration(DSE), which is usually time-consuming and needs high manual effort. Neural Hardware Search(NHS) is an upcoming approach to automate the DSE for Neural Networks. Therefore, automatic approaches for power, performance, and memory estimations are needed.In this paper, we present a novel approach, enabling designers to fast and accurately estimate the power consumption of CNNs inferencing on GPGPUs with Dynamic Frequency Scaling(DFS) in the early stages of the design process. The proposed approach uses static analysis for feature extraction and Random Forest Tree regression analysis for predictive model generation. Experimental results demonstrate that our approach can predict the CNNs power consumption with a Mean Absolute Percentage Error(MAPE) of 5.03% compared to the actual hardware.","Metz CA,Goli M,Drechsler R",,2022,2022,https://doi.org/10.1145/3551901.3556481;http://dx.doi.org/10.1145/3551901.3556481,towards neural hardware search power estimation of cnns for gpgpus with dynamic frequency scaling,1
157,Scalability Evaluation of an Energy-Aware Resource Management System for Clusters of Web Servers,"9,78151E+12",,,,Proceedings of the International Symposium on Performance Evaluation of Computer and Telecommunication Systems,"For green cluster computing resource management systems have to be energy-aware. CHERUB is such an energy-aware resource management system which works together with the Linux Virtual Server. Experiments in a small cluster setup with two nodes have shown the benefit of CHERUB. This paper presents necessary design changes to make CHERUB also work in big cluster setups. Our methodological approach is two-fold. First, we present unit measurements to evaluate the scaling of the re-implemented functions. Second, a cluster simulator is presented and validated which makes it possible to test CHERUB for backend clusters of arbitrary size.","Kiertscher S,Schnor B",,2015,2015,,scalability evaluation of an energy aware resource management system for clusters of web servers,1
158,An Experimental Evaluation of Datacenter Workloads on Low-Power Embedded Micro Servers,,2150-8097,10.14778/2947618.2947625,https://doi.org/10.14778/2947618.2947625;http://dx.doi.org/10.14778/2947618.2947625,,"This paper presents a comprehensive evaluation of an ultra-low power cluster, built upon the Intel Edison based micro servers. The improved performance and high energy efficiency of micro servers have driven both academia and industry to explore the possibility of replacing conventional brawny servers with a larger swarm of embedded micro servers. Existing attempts mostly focus on mobile-class micro servers, whose capacities are similar to mobile phones. We, on the other hand, target on sensor-class micro servers, which are originally intended for uses in wearable technologies, sensor networks, and Internet-of-Things. Although sensor-class micro servers have much less capacity, they are touted for minimal power consumption (< 1 Watt), which opens new possibilities of achieving higher energy efficiency in datacenter workloads. Our systematic evaluation of the Edison cluster and comparisons to conventional brawny clusters involve careful workload choosing and laborious parameter tuning, which ensures maximum server utilization and thus fair comparisons. Results show that the Edison cluster achieves up to 3.5x improvement on work-done-per-joule for web service applications and data-intensive MapReduce jobs. In terms of scalability, the Edison cluster scales linearly on the throughput of web service workloads, and also shows satisfactory scalability for MapReduce workloads despite coordination overhead.","Zhao Y,Li S,Hu S,Wang H,Yao S,Shao H,Abdelzaher T",Proc. VLDB Endow.,2016,2016-05,https://doi.org/10.14778/2947618.2947625;http://dx.doi.org/10.14778/2947618.2947625,an experimental evaluation of datacenter workloads on low power embedded micro servers,1
159,Green Tracker: A Tool for Estimating the Energy Consumption of Software,"9,78161E+12",,10.1145/1753846.1753981,https://doi.org/10.1145/1753846.1753981;http://dx.doi.org/10.1145/1753846.1753981,CHI '10 Extended Abstracts on Human Factors in Computing Systems,"The energy consumption of computers has become an important environmental issue. This paper describes the development of Green Tracker, a tool that estimates the energy consumption of software in order to help concerned users make informed decisions about the software they use. We present preliminary results gathered from this system's initial usage. Ultimately the information gathered from this tool will be used to raise awareness and help make the energy consumption of software a more central concern among software developers.","Amsel N,Tomlinson B",,2010,2010,https://doi.org/10.1145/1753846.1753981;http://dx.doi.org/10.1145/1753846.1753981,green tracker a tool for estimating the energy consumption of software,1
161,"Modeling, Measurement and Computer Power","9,78145E+12",,10.1145/1478873.1478967,https://doi.org/10.1145/1478873.1478967;http://dx.doi.org/10.1145/1478873.1478967,"Proceedings of the May 16-18, 1972, Spring Joint Computer Conference","Since the early 1960s the literature reveals increasing concern with effectiveness of information processing systems and our ability to predict influences of system parameters. A recent survey paper discusses methods of performance evaluation related to three practical goals: selection of the best among several existing systems; design of a not-yet existing system; and analysis of an existing accessible system. The classification of goals is useful, but we can point to neither the models nor the measures nor the measurement tools to allow reliable judgments with respect to those three important goals at this time.","Estrin G,Muntz RR,Uzgalis RC",,1971,1971,https://doi.org/10.1145/1478873.1478967;http://dx.doi.org/10.1145/1478873.1478967,modeling measurement and computer power,1
162,The Accuracy of Android Energy Measurements for Offloading Computational Expensive Tasks: Poster,"9,78145E+12",,10.1145/2942358.2942412,https://doi.org/10.1145/2942358.2942412;http://dx.doi.org/10.1145/2942358.2942412,Proceedings of the 17th ACM International Symposium on Mobile Ad Hoc Networking and Computing,"Computational offloading from smartphones into the cloud has proved to be one of useful approaches for improving energy efficiency. To assess the benefit of offloading and decide the most suitable offloading strategies, it is essential to account the energy consumption of smartphones. In this paper, we investigate the accuracy and capabilities of the Android smart battery interface. For comparison, we measure the energy consumption using an oscilloscope. We experimentally investigate the energy consumption of different applications on a modern smartphones including local computation and network communication over WiFi. Our results show that both of methods bring high accuracy. Our work builds the basis for next generation offloading algorithms.","Nguyen QH,Dressler F",,2016,2016,https://doi.org/10.1145/2942358.2942412;http://dx.doi.org/10.1145/2942358.2942412,the accuracy of android energy measurements for offloading computational expensive tasks poster,1
163,Saving Energy on Smartphones through Edge Computing: An Experimental Evaluation,"9,78145E+12",,10.1145/3538393.3544935,https://doi.org/10.1145/3538393.3544935;http://dx.doi.org/10.1145/3538393.3544935,Proceedings of the ACM SIGCOMM Workshop on Networked Sensing Systems for a Sustainable Society,"Edge computing is a network architecture in which computing and storage capabilities are moved at the fringes of the Internet, close to the end-users. The main goal of edge computing is to enable responsive services, thanks to much shorter paths compared to the ones encountered when communicating with remotely positioned cloud servers. In this paper, we report experimental results concerning an overlooked benefit of edge computing: energy is saved on client devices. We carried out an experimental evaluation using both software-based and hardware-based energy estimation methods. Results show that, for HTTP-based communication, the lifetime of a device can be extended significantly when using the edge instead of a remote cloud.","Caiazza C,Luconi V,Vecchio A",,2022,2022,https://doi.org/10.1145/3538393.3544935;http://dx.doi.org/10.1145/3538393.3544935,saving energy on smartphones through edge computing an experimental evaluation,1
166,Estimating Energy Impact of Software Releases and Deployment Strategies: The KPMG Case Study,"9,78151E+12",,10.1109/ESEM.2017.39,https://doi.org/10.1109/ESEM.2017.39;http://dx.doi.org/10.1109/ESEM.2017.39,Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement,"Background. Often motivated by optimization objectives, software products are characterized by different subsequent releases and deployed through different strategies. The impact of these two aspects of software on energy consumption has still to be completely understood and can be improved by carrying out ad-hoc analyses for specific software products.Aims. In this research we report on an industrial collaboration aiming at assessing the different impact that releases and deployment strategies of a software product can have on the energy consumption of its underlying hardware infrastructure.Method. We designed and performed an empirical experiment in a controlled environment. Deployment strategies, releases and use case scenarios of an industrial third-party software product were adopted as experimental factors. The use case scenarios were used as a blocking factor and adopted to dynamically load-test the software product. Power consumption and execution time were selected as response variables to measure the energy consumption.Results. We observed that both deployment strategies and software releases significantly influence the energy consumption of the hardware infrastructure. A strong interaction between the two factors was identified. The impact of such interaction highly varied depending on which use case scenario was considered, making the identification of the most frequently adopted use case scenario critical for energy optimisation. The collaboration between industry and academia has been productive for both parties, even if some practitioners manifested low interest/awareness on software energy efficiency.Conclusions. For the software product considered there is no absolute preferable release or deployment strategy with respect to energy efficiency, as the interaction of these factors has to be considered. The number of machines involved in a software deployment strategy does not simply constitute an additive effect of the energy consumption of the underlying hardware infrastructure.","Verdecchia R,Procaccianti G,Malavolta I,Lago P,Koedijk J",,2017,2017,https://doi.org/10.1109/ESEM.2017.39;http://dx.doi.org/10.1109/ESEM.2017.39,estimating energy impact of software releases and deployment strategies the kpmg case study,1
167,Energy Evaluation of Software Implementations of Block Ciphers under Memory Constraints,"9,78398E+12",,,,"Proceedings of the Conference on Design, Automation and Test in Europe","Software implementations of modern block ciphers often require large lookup tables along with code size increasing optimizations like loop unrolling to reach peak performance on general-purpose processors. Therefore, block ciphers are difficult to implement efficiently on embedded devices like cell phones or sensor nodes where run-time memory and program ROM are scarce resources. In this paper we analyze and compare the performance, energy consumption, run-time memory requirements, and code size of the five block ciphers RC6, Rijndael, Serpent, Twofish, and XTEA on the StrongARM SA-1100 processor. Most previous evaluations of block ciphers considered performance as the sole metric of interest and did not care about memory requirements or code size. In contrast to previous work, our study of the performance and energy characteristics of block ciphers has been conducted with ""lightweight"" implementations which restrict the size of lookup tables to 1 kB and also impose constraints on the code size. We found that Rijndael and RC6 can be well optimized for high performance and energy efficiency, while at the same time meeting the demand for low memory (RAM and ROM) footprint. In addition, we discuss the impact of key expansion and modes of operation on the overall performance and energy consumption of each block cipher. Our simulation results show that RC6 is the most energy-efficient block cipher under memory constraints and thus the best choice for resource-restricted devices.","GroÃŸschÃ¤dl J,Tillich S,Rechberger C,Hofmann M,Medwed M",,2007,2007,,energy evaluation of software implementations of block ciphers under memory constraints,1
168,In-Depth Measurement and Analysis on Densification Power Law of Software Execution,"9,78145E+12",,10.1145/2593868.2593878,https://doi.org/10.1145/2593868.2593878;http://dx.doi.org/10.1145/2593868.2593878,Proceedings of the 5th International Workshop on Emerging Trends in Software Metrics,"Measuring software execution is important for many software engineering tasks. In this paper, Densification Power Law (DPL) of software execution is measured and studied as a feature of growing software complexity. Densification means that during a networked system's evolution, it usually becomes denser and the number of edges and nodes grows with a consistent super linear relation. This feature was discovered and reported in 2005. In this paper, based on a measurement of 15 open-source Java programs, we show that when software systems are modeled as a series of dynamic Call Graphs during their executions, they always obey DPL with very close correlation. Then a comparison between static Call Graph and DPL is presented, showing that DPL's properties cannot be derived statically. An explanation for DPL of software execution is given and verified. We believe the universality of DPL makes it an appropriate metric for software execution process.","Qu Y,Zheng Q,Liu T,Li J,Guan X",,2014,2014,https://doi.org/10.1145/2593868.2593878;http://dx.doi.org/10.1145/2593868.2593878,in depth measurement and analysis on densification power law of software execution,1
170,On Energy Debt: Managing Consumption on Evolving Software,"9,78145E+12",,10.1145/3387906.3388628,https://doi.org/10.1145/3387906.3388628;http://dx.doi.org/10.1145/3387906.3388628,Proceedings of the 3rd International Conference on Technical Debt,"This paper introduces the concept of energy debt: a new metric, reflecting the implied cost in terms of energy consumption over time, of choosing a flawed implementation of a software system rather than a more robust, yet possibly time consuming, approach. A flawed implementation is considered to contain code smells, known to have a negative influence on the energy consumption.Similar to technical debt, if energy debt is not properly addressed, it can accumulate an energy ""interest"". This interest will keep increasing as new versions of the software are released, and eventually reach a point where the interest will be higher than the initial energy debt. Addressing the issues/smells at such a point can remove energy debt, at the cost of having already consumed a significant amount of energy which can translate into high costs. We present all underlying concepts of energy debt, bridging the connection with the existing concept of technical debt and show how to compute the energy debt through a motivational example.","Couto M,Maia D,Saraiva J,Pereira R",,2020,2020,https://doi.org/10.1145/3387906.3388628;http://dx.doi.org/10.1145/3387906.3388628,on energy debt managing consumption on evolving software,1
171,GreenOracle: Estimating Software Energy Consumption with Energy Measurement Corpora,"9,78145E+12",,10.1145/2901739.2901763,https://doi.org/10.1145/2901739.2901763;http://dx.doi.org/10.1145/2901739.2901763,Proceedings of the 13th International Conference on Mining Software Repositories,"Software energy consumption is a relatively new concern for mobile application developers. Poor energy performance can harm adoption and sales of applications. Unfortunately for the developers, the measurement of software energy consumption is expensive in terms of hardware and difficult in terms of expertise. Many prior models of software energy consumption assume that developers can use hardware instrumentation and thus cannot evaluate software running within emulators or virtual machines. Some prior models require actual energy measurements from the previous versions of applications in order to model the energy consumption of later versions of the same application.In this paper, we take a big-data approach to software energy consumption and present a model that can estimate software energy consumption mostly within 10% error (in joules) and does not require the developer to train on energy measurements of their own applications. This model leverages a big-data approach whereby a collection of prior applications' energy measurements allows us to train, transmit, and apply the model to estimate any foreign application's energy consumption for a test run. Our model is based on the dynamic traces of system calls and CPU utilization.","Chowdhury SA,Hindle A",,2016,2016,https://doi.org/10.1145/2901739.2901763;http://dx.doi.org/10.1145/2901739.2901763,greenoracle estimating software energy consumption with energy measurement corpora,1
173,Poster: A Novel Computation Offloading Technique for Reducing Energy Consumption of Smart Watch,"9,78145E+12",,10.1145/2938559.2948825,https://doi.org/10.1145/2938559.2948825;http://dx.doi.org/10.1145/2938559.2948825,"Proceedings of the 14th Annual International Conference on Mobile Systems, Applications, and Services Companion",,"Ko J,Lee J,Choi YJ",,2016,2016,https://doi.org/10.1145/2938559.2948825;http://dx.doi.org/10.1145/2938559.2948825,poster a novel computation offloading technique for reducing energy consumption of smart watch,1
175,DeepPM: Transformer-Based Power and Performance Prediction for Energy-Aware Software,"9,78398E+12",,,,"Proceedings of the 2022 Conference & Exhibition on Design, Automation & Test in Europe","Many system-level management and optimization techniques need accurate estimates of power consumption and performance. Earlier research has proposed many high-level/source-level estimation modeling works, particularly for basic blocks. However, most of them still need to execute the target software at least once on a fine-grained simulator or real hardware to extract required features. This paper proposes a performance/power prediction framework, called Deep Power Meter (DeepPM), which estimates them accurately only using the compiled binary. Inspired by the deep learning techniques in natural language processing, we convert the program instructions in the form of vectors and predict the average power and performance of basic blocks based on a transformer model. In addition, unlike existing works based on a Long Short-Term Memory (LSTM) model structure, which only works for basic blocks with a small number of instructions, DeepPM provides highly accurate results for long basic blocks, which takes the majority of the execution time for actual application runs. In our evaluation conducted with SPEC2006 benchmark suite, we show that DeepPM can provide accurate prediction for performance and power consumption with 10.2% and 12.3% error, respectively. DeepPM also outperforms the LSTM-based model by up to 67.2% and 34.9% error for performance and power, respectively.","Shim JS,Han B,Kim Y,Kim J",,2022,2022,,deeppm transformer based power and performance prediction for energy aware software,1
176,SpinSmart: Exploring Optimal Server Fan Speeds to Improve Overall System Energy Consumption,"9,78145E+12",,10.1145/3396851.3402655,https://doi.org/10.1145/3396851.3402655;http://dx.doi.org/10.1145/3396851.3402655,Proceedings of the Eleventh ACM International Conference on Future Energy Systems,"Cost of data centers has risen sharply in the past few years. Today, it represents about 3% of total US energy consumption with projections to increase further in the coming years. In this paper, we focus on the server infrastructure and observe that workload consolidation techniques, which maximize power efficiency of server systems, do not automatically optimize the overall system power efficiency especially when compute engines and the corresponding on-board cooling systems are considered holistically. We design SpinSmart, a framework that explores optimal server fan speeds to minimize the overall system energy consumption. We explore core capping strategies that estimate the desired number of CPU cores to be used at any given time to minimize combined CPU+fan power. Our experimental results show that we are able to achieve 1) energy savings of up to 10% of total energy and 80% of cooling energy when compared to workload consolidation without core capping strategy; 2) cooling energy savings up to 42% when compared to the strategy that randomly assigns jobs to all the servers and cores.","Tian M,Vishwanath A,Venkataramani G,Subramaniam S",,2020,2020,https://doi.org/10.1145/3396851.3402655;http://dx.doi.org/10.1145/3396851.3402655,spinsmart exploring optimal server fan speeds to improve overall system energy consumption,1
178,LEMAX: Learning-Based Energy Consumption Minimization in Approximate Computing with Quality Guarantee,"9,78145E+12",,10.1145/3195970.3196069,https://doi.org/10.1145/3195970.3196069;http://dx.doi.org/10.1145/3195970.3196069,Proceedings of the 55th Annual Design Automation Conference,"Approximate computing aims to trade accuracy for energy efficiency. Various approximate methods have been proposed in the literature that demonstrate the effectiveness of relaxing accuracy requirements in a specific unit. This provides a basis for exploring simultaneous use of multiple approximate units to improve efficiency under guarantees on quality of results. In this paper, we explore the effect of combining multiple approximate units on the energy consumption and identify the best setting that minimizes energy consumption under a quality constraint. Our approach also enables changes in unit configurations throughout the program. To do this effectively, we need a method to examine the combined impact of multiple approximate units on the output quality, and configure individual units accordingly. To solve this problem, we propose LEMAX that uses gradient descent approach to identify the best configuration of the individual approximate units for a given program. We evaluate the efficacy of LEMAX in minimizing the energy consumption of several machine learning applications with varying size (i.e., number of operations) under different quality constraints. Our evaluation shows that the configuration provided by LEMAX for a system with multiple approximate units improves the energy consumption by on average, 97.7%, 83.12%, and 73.95% for quality loss of 5%, 2% and 0.5%, respectively, compared to configurations obtained for a system with a single approximate resource.","Akhlaghi V,Gao S,Gupta RK",,2018,2018,https://doi.org/10.1145/3195970.3196069;http://dx.doi.org/10.1145/3195970.3196069,lemax learning based energy consumption minimization in approximate computing with quality guarantee,1
179,Power: A Tool for Quantitative Evaluation of Software Project Effectiveness,"9,78082E+12",,,,Proceedings of the 7th International Conference on Software Engineering,In this paper a tool for quantitative method of evaluating software projects is presented. This method constitutes the Project Observation Workbench and Evaluation Reportor (POWER). A representative project evaluation with POWER is presented to demonstrate its application.,"Evans MW,Picinich LM",,1984,1984,,power a tool for quantitative evaluation of software project effectiveness,1
180,The Cost of Virtue: Reward as Well as Feedback Are Required to Reduce User ICT Power Consumption,"9,78145E+12",,10.1145/2602044.2602063,https://doi.org/10.1145/2602044.2602063;http://dx.doi.org/10.1145/2602044.2602063,Proceedings of the 5th International Conference on Future Energy Systems,"We show that students in a school lab environment will change their behaviour to be more energy efficient, when appropriate incentives are in place, and when measurement-based, real-time feedback about their energy usage is provided. Rewards incentivise `non-green' users to be `green' as well as encouraging those users who already claim to be `green'. Measurement-based feedback improves user energy awareness and helps users to explore and adjust their use of computers to become `greener', but is not sufficient by itself. In our measurements, weekly mean group energy use as a whole reduced by up to 16%; and weekly individual user energy consumption reduced by up to 56% during active use. The findings are drawn from our longitudinal study that involved 83 Computer Science students; lasted 48 weeks across 2 academic years; monitored a total of 26778 hours of active computer use; collected approximately 2TB of raw data.","Yu Y,Bhatti SN",,2014,2014,https://doi.org/10.1145/2602044.2602063;http://dx.doi.org/10.1145/2602044.2602063,the cost of virtue reward as well as feedback are required to reduce user ict power consumption,1
181,Products Go Green: Worst-Case Energy Consumption in Software Product Lines,"9,78145E+12",,10.1145/3106195.3106214,https://doi.org/10.1145/3106195.3106214;http://dx.doi.org/10.1145/3106195.3106214,Proceedings of the 21st International Systems and Software Product Line Conference - Volume A,"The optimization of software to be (more) energy efficient is becoming a major concern for the software industry. Although several techniques have been presented to measure energy consumption for software, none has addressed software product lines (SPLs). Thus, to measure energy consumption of a SPL, the products must be generated and measured individually, which is too costly.In this paper, we present a technique and a prototype tool to statically estimate the worst case energy consumption for SPL. The goal is to provide developers with techniques and tools to reason about the energy consumption of all products in a SPL, without having to produce, run and measure the energy in all of them.Our technique combines static program analysis techniques and worst case execution time prediction with energy consumption analysis. This technique analyzes all products in a feature-sensitive manner, that is, a feature used in several products is analyzed only once, while the energy consumption is estimated once per product.We implemented our technique in a tool called Serapis. We did a preliminary evaluation using a product line for image processing implemented in C. Our experiments considered 7 products from such line and our initial results show that the tool was able to estimate the worst-case energy consumption with a mean error percentage of 9.4% and standard deviation of 6.2% when compared with the energy measured when running the products.","Couto M,Borba P,Cunha J,Fernandes JP,Pereira R,Saraiva J",,2017,2017,https://doi.org/10.1145/3106195.3106214;http://dx.doi.org/10.1145/3106195.3106214,products go green worst case energy consumption in software product lines,1
183,SWEEP: Evaluating Computer System Energy Efficiency Using Synthetic Workloads,"9,78145E+12",,10.1145/1944862.1944886,https://doi.org/10.1145/1944862.1944886;http://dx.doi.org/10.1145/1944862.1944886,Proceedings of the 6th International Conference on High Performance and Embedded Architectures and Compilers,"Energy efficiency is a key design concern in contemporary processor and system design, in the embedded domain as well as in the enterprise domain. The focus on energy efficiency has led to a number of power benchmarking methods recently. For example, EEMBC released EnergyBench and SPEC released SPECpower to quantify a system's energy efficiency; also academics have proposed power benchmarks, such as JouleSort. A major limitation for each of these proposals is that they are tied to a specific benchmark, and hence, they provide limited insight with respect to why one system may be more energy-efficient than another.This paper proposes SWEEP, Synthetic Workloads for Energy Efficiency and Performance evaluation, a framework for generating synthetic workloads with specific behavioral characteristics. We employ SWEEP to generate a wide range of synthetic workloads while varying the instruction mix, ILP, memory access patterns, and I/O-intensiveness; and we use SWEEP to evaluate the energy efficiency of commercial computer systems across the workload space and learn about how the energy efficiency of a computer system is tied to its workload's characteristics.This paper also presents the Energy-Delay Diagram (EDD), a novel method for visualizing energy efficiency. The EDD clearly illustrates the energy versus performance trade-off, and provides more intuitive insight than the traditionally used EDP and ED2P metrics.","Du Bois K,Schaeps T,Polfliet S,Ryckbosch F,Eeckhout L",,2011,2011,https://doi.org/10.1145/1944862.1944886;http://dx.doi.org/10.1145/1944862.1944886,sweep evaluating computer system energy efficiency using synthetic workloads,1
184,SEFLab: A Lab for Measuring Software Energy Footprints,"9,78147E+12",,,,Proceedings of the 2nd International Workshop on Green and Sustainable Software,"Hardware dissipates energy because software tells it to. But attributing hardware energy usage to particular software functions is complicated due to distribution, resource sharing, and layering of software. To enable research on energy usage attribution, we have created the Software Energy Footprint Lab. We explain the experimental setup offered by the lab and the measurement and analysis methodology that it supports. We also describe some preliminary results aimed at deciphering hardware dissipation profiles for various types of servers under various forms of software stress. Finally, we provide an outlook of how energy footprint measurements can contribute to a body of knowledge on software-level energy optimization.","Ferreira MA,Hoekstra E,Merkus B,Visser B,Visser J",,2013,2013,,seflab a lab for measuring software energy footprints,1
185,WattsKit: Software-Defined Power Monitoring of Distributed Systems,"9,78151E+12",,10.1109/CCGRID.2017.27,https://doi.org/10.1109/CCGRID.2017.27;http://dx.doi.org/10.1109/CCGRID.2017.27,"Proceedings of the 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing","The design and the deployment of energy-efficient distributed systems is a challenging task, which requires software engineers to consider all the layers of a system, from hardware to software. In particular, monitoring and analyzing the power consumption of a distributed system spanning several---potentially heterogeneous---nodes becomes particularly tedious when aiming at a finer granularity than observing the power consumption of hosting nodes. While the state-of-the-art in software-defined power meters fails to deliver adaptive solutions to offer such service-level perspective and to cope with the diversity of hardware CPU architectures, this paper proposes to automatically learn the power models of the nodes supporting a distributed system, and then to use these inferred power models to better understand how the power consumption of the system's processes is distributed across nodes at runtime.Our solution, named WattsKit, offers a modular toolkit to build software-defined power meters ""Ã  la carte"", thus dealing with the diversity of user and hardware requirements. Beyond the demonstrated capability of covering a wide diversity of CPU architectures with high accuracy, we illustrate the benefits of adopting software-defined power meters to analyze the power consumption of complex layered and distributed systems. In particular, we illustrate the capability of our approach to monitor the power consumption of a system composed of Docker Swarm, Weave,Elasticsearch, and Apache ZooKeeper. Thanks to WattsKit, developers and administrators are now able to identify potential power leaks in their software infrastructure.","Colmant M,Felber P,Rouvoy R,Seinturier L",,2017,2017,https://doi.org/10.1109/CCGRID.2017.27;http://dx.doi.org/10.1109/CCGRID.2017.27,wattskit software defined power monitoring of distributed systems,1
186,"A Survey of the Potential Long-Term Impacts of AI: How AI Could Lead to Long-Term Changes in Science, Cooperation, Power, Epistemics and Values","9,78145E+12",,10.1145/3514094.3534131,https://doi.org/10.1145/3514094.3534131;http://dx.doi.org/10.1145/3514094.3534131,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society","It is increasingly recognised that advances in artificial intelligence could have large and long-lasting impacts on society. However, what form those impacts will take, just how large and long-lasting they will be, and whether they will ultimately be positive or negative for humanity, is far from clear. Based on surveying literature on the societal impacts of AI, we identify and discuss five potential long-term impacts of AI: how AI could lead to long-term chances in science, cooperation, power, epistemics, and values. We review the state of existing research in each of these areas and highlight priority questions for future research.","Clarke S,Whittlestone J",,2022,2022,https://doi.org/10.1145/3514094.3534131;http://dx.doi.org/10.1145/3514094.3534131,a survey of the potential long term impacts of ai how ai could lead to long term changes in science cooperation power epistemics and values,1
187,Multiple-Swarm Ensembles: Improving the Predictive Power and Robustness of Predictive Models and Its Use in Computational Biology,,1545-5963,10.1109/TCBB.2017.2691329,https://doi.org/10.1109/TCBB.2017.2691329;http://dx.doi.org/10.1109/TCBB.2017.2691329,,"Machine learning is an integral part of computational biology, and has already shown its use in various applications, such as prognostic tests. In the last few years in the non-biological machine learning community, ensembling techniques have shown their power in data mining competitions such as the Netflix challenge; however, such methods have not found wide use in computational biology. In this work, we endeavor to show how ensembling techniques can be applied to practical problems, including problems in the field of bioinformatics, and how they often outperform other machine learning techniques in both predictive power and robustness. Furthermore, we develop a methodology of ensembling, Multi-Swarm Ensemble MSWE by using multiple particle swarm optimizations and demonstrate its ability to further enhance the performance of ensembles.","Alves P,Liu S,Wang D,Gerstein M",IEEE/ACM Trans. Comput. Biol. Bioinformatics,2018,2018-05,https://doi.org/10.1109/TCBB.2017.2691329;http://dx.doi.org/10.1109/TCBB.2017.2691329,multiple swarm ensembles improving the predictive power and robustness of predictive models and its use in computational biology,1
189,ILid: Low-Power Sensing of Fatigue and Drowsiness Measures on a Computational Eyeglass,,,10.1145/3090088,https://doi.org/10.1145/3090088;http://dx.doi.org/10.1145/3090088,,"The ability to monitor eye closures and blink patterns has long been known to enable accurate assessment of fatigue and drowsiness in individuals. Many measures of the eye are known to be correlated with fatigue including coarse-grained measures like the rate of blinks as well as fine-grained measures like the duration of blinks and the extent of eye closures. Despite a plethora of research validating these measures, we lack wearable devices that can continually and reliably monitor them in the natural environment. In this work, we present a low-power system, iLid, that can continually sense fine-grained measures such as blink duration and Percentage of Eye Closures (PERCLOS) at high frame rates of 100fps. We present a complete solution including design of the sensing, signal processing, and machine learning pipeline; implementation on a prototype computational eyeglass platform; and extensive evaluation under many conditions including illumination changes, eyeglass shifts, and mobility. Our results are very encouraging, showing that we can detect blinks, blink duration, eyelid location, and fatigue-related metrics such as PERCLOS with less than a few percent error.","Rostaminia S,Mayberry A,Ganesan D,Marlin B,Gummeson J",Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.,2017,2017-06,https://doi.org/10.1145/3090088;http://dx.doi.org/10.1145/3090088,ilid low power sensing of fatigue and drowsiness measures on a computational eyeglass,1
190,Eco: A Hardware-Software Co-Design for In Situ Power Measurement on Low-End IoT Systems,"9,78145E+12",,10.1145/3362053.3363495,https://doi.org/10.1145/3362053.3363495;http://dx.doi.org/10.1145/3362053.3363495,Proceedings of the 7th International Workshop on Energy Harvesting & Energy-Neutral Sensing Systems,"In this paper, we present Eco, a hardware-software co-design enabling generic energy management on IoT nodes. Eco is tailored to devices with limited resources and thus targets most of the upcoming IoT scenarios. The proposed measurement module combines commodity components with common system interfaces to achieve easy, flexible integration with various hardware platforms and the RIOT IoT operating system. We thoroughly evaluate and compare accuracy and overhead. Our findings indicate that our commodity design competes well with highly optimized solutions, while being significantly more versatile. We employ Eco for energy management on RIOT and validate its readiness for deployment in a five-week field trial integrated with energy harvesting.","Rottleuthner M,Schmidt TC,WÃ¤hlisch M",,2019,2019,https://doi.org/10.1145/3362053.3363495;http://dx.doi.org/10.1145/3362053.3363495,eco a hardware software co design for in situ power measurement on low end iot systems,1
