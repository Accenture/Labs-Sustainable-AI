{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "658755ba",
   "metadata": {},
   "source": [
    "# Load data extraction\n",
    "\n",
    "\n",
    "Data has been extracted from the selected studies. \n",
    "This file permits to load the extracted data, post-process it, and create LaTeX code from it.\n",
    "\n",
    "This notebook is organized as follows:\n",
    "1. In Section 1, we create tables and timelines for studies in the group YY\n",
    "2. In Section 2, we create tables and timelines for studies in the group NY\n",
    "3. In Section 3, we create tables and timelines for studies in the group YN\n",
    "4. In Section 4, we create lists of links\n",
    "5. In Section 5, we create tables for methods and tools not selected in the review process.\n",
    "\n",
    "***\n",
    "\n",
    "**Importing libraries:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba17e781",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba17e781",
   "metadata": {},
   "source": [
    "**Usefull funtions and variables:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b989603a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TABLE_PARAM=''\n",
    "\n",
    "def transpose_table(df):\n",
    "    df = pd.DataFrame(df.values.T[1:], columns=df.id.tolist())\n",
    "    return(df)\n",
    "\n",
    "links_list = []\n",
    "\n",
    "# for LaTeX acronyms:\n",
    "abbrev_list_lower = ['cnn', 'epm', 'flop', 'ict', 'mac', 'nas', 'nlp', 'nn', 'nu', 'nvml', 'pmc', 'ptx', 'rapl', 'slr', 'smc', 'tdp']\n",
    "abbrev_list_upper = [w.upper() for w in abbrev_list_lower]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b989603a",
   "metadata": {},
   "source": [
    "## 1. YY studies\n",
    "\n",
    "### 1.1 Making the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84714a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "_path = os.path.join('data_YY.xlsx')\n",
    "df_YY = pd.DataFrame(pd.read_excel(_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e2f2b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analytical estimation model :\n",
      "\\paragraph{YY studies in the group ``analytical estimation model''} \n",
      " \\label{tab:YY-analytical-estimation-model} \n",
      "\\begin{longtable}{|>{\\raggedright\\arraybackslash}p{2cm}|p{5.9cm}|>{\\raggedright\\arraybackslash}p{1.85cm}|>{\\raggedright\\arraybackslash}p{1.95cm}|>{\\raggedright\\arraybackslash}p{1.5cm}|p{0.75cm}|}\n",
      "\\toprule\n",
      "\\bfseries study & \\bfseries detail & \\bfseries target task & \\bfseries constraints & \\bfseries available & \\bfseries cites \\\\\n",
      "\\midrule \n",
      "\\endhead\n",
      "\\cite{desislavov2023}, 2023, \\acrshort{nu} & Model with \\acrshort{flop} count as input, accounting for computations only, on CPU or GPU & any & no & no & 15 \\\\\n",
      "\\cite{lemaire2022}, 2022, \\acrshort{nu} & Model with \\acrshort{nn} architecture as inputs, uses energy consumption of single operations drawn from the literature, and memory size, accounting for CPU or accelerator & Spiking and non-spiking \\acrshort{nn} inference & no & code and models upon request & 11 \\\\\n",
      "\\cite{lannelongue2021}, 2021, GA & (Green-Algorithms) Model with task duration and hardware utilization as inputs, accounting for CPU, RAM and GPU & any & no & code, API \\href{https://github.com/GreenAlgorithms/green-algorithms-tool}{\\ref*{link-lannelongue2021}} & 165 \\\\\n",
      "\\cite{trebaol2020}, 2020, Cumulator & Model with computing task duration as input, accounting for CPU, RAM and GPU & any & Python & PyPI package, code \\href{https://github.com/epfl-iglobalhealth/cumulator}{\\ref*{link-trebaol2020}} & 5 \\\\\n",
      "\\cite{lacoste2019}, 2019, MLCI & (ML-Co2-Impact) Model with task duration as input, accounting for one GPU & any & no & code, API \\href{https://github.com/mlco2/impact}{\\ref*{link-lacoste2019}} & 442 \\\\\n",
      "\\cite{yang2017}, 2017, D\\acrshort{nn}EET, for normalized energy consumed & (Deep-Neural-Network-Energy-Estimation-Tool) Model with shape of layers, and number of non-zero values and and bitwidths in filters and feature maps as inputs (uses pre-computed dataflows to calculate the number of bits accessed at each memory level), accounting for CPU or Deep \\acrshort{nn} processor (= accelerator) and RAM & \\acrshort{cnn} inference & no & API \\href{https://energyestimation.mit.edu/}{\\ref*{link-yang2017}} & 195 \\\\\n",
      "\\bottomrule\n",
      "\\end{longtable}\n",
      "\n",
      "\n",
      "\n",
      "data-based estimation model :\n",
      "\\paragraph{YY studies in the group ``data-based estimation model''} \n",
      " \\label{tab:YY-data-based-estimation-model} \n",
      "\\begin{longtable}{|>{\\raggedright\\arraybackslash}p{2cm}|p{5.9cm}|>{\\raggedright\\arraybackslash}p{1.85cm}|>{\\raggedright\\arraybackslash}p{1.95cm}|>{\\raggedright\\arraybackslash}p{1.5cm}|p{0.75cm}|}\n",
      "\\toprule\n",
      "\\bfseries study & \\bfseries detail & \\bfseries target task & \\bfseries constraints & \\bfseries available & \\bfseries cites \\\\\n",
      "\\midrule \n",
      "\\endhead\n",
      "\\cite{dariol2023}, 2023, \\acrshort{nu} & Based on SystemC simulation (from the authors), regression model with simulated execution traces as inputs, accounting for CPU and RAM on FPGA/embeded devices & \\acrshort{nn}s deployment & no & no & 0 \\\\\n",
      "\\cite{metz2022b}, 2022, \\acrshort{nu} & Random Forest Tree regression model with \\acrshort{ptx} code and \\acrshort{cnn} characteristics as inputs, accounting for GPU and RAM & \\acrshort{cnn} inference & cuda-based \\acrshort{cnn}, Nvidia GPU & no & 3 \\\\\n",
      "\\cite{metz2022a}, 2022, \\acrshort{nu} & K-Nearest Neighbor regression model with \\acrshort{ptx} code and \\acrshort{cnn} characteristics as inputs, accounting for GPU and RAM & \\acrshort{cnn} inference & cuda-based \\acrshort{cnn}, Nvidia GPU & no & 4 \\\\\n",
      "\\cite{goel2021}, 2021, Energy\\acrshort{nn} & Linear regression model with \\acrshort{mac} count and memory needed as inputs, accounting for Deep Learning Processor Unit (type of \\acrshort{cnn} accelerator) on embedded platforms & \\acrshort{cnn} training and inference & no & no & 2 \\\\\n",
      "\\cite{igescu2021}, 2021, EcoML, based on Cumulator & ML models (Decision Tree, Linear Regression, \\acrshort{nn}, Random Forest) with training dataset of a given ML model as input, accounting for CPU, RAM and GPU & training for fixed set of ML models & Python, Sklearn & PyPI package, code \\href{https://github.com/epfl-iglobalhealth/CS433-2021-ecoML}{\\ref*{link-igescu2021}} & 0 \\\\\n",
      "\\cite{metz2021}, 2021, \\acrshort{nu} & ML model (\\acrshort{nn}) with GPGPU architecture and \\acrshort{ptx} code as inputs, accounting for GPU & \\acrshort{cnn} inference & cuda-based \\acrshort{cnn}, Nvidia GPU & no & 4 \\\\\n",
      "\\cite{rodrigues2018}, 2018, SyNERGY & ML model (Linear Regression) with \\acrshort{mac} count as input, accounting for CPU, RAM and peripherals of Jetson TX1 board & \\acrshort{cnn} inference & Jetson TX1 board & (part of) code \\href{https://github.com/Crefeda/SyNERGY }{\\ref*{link-rodrigues2018}} & 41 \\\\\n",
      "\\cite{cai2017}, 2017, NeuralP & (Neuralpower) Sparse polynomial regression model with \\acrshort{cnn} architecture and target platform as inputs, accounting for GPU & \\acrshort{cnn} inference & no & code, model \\href{https://github.com/enyac-group/NeuralPower }{\\ref*{link-cai2017}} & 146 \\\\\n",
      "\\bottomrule\n",
      "\\end{longtable}\n",
      "\n",
      "\n",
      "\n",
      "on-chip sensors :\n",
      "\\paragraph{YY studies in the group ``on-chip sensors''} \n",
      " \\label{tab:YY-on-chip-sensors} \n",
      "\\begin{longtable}{|>{\\raggedright\\arraybackslash}p{2cm}|p{5.9cm}|>{\\raggedright\\arraybackslash}p{1.85cm}|>{\\raggedright\\arraybackslash}p{1.95cm}|>{\\raggedright\\arraybackslash}p{1.5cm}|p{0.75cm}|}\n",
      "\\toprule\n",
      "\\bfseries study & \\bfseries detail & \\bfseries target task & \\bfseries constraints & \\bfseries available & \\bfseries cites \\\\\n",
      "\\midrule \n",
      "\\endhead\n",
      "\\cite{you2023}, 2023, ZeusM & (Zeus-Monitor) \\acrshort{nvml}; is part of the Zeus framework, accounting for GPU & any & Python, Nvidia GPU & PyPI package, code \\href{https://github.com/SymbioticLab/Zeus }{\\ref*{link-you2023}} & 18 \\\\\n",
      "\\cite{carastan-santos2022}, 2022, BT, based on Cumulator & (Benchmark-Tracker) Based on Experiment-Impact-Tracker and AI Benchmark Alpha, accounting for process-level, for CPU, RAM and GPU & training, inference tasks from AI Benchmark Alpha\\footnote{AI Benchmark Alpha is an open source library for evaluating AI performance of various hardware platforms, it contains training and inference scripts for various ML models.} & Linux OS, Intel CPU, Nvidia GPU, Python & PyPi package, code \\href{https://github.com/phamthi1812/Benchmark-Tracker}{\\ref*{link-carastan-santos2022}} & 1 \\\\\n",
      "\\cite{anthony2020}, 2020, CT & (Carbon-Tracker) \\acrshort{rapl}, \\acrshort{nvml}, accounting for CPU, RAM and GPU & ML training & Linux OS, Intel CPU, Nvidia GPU, Python & PyPI package, code \\href{https://github.com/lfwa/carbontracker }{\\ref*{link-anthony2020}} & 254 \\\\\n",
      "\\cite{henderson2020}, 2020, EIT & (Experiment-Impact-Tracker) \\acrshort{rapl}, \\acrshort{nvml}, accounting for process-level, for CPU, RAM and GPU & any & Linux OS, Intel CPU, Nvidia GPU, Python & PyPI package, code \\href{https://github.com/Breakend/experiment-impact-tracker}{\\ref*{link-henderson2020}} & 324 \\\\\n",
      "\\cite{lottick2019}, 2019, CC, previously Energy-Usage & (Code-Carbon) \\acrshort{rapl}, \\acrshort{nvml}, accounting for CPU, RAM and GPU & any & Linux OS, Intel CPU, Nvidia GPU, Python & PyPI packages, codes \\href{https://github.com/responsibleproblemsolving/energy-usage}{\\ref*{link-lottick2019}} \\href{https://github.com/mlco2/codecarbon}{\\ref*{link-lottick2019-bis}} & 44 \\\\\n",
      "\\bottomrule\n",
      "\\end{longtable}\n",
      "\n",
      "\n",
      "\n",
      "analytical and data-based estimation model :\n",
      "\\paragraph{YY studies in the group ``analytical and data-based estimation model''} \n",
      " \\label{tab:YY-analytical-and-data-based-estimation-model} \n",
      "\\begin{longtable}{|>{\\raggedright\\arraybackslash}p{2cm}|p{5.9cm}|>{\\raggedright\\arraybackslash}p{1.85cm}|>{\\raggedright\\arraybackslash}p{1.95cm}|>{\\raggedright\\arraybackslash}p{1.5cm}|p{0.75cm}|}\n",
      "\\toprule\n",
      "\\bfseries study & \\bfseries detail & \\bfseries target task & \\bfseries constraints & \\bfseries available & \\bfseries cites \\\\\n",
      "\\midrule \n",
      "\\endhead\n",
      "\\cite{getzner2023}, 2023, DLEE & (dl-energy-estimator) Linear and polynomial regression models with \\acrshort{nn} architecture and \\acrshort{mac} count as inputs (layer-wise consumption), accounting for CPU and RAM & Deep \\acrshort{nn} inference & no & code (data collection, training), model \\href{https://github.com/JohannesGetzner/dl-energy-estimator}{\\ref*{link-getzner2023}} & 1 \\\\\n",
      "\\cite{lahmer2022}, 2022, \\acrshort{nu} & Model with \\acrshort{mac} count and platform-specific parameters (parameters obtained empirically from data) as inputs, accounting for CPU and RAM (unclear if GPU power is accounted for) & Deep \\acrshort{nn} inference (fully connected, convolutional) & Nvidia Jetson edge computer & no & 7 \\\\\n",
      "\\bottomrule\n",
      "\\end{longtable}\n",
      "\n",
      "\n",
      "\n",
      "analytical estimation model and on-chip sensors :\n",
      "\\paragraph{YY studies in the group ``analytical estimation model and on-chip sensors''} \n",
      " \\label{tab:YY-analytical-estimation-model-and-on-chip-sensors} \n",
      "\\begin{longtable}{|>{\\raggedright\\arraybackslash}p{2cm}|p{5.9cm}|>{\\raggedright\\arraybackslash}p{1.85cm}|>{\\raggedright\\arraybackslash}p{1.95cm}|>{\\raggedright\\arraybackslash}p{1.5cm}|p{0.75cm}|}\n",
      "\\toprule\n",
      "\\bfseries study & \\bfseries detail & \\bfseries target task & \\bfseries constraints & \\bfseries available & \\bfseries cites \\\\\n",
      "\\midrule \n",
      "\\endhead\n",
      "\\cite{qiu2023}, 2023, \\acrshort{nu} & \\acrshort{rapl}, \\acrshort{nvml}, and model with download/upload speed, ML model size and router power as inputs (for communications), accounting for CPU, RAM, GPU and Wide Area Networking & Federated Learning training & no & no & 42 \\\\\n",
      "\\cite{budennyy2022}, 2022, Eco2AI & Model with hardware utilization as inputs for the CPU and RAM, \\acrshort{nvml} for the GPU, accounting for process-level, for CPU, RAM and GPUs (of the same type) & any & Nvidia GPU, Python & PyPI package, code \\href{https://github.com/sb-ai-lab/Eco2AI }{\\ref*{link-budennyy2022}} & 35 \\\\\n",
      "\\bottomrule\n",
      "\\end{longtable}\n",
      "\n",
      "\n",
      "\n",
      "other :\n",
      "\\paragraph{YY studies without group} \n",
      " \\label{tab:YY-other} \n",
      "\\begin{longtable}{|>{\\raggedright\\arraybackslash}p{2cm}|p{5.9cm}|>{\\raggedright\\arraybackslash}p{1.85cm}|>{\\raggedright\\arraybackslash}p{1.95cm}|>{\\raggedright\\arraybackslash}p{1.5cm}|p{0.75cm}|}\n",
      "\\toprule\n",
      "\\bfseries study & \\bfseries detail & \\bfseries target task & \\bfseries constraints & \\bfseries available & \\bfseries cites \\\\\n",
      "\\midrule \n",
      "\\endhead\n",
      "\\cite{ortega2023}, 2023, \\acrshort{nu} & Design method of an interface for external power meters, accounting for whole system & any & no & no & 1 \\\\\n",
      "\\cite{montgomerie-corcoran2021}, 2021, Pommel & Based on Ramulator, Cacti-io and DRAMPower, with memory access traces (from Ramulator) and accelerator specifications as inputs, accounting for off-chip memory on \\acrshort{cnn}s accelerator & \\acrshort{cnn} inference & \\acrshort{cnn} accelerator & code \\href{https://github.com/AlexMontgomerie/pommel}{\\ref*{link-montgomerie-corcoran2021}} & 0 \\\\\n",
      "\\cite{wang2021}, 2021, Accelergy & (Gem5-Accelergy-system) Based on MacPAT (simulator) and Timeloop tools, accounting for CPU, RAM, accelerators and data transfer between them & any (tested on Deep \\acrshort{nn} inference) & no & code \\href{https://github.com/Accelergy-Project/accelergy}{\\ref*{link-wang2021}} & 4 \\\\\n",
      "\\bottomrule\n",
      "\\end{longtable}\n",
      "\n",
      "\n",
      "\n",
      "\\ref{tab:YY-analytical-estimation-model} for the group `analytical estimation model', \\ref{tab:YY-data-based-estimation-model} for the group `data-based estimation model', \\ref{tab:YY-on-chip-sensors} for the group `on-chip sensors', \\ref{tab:YY-analytical-and-data-based-estimation-model} for the group `analytical and data-based estimation model', \\ref{tab:YY-analytical-estimation-model-and-on-chip-sensors} for the group `analytical estimation model and on-chip sensors', \\ref{tab:YY-other} for studies without group.%\n"
     ]
    }
   ],
   "source": [
    "# transpose the dataframe:\n",
    "df_YY = transpose_table(df_YY)\n",
    "\n",
    "df_YY = df_YY.sort_values('year', ascending=False)\n",
    "df_YY['name'] = df_YY['name'].replace(['none'], 'NU')\n",
    "df_YY['constraints'] = df_YY['constraints'].replace(['none'], 'no')\n",
    "df_YY['available'] = df_YY['available'].replace(['none'], 'no')\n",
    "df_YY['general'] = df_YY['fulltext_id'].apply(lambda x: '\\cite{' + str(x) + '}').astype(str) + ', ' + df_YY['year'].astype(str) + ', ' + df_YY['name'].astype(str)\n",
    "\n",
    "df_YY.loc[df_YY['available_link'].str.contains('http'), 'available'] = df_YY['available'].astype(str) + ' \\\\href{' + df_YY['available_link'].astype(str) + '}{\\\\ref*{link-' + df_YY['fulltext_id'].astype(str) + '}}' \n",
    "df_YY.loc[df_YY['available_link_bis'].str.contains('http'), 'available'] =  df_YY['available'].astype(str) + ' \\\\href{' + df_YY['available_link_bis'].astype(str) + '}{\\\\ref*{link-' + df_YY['fulltext_id'].astype(str) + '-bis}}'\n",
    "\n",
    "df_YY.loc[df_YY['available_link'].str.contains('http'), 'for_links_list'] = '\\\\item \\\\label{link-' + df_YY['fulltext_id'].astype(str) + '} \\\\url{' + df_YY['available_link'].astype(str) + '}'\n",
    "df_YY.loc[df_YY['available_link_bis'].str.contains('http'), 'for_links_list_bis'] = '\\\\item \\\\label{link-' + df_YY['fulltext_id'].astype(str) + '-bis} \\\\url{' + df_YY['available_link_bis'].astype(str) + '}'\n",
    "\n",
    "df_YY['for_timeline'] = df_YY['name'].astype(str) + ' ' + df_YY['fulltext_id'].apply(lambda x: '\\cite{' + str(x) + '}').astype(str)\n",
    "\n",
    "df_YY['detail_1'] = df_YY['detail_1'].astype(str) + ', accounting for ' + df_YY['detail_2'].astype(str)\n",
    "\n",
    "df_YY.loc[df_YY['note'].notna(), 'detail_1'] = df_YY['note'].astype(str) + ' ' + df_YY['detail_1'].astype(str)\n",
    "df_YY.loc[df_YY['note_2'].notna(), 'general'] = df_YY['general'].astype(str) + ', ' + df_YY['note_2'].astype(str)\n",
    "\n",
    "columns_new_names = {'general': 'study', 'detail_1': 'detail', 'citations':'cites', 'constraints':'constraints', 'task':'target task', 'available':'available'}\n",
    "\n",
    "for key in columns_new_names:\n",
    "    columns_new_names[key] = '\\\\bfseries ' + columns_new_names[key]\n",
    "\n",
    "def create_latex_YY(df_YY, category, disp):\n",
    "    list_cols = ['general', 'detail_1', 'task', 'constraints', 'available', 'citations']\n",
    "    a = '>{\\\\raggedright\\\\arraybackslash}'\n",
    "    latex_col_width = '|'+a+'p{2cm}|p{5.9cm}|'+a+'p{1.85cm}|'+a+'p{1.95cm}|'+a+'p{1.5cm}|p{0.75cm}|'\n",
    "\n",
    "    df_YY_category = df_YY[df_YY.category == category][list_cols]\n",
    "\n",
    "    if disp:\n",
    "        display(df_YY_category)\n",
    "    df_YY_category = df_YY_category.rename(columns=columns_new_names)\n",
    "    latex_YY = df_YY_category.to_latex(index=False).replace('llllll', latex_col_width)\n",
    "    # latex_YY = latex_YY.replace('\\\\\\\\\\n\\\\cite', '\\\\vspace{2mm}\\\\\\\\\\\\\\n\\\\cite')\n",
    "\n",
    "    if group == 'other':\n",
    "        caption='YY studies without group'\n",
    "    else:\n",
    "        caption=\"YY studies in the group ``\" + category + \"''\"\n",
    "    lab='YY-'+'-'.join(category.split())\n",
    "\n",
    "    # for the version with tables:\n",
    "    # latex_YY = '\\\\begin{table}['+ TABLE_PARAM +'] \\n\\\\begin{minipage}{\\\\textwidth} \\n\\\\centering \\n' + latex_YY + '\\\\caption{'+ caption +'} \\n\\\\label{tab:' + lab + '} \\n\\\\end{minipage} \\n\\\\end{table}'\n",
    "    \n",
    "    # for the version with paragraphs:\n",
    "    latex_YY = '\\\\paragraph{'+caption+ '} \\n \\\\label{tab:'+lab+'} \\n' + latex_YY\n",
    "    latex_YY = latex_YY.replace('{tabular}', '{longtable}')\n",
    "    latex_YY = latex_YY.replace('\\\\midrule', '\\\\midrule \\n\\\\endhead')\n",
    "\n",
    "    for w in abbrev_list_upper:\n",
    "        latex_YY = latex_YY.replace(w, '\\\\acrshort{'+w.lower()+'}')\n",
    "\n",
    "    file_name = '-'.join(category.split())\n",
    "    with open(os.path.join('tables', 'YY-' + file_name + '.tex'), 'w') as f:\n",
    "        f.write(latex_YY)\n",
    "\n",
    "    return(latex_YY)\n",
    "\n",
    "\n",
    "groups = ['analytical estimation model', 'data-based estimation model', 'on-chip sensors',\\\n",
    "        'analytical and data-based estimation model','analytical estimation model and on-chip sensors',\\\n",
    "        'other']\n",
    "description=[]\n",
    "for group in groups:\n",
    "    print(group, ':')\n",
    "    latex_YY = create_latex_YY(df_YY, group, disp=False)\n",
    "    print(latex_YY)\n",
    "    print('\\n')\n",
    "\n",
    "    if group != 'other':\n",
    "        description.append(\"\\\\ref{tab:YY-\" + '-'.join(group.split()) + \"} for the group `\" + group + \"'\")\n",
    "    else:\n",
    "        description.append(\"\\\\ref{tab:YY-\" + '-'.join(group.split()) + '} for studies without group')\n",
    "description = ', '.join(description)\n",
    "description += '.%'\n",
    "print(description)\n",
    "\n",
    "file_name = 'YY-description'\n",
    "with open(os.path.join('tables', file_name + '.tex'), 'w') as f:\n",
    "    f.write(description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145fb3d6",
   "metadata": {},
   "source": [
    "### 1.2. Timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bd07cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df_YY.groupby('year')['for_timeline'].apply(list)\n",
    "dict_timeline = grouped.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffb952fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2017: ['DNNEET \\\\cite{yang2017}', 'NeuralP \\\\cite{cai2017}'], 2018: ['SyNERGY \\\\cite{rodrigues2018}'], 2019: ['CC \\\\cite{lottick2019}', 'MLCI \\\\cite{lacoste2019}'], 2020: ['CT \\\\cite{anthony2020}', 'Cumulator \\\\cite{trebaol2020}', 'EIT \\\\cite{henderson2020}'], 2021: ['Accelergy \\\\cite{wang2021}', 'EcoML \\\\cite{igescu2021}', 'EnergyNN \\\\cite{goel2021}', 'GA \\\\cite{lannelongue2021}', 'NU \\\\cite{metz2021}', 'Pommel \\\\cite{montgomerie-corcoran2021}'], 2022: ['BT \\\\cite{carastan-santos2022}', 'Eco2AI \\\\cite{budennyy2022}', 'NU \\\\cite{lemaire2022} \\\\cite{metz2022b} \\\\cite{metz2022a} \\\\cite{lahmer2022}'], 2023: ['DLEE \\\\cite{getzner2023}', 'NU \\\\cite{desislavov2023} \\\\cite{qiu2023} \\\\cite{dariol2023} \\\\cite{ortega2023}', 'ZeusM \\\\cite{you2023}']}\n"
     ]
    }
   ],
   "source": [
    "def post_process_timeline(dict_timeline):\n",
    "    d = {}\n",
    "    res = {}\n",
    "    for key,value in dict_timeline.items():\n",
    "        d[key]={}\n",
    "        \n",
    "        for v in value:\n",
    "            if v.split()[0] not in d[key]:\n",
    "                d[key][v.split()[0]] = []\n",
    "            d[key][v.split()[0]].append(v.split()[1])\n",
    "        \n",
    "            \n",
    "    for key,value in d.items():\n",
    "        res[key]=[]\n",
    "        \n",
    "        #order values by keys\n",
    "        value = collections.OrderedDict(sorted(value.items()))\n",
    "\n",
    "        for k,v in value.items():\n",
    "\n",
    "            res[key].append(k + ' ' + ' '.join(v))\n",
    "    print(res)\n",
    "\n",
    "    return(res)\n",
    "\n",
    "dict_timeline = post_process_timeline(dict_timeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4befa98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tikzpicture} \n",
      "\\draw[->, thick] (0,0) -- (15,0); \n",
      "\\foreach \\x/\\year in {0/2017, 2/2018, 4/2019, 6/2020, 8/2021, 10/2022, 12/2023} { \n",
      "    \\draw[shift={(\\x,0)}, color=black] (0pt,2pt) -- (0pt,-2pt); \n",
      "    \\node[below] at (\\x, 0) {\\year}; \n",
      "    } \n",
      "\\node[above right, align=left, rotate=45] at (0,0) {DNNEET \\cite{yang2017}\\\\ NeuralP \\cite{cai2017}}; \n",
      "\\node[above right, align=left, rotate=45] at (2,0) {SyNERGY \\cite{rodrigues2018}}; \n",
      "\\node[above right, align=left, rotate=45] at (4,0) {CC \\cite{lottick2019}\\\\ MLCI \\cite{lacoste2019}}; \n",
      "\\node[above right, align=left, rotate=45] at (6,0) {CT \\cite{anthony2020}, EIT \\cite{henderson2020}\\\\ Cumulator \\cite{trebaol2020}}; \n",
      "\\node[above right, align=left, rotate=45] at (8.3,0) {Accelergy \\cite{wang2021}, EcoML \\cite{igescu2021}\\\\ EnergyNN \\cite{goel2021}, GA \\cite{lannelongue2021}\\\\ NU \\cite{metz2021}, Pommel \\cite{montgomerie-corcoran2021}}; \n",
      "\\node[above right, align=left, rotate=45] at (10,0) {BT \\cite{carastan-santos2022}, Eco2AI \\cite{budennyy2022}\\\\ NU \\cite{lemaire2022} \\cite{metz2022b} \\cite{metz2022a} \\cite{lahmer2022}}; \n",
      "\\node[above right, align=left, rotate=45] at (12,0) {NU \\cite{desislavov2023} \\cite{qiu2023} \\cite{dariol2023} \\cite{ortega2023}\\\\ DLEE \\cite{getzner2023}, ZeusM \\cite{you2023}}; \n",
      "\\end{tikzpicture}\n"
     ]
    }
   ],
   "source": [
    "dict_year_point = {}\n",
    "cpt = 0\n",
    "step = 2\n",
    "for key in dict_timeline.keys():\n",
    "    dict_year_point[key] = str(cpt)\n",
    "    cpt += step\n",
    "\n",
    "a = ''\n",
    "for year, studies in dict_timeline.items():\n",
    "    if year == 2020:\n",
    "        aa = studies[1]\n",
    "        ab = studies[0]+', '+ studies[2]\n",
    "        ac = ab + '\\\\\\\\ ' + aa\n",
    "    elif year == 2021:\n",
    "        aa = ', '.join(studies[:len(studies)//3])\n",
    "        ab = ', '.join(studies[len(studies)//3:2*len(studies)//3])\n",
    "        abb = ', '.join(studies[2*len(studies)//3:])\n",
    "        ac = aa + '\\\\\\\\ ' + ab + '\\\\\\\\ ' + abb  \n",
    "    elif year == 2022:\n",
    "        aa = studies[0]+', '+ studies[1]\n",
    "        ab = studies[2]\n",
    "        ac = aa + '\\\\\\\\ ' + ab\n",
    "    elif year == 2023:\n",
    "        aa = studies[1]\n",
    "        ab = studies[0]+', '+ studies[2]\n",
    "        ac = aa + '\\\\\\\\ ' + ab\n",
    "    elif len(studies)>=2:\n",
    "        aa = ', '.join(studies[:len(studies)//2])\n",
    "        ab = ', '.join(studies[len(studies)//2:])\n",
    "        ac = aa + '\\\\\\\\ ' + ab \n",
    "    else:\n",
    "        ac = ', '.join(studies)\n",
    "    if year == 2021:\n",
    "        ad = '\\\\node[above right, align=left, rotate=45] at (8.3,0) {' + ac + '}; \\n'\n",
    "    else:\n",
    "        ad = '\\\\node[above right, align=left, rotate=45] at (' + dict_year_point[year] + ',0) {' + ac + '}; \\n'\n",
    "    a += ad\n",
    "\n",
    "ba = ', '.join([f\"{v}/{k}\" for k, v in dict_year_point.items()])\n",
    "\n",
    "ba + '\\n'\n",
    "\n",
    "b = '\\\\begin{tikzpicture} \\n\\\n",
    "\\\\draw[->, thick] (0,0) -- (15,0); \\n\\\n",
    "\\\\foreach \\\\x/\\\\year in {' + ba + '} { \\n\\\n",
    "    \\\\draw[shift={(\\\\x,0)}, color=black] (0pt,2pt) -- (0pt,-2pt); \\n\\\n",
    "    \\\\node[below] at (\\\\x, 0) {\\\\year}; \\n\\\n",
    "    } \\n'\n",
    "\n",
    "c = '\\\\end{tikzpicture}'\n",
    "\n",
    "\n",
    "d = b + a + c\n",
    "\n",
    "print(d)\n",
    "\n",
    "with open(os.path.join('tables', 'YY-timeline.tex'), 'w') as f:\n",
    "    f.write(d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5cf185",
   "metadata": {},
   "source": [
    "## 2. NY studies\n",
    "\n",
    "### 2.1. Making the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15c881ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "measurement :\n",
      "\\paragraph{NY studies in the group ``measurement''} \n",
      " \\label{tab:NY-measurement} \n",
      "\\begin{longtable}{|>{\\raggedright\\arraybackslash}p{0.85cm}|>{\\raggedright\\arraybackslash}p{4cm}|>{\\raggedright\\arraybackslash}p{6cm}|>{\\raggedright\\arraybackslash}p{3.75cm}|}\n",
      "\\toprule\n",
      "\\bfseries study & \\bfseries detail & \\bfseries ML task & \\bfseries setup \\\\\n",
      "\\midrule \n",
      "\\endhead\n",
      "\\cite{hauschild2023}, 2023 & \\acrshort{epm} JT-TC66C & inference with \\acrshort{cnn}s: MobileNetV2, \\acrshort{nas}NetMobile, ResNet (50, 101), VGG (16, 19) & edge, server \\\\\n",
      "\\cite{trihinas2022}, 2022 & \\acrshort{epm} & inference with \\acrshort{cnn} for object detection (on ImageNet) & edge  \\\\\n",
      "\\cite{machado2022}, 2022 & \\acrshort{epm}/sensors on the board & inference with YOLOv5 for object classification & edge (Nvidia Jetson Nano board) \\\\\n",
      "\\cite{hampau2022}, 2022 & \\acrshort{epm} Monsoon's High Voltage Power Monitor & inference with Image Classifyer (on MNIST, Emotion, CIFAR10) and YOLO & edge \\\\\n",
      "\\cite{hesse2021}, 2021 & \\acrshort{epm} Voltcraft Energy Logger EL4000 (also on-chip sensors if available) & training and inference with \\acrshort{cnn}s, including inference with YOLOv3 & FGPA, Apple M1, classical CPU-GPU \\\\\n",
      "\\cite{wang2019}, 2019 & \\acrshort{epm} Monsoon Power Monitor (for mobile) & inference with YOLOv3 & edge server, smartphone \\\\\n",
      "\\cite{mcintosh2019}, 2019 & \\acrshort{epm} INA219 current sensor (within GreenMiner framework) & training and inference with various models (e.g., J48 Decision Tree, ZeroR) & Raspberry Pi for data-collection, Smartphone \\\\\n",
      "\\cite{rungsuptaweekoon2017}, 2017 & \\acrshort{epm} & inference with YOLO & edge (Nvidia Jetson TX1, TX2) \\\\\n",
      "\\bottomrule\n",
      "\\end{longtable}\n",
      "\n",
      "\n",
      "\n",
      "analytical estimation model :\n",
      "\\paragraph{NY studies in the group ``analytical estimation model''} \n",
      " \\label{tab:NY-analytical-estimation-model} \n",
      "\\begin{longtable}{|>{\\raggedright\\arraybackslash}p{0.85cm}|>{\\raggedright\\arraybackslash}p{4cm}|>{\\raggedright\\arraybackslash}p{6cm}|>{\\raggedright\\arraybackslash}p{3.75cm}|}\n",
      "\\toprule\n",
      "\\bfseries study & \\bfseries detail & \\bfseries ML task & \\bfseries setup \\\\\n",
      "\\midrule \n",
      "\\endhead\n",
      "\\cite{wu2023}, 2023 & power directly approximated by \\acrshort{tdp} & training Logistic Regression and \\acrshort{cnn} with different algorithms & Intel Xeon Gold 6126 CPU, Nvidia A100 \\\\\n",
      "\\cite{arnautovic2021}, 2021 & power approximated by calculation of maximum power of the board/chip & inference with MobileNetSSDv2  & edge \\\\\n",
      "\\cite{desislavov2021}, 2021 & based on \\acrshort{flop} count and hardware specifications & inference with Computer Vision and \\acrshort{nlp} models & GPUs V100, A100, T4 \\\\\n",
      "\\cite{canilang2021}, 2021 & Tool of \\cite{yang2017} & inference with Deep \\acrshort{nn} model for face detection & edge \\\\\n",
      "\\bottomrule\n",
      "\\end{longtable}\n",
      "\n",
      "\n",
      "\n",
      "on-chip sensors :\n",
      "\\paragraph{NY studies in the group ``on-chip sensors''} \n",
      " \\label{tab:NY-on-chip-sensors} \n",
      "\\begin{longtable}{|>{\\raggedright\\arraybackslash}p{0.85cm}|>{\\raggedright\\arraybackslash}p{4cm}|>{\\raggedright\\arraybackslash}p{6cm}|>{\\raggedright\\arraybackslash}p{3.75cm}|}\n",
      "\\toprule\n",
      "\\bfseries study & \\bfseries detail & \\bfseries ML task & \\bfseries setup \\\\\n",
      "\\midrule \n",
      "\\endhead\n",
      "\\cite{caspart2023}, 2023 & internal power sensors of the HoreKa nodes with slurm plugin, \\acrshort{nvml} & training and inference with \\acrshort{cnn} and Long-Short Term Memory (time series) models & supercomputing system with Intel Xeon Platinum 8368 CPU, Nvidia A100-40 GPUs \\\\\n",
      "\\cite{escribano2023}, 2023 & Code-Carbon & inference with \\acrshort{nlp} model T5-small & application wrapped in Docker and deployed in cloud \\\\\n",
      "\\cite{tekin2023}, 2023 & Intel-Power-Gadget & traning and inference with ML models (e.g., Logistic Regression, \\acrshort{nn}) & edge, IoT, cloud with CPU \\\\\n",
      "\\cite{naidu2021}, 2021 & Code-Carbon & training (with differential privacy) Bert, Image Classifier, and Reinforcement Learning model for cartpole control & unknown \\\\\n",
      "\\cite{sun2021}, 2021 & \\acrshort{rapl}, \\acrshort{nvml} & training LeNet, GoogLeNet, AlexNet, CaffeNet, AlexNet-MNIST & CPU and CPU-GPU platforms (Intel Xeon X5-2650 v3, Nvidia Tesla K80) \\\\\n",
      "\\cite{jurj2020}, 2020 & powerstat, tegrastats & tranining of VGG-19, InceptionV3, ResNet-50, MobileNetV2 for image classification, inference with MobileNetV2 & edge (Nvidia Jetson TX2), laptop (Nvidia GTX 1060) \\\\\n",
      "\\cite{holly2020}, 2020 & based on measurements: sensors on the Nvidia Jetson Nano board & inference with MobileNet (V1, V2) and ResNet (18, 50) & Nvidia Jetson Nano \\\\\n",
      "\\cite{yao2021}, 2020 & \\acrshort{nvml} & inference with \\acrshort{cnn}s: VGG-16, ResNet-50, Inception-v3 & high-performance GPUs: M40, P4, V100 \\\\\n",
      "\\cite{strubell2019}, 2019 & \\acrshort{rapl}, \\acrshort{nvml} & training \\acrshort{nlp} models (Transformer T2T, ELMo, BERT, \\acrshort{nas}, GPT-2) & accelerators P100, V100, TPUv2, TPUv3 \\\\\n",
      "\\cite{li2016a}, 2016 & \\acrshort{rapl}, \\acrshort{nvml} & training and inference with \\acrshort{cnn}s: AlexNet v2, OverFeat, VGG-A, and GoogleNet (on ImageNet) & Xeon CPU, K20 GPU, Titan X GPU \\\\\n",
      "\\bottomrule\n",
      "\\end{longtable}\n",
      "\n",
      "\n",
      "\n",
      "data-based estimation model :\n",
      "\\paragraph{NY studies in the group ``data-based estimation model''} \n",
      " \\label{tab:NY-data-based-estimation-model} \n",
      "\\begin{longtable}{|>{\\raggedright\\arraybackslash}p{0.85cm}|>{\\raggedright\\arraybackslash}p{4cm}|>{\\raggedright\\arraybackslash}p{6cm}|>{\\raggedright\\arraybackslash}p{3.75cm}|}\n",
      "\\toprule\n",
      "\\bfseries study & \\bfseries detail & \\bfseries ML task & \\bfseries setup \\\\\n",
      "\\midrule \n",
      "\\endhead\n",
      "\\cite{rodrigues2020}, 2020 & SyNERGY & inference with \\acrshort{cnn}s (e.g., GoogleNet, ResRet50, MobileNet) & Jetson TX1 \\\\\n",
      "\\bottomrule\n",
      "\\end{longtable}\n",
      "\n",
      "\n",
      "\n",
      "other :\n",
      "\\paragraph{NY studies without group} \n",
      " \\label{tab:NY-other} \n",
      "\\begin{longtable}{|>{\\raggedright\\arraybackslash}p{0.85cm}|>{\\raggedright\\arraybackslash}p{4cm}|>{\\raggedright\\arraybackslash}p{6cm}|>{\\raggedright\\arraybackslash}p{3.75cm}|}\n",
      "\\toprule\n",
      "\\bfseries study & \\bfseries detail & \\bfseries ML task & \\bfseries setup \\\\\n",
      "\\midrule \n",
      "\\endhead\n",
      "\\cite{islam2023}, 2023 & unknown & inference with frequently used ML models (e.g., Logistic Regression, Multi-Layer Perceptron) & unknown \\\\\n",
      "\\cite{guo2021}, 2021 & hardware simulation: Design Compiler simulation and CACTI 6.5 & training Binary \\acrshort{nn} (on ImageNet): Boolnet, ReActNet, Bi-RealNet, XNOR-Net, BaseNet & 5 accelerators designed in RTL language \\\\\n",
      "\\bottomrule\n",
      "\\end{longtable}\n",
      "\n",
      "\n",
      "\n",
      "\\ref{tab:NY-measurement} for the group `measurement', \\ref{tab:NY-analytical-estimation-model} for the group `analytical estimation model', \\ref{tab:NY-on-chip-sensors} for the group `on-chip sensors', \\ref{tab:NY-data-based-estimation-model} for the group `data-based estimation model', \\ref{tab:NY-other} for studies without group.%\n"
     ]
    }
   ],
   "source": [
    "_path = os.path.join('data_NY.xlsx')\n",
    "df_NY = pd.DataFrame(pd.read_excel(_path))\n",
    "\n",
    "\n",
    "# transpose the dataframe:\n",
    "df_NY = transpose_table(df_NY)\n",
    "df_NY\n",
    "\n",
    "df_NY = df_NY.sort_values('year', ascending=False)\n",
    "df_NY['general'] = df_NY['fulltext_id'].apply(lambda x: '\\cite{' + str(x) + '}').astype(str) + ', ' + df_NY['year'].astype(str)\n",
    "df_NY['for_timeline'] = df_NY['fulltext_id'].apply(lambda x: '\\cite{' + str(x) + '}').astype(str)\n",
    "\n",
    "columns_new_names = {'general': 'study', 'detail': 'detail', 'task':'ML task', 'setup':'setup'}\n",
    "for key in columns_new_names:\n",
    "    columns_new_names[key] = '\\\\bfseries ' + columns_new_names[key]\n",
    "\n",
    "def create_latex_NY(df_NY, category, disp):\n",
    "    list_cols = ['general', 'detail', 'task', 'setup']\n",
    "    a = '>{\\\\raggedright\\\\arraybackslash}'\n",
    "    latex_col_width = '|'+a+'p{0.85cm}|'+a+'p{4cm}|'+a+'p{6cm}|'+a+'p{3.75cm}|'\n",
    "\n",
    "    df_NY_category = df_NY[df_NY.category == category][list_cols]\n",
    "\n",
    "    if disp:\n",
    "        display(df_NY_category)\n",
    "    df_NY_category = df_NY_category.rename(columns=columns_new_names)\n",
    "    latex_NY = df_NY_category.to_latex(index=False).replace('llll', latex_col_width)\n",
    "\n",
    "    if group == 'other':\n",
    "        caption='NY studies without group'\n",
    "    else:\n",
    "        caption=\"NY studies in the group ``\" + category + \"''\"\n",
    "    lab='NY-'+'-'.join(category.split())\n",
    "\n",
    "    # for the version with paragraphs:\n",
    "    latex_NY = '\\\\paragraph{'+caption+ '} \\n \\\\label{tab:'+lab+'} \\n' + latex_NY\n",
    "    latex_NY= latex_NY.replace('{tabular}', '{longtable}')\n",
    "    latex_NY = latex_NY.replace('\\\\midrule', '\\\\midrule \\n\\\\endhead')\n",
    "\n",
    "    for w in abbrev_list_upper:\n",
    "        latex_NY = latex_NY.replace(w, '\\\\acrshort{'+w.lower()+'}')\n",
    "\n",
    "    file_name = '-'.join(category.split())\n",
    "    with open(os.path.join('tables', 'NY-' + file_name + '.tex'), 'w') as f:\n",
    "        f.write(latex_NY)\n",
    "\n",
    "    return(latex_NY)\n",
    "\n",
    "groups = ['measurement', 'analytical estimation model', 'on-chip sensors', 'data-based estimation model',\\\n",
    "          'other']\n",
    "description=[]\n",
    "for group in groups:\n",
    "    print(group, ':')\n",
    "    latex_NY = create_latex_NY(df_NY, group, disp=False)\n",
    "    print(latex_NY)\n",
    "    print('\\n')\n",
    "\n",
    "    if group != 'other':\n",
    "        description.append(\"\\\\ref{tab:NY-\" + '-'.join(group.split()) + \"} for the group `\" + group + \"'\")\n",
    "    else:\n",
    "        description.append(\"\\\\ref{tab:NY-\" + '-'.join(group.split()) + '} for studies without group')\n",
    "description = ', '.join(description)\n",
    "description += '.%'\n",
    "print(description)\n",
    "\n",
    "file_name = 'NY-description'\n",
    "with open(os.path.join('tables', file_name + '.tex'), 'w') as f:\n",
    "    f.write(description)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fdbc58",
   "metadata": {},
   "source": [
    "### 2.2. Timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08069c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tikzpicture} \n",
      "\\draw[->, thick] (0,0) -- (15,0); \n",
      "\\foreach \\x/\\year in {0/2016, 2/2017, 4/2019, 6/2020, 8/2021, 10/2022, 12/2023} { \n",
      "    \\draw[shift={(\\x,0)}, color=black] (0pt,2pt) -- (0pt,-2pt); \n",
      "    \\node[below] at (\\x, 0) {\\year}; \n",
      "    } \n",
      "\\node[above right, align=left, rotate=45] at (0,0) {\\cite{li2016a}}; \n",
      "\\node[above right, align=left, rotate=45] at (2,0) {\\cite{rungsuptaweekoon2017}}; \n",
      "\\node[above right, align=left, rotate=45] at (4,0) {\\cite{wang2019}\\\\ \\cite{strubell2019}, \\cite{mcintosh2019}}; \n",
      "\\node[above right, align=left, rotate=45] at (6,0) {\\cite{rodrigues2020}, \\cite{jurj2020}\\\\ \\cite{holly2020}, \\cite{yao2021}}; \n",
      "\\node[above right, align=left, rotate=45] at (8.3,0) {\\cite{arnautovic2021}, \\cite{guo2021}\\\\ \\cite{naidu2021}, \\cite{desislavov2021}\\\\ \\cite{sun2021}, \\cite{hesse2021}, \\cite{canilang2021}}; \n",
      "\\node[above right, align=left, rotate=45] at (10,0) {\\cite{trihinas2022}\\\\ \\cite{machado2022}, \\cite{hampau2022}}; \n",
      "\\node[above right, align=left, rotate=45] at (12.3,0) {\\cite{islam2023}, \\cite{caspart2023}\\\\ \\cite{escribano2023}, \\cite{hauschild2023}\\\\ \\cite{tekin2023}, \\cite{wu2023}}; \n",
      "\\end{tikzpicture}\n"
     ]
    }
   ],
   "source": [
    "grouped = df_NY.groupby('year')['for_timeline'].apply(list)\n",
    "dict_timeline = grouped.to_dict()\n",
    "\n",
    "dict_year_point = {}\n",
    "cpt = 0\n",
    "step = 2\n",
    "for key in dict_timeline.keys():\n",
    "    dict_year_point[key] = str(cpt)\n",
    "    cpt += step\n",
    "\n",
    "a = ''\n",
    "for year, studies in dict_timeline.items():\n",
    "    if year == 2021 or year == 2023:\n",
    "        aa = ', '.join(studies[:len(studies)//3])\n",
    "        ab = ', '.join(studies[len(studies)//3:2*len(studies)//3])\n",
    "        abb = ', '.join(studies[2*len(studies)//3:])\n",
    "        ac = aa + '\\\\\\\\ ' + ab + '\\\\\\\\ ' + abb  \n",
    "    elif len(studies)>2:\n",
    "        aa = ', '.join(studies[:len(studies)//2])\n",
    "        ab = ', '.join(studies[len(studies)//2:])\n",
    "        ac = aa + '\\\\\\\\ ' + ab \n",
    "    else:\n",
    "        ac = ', '.join(studies)\n",
    "    if year == 2021:\n",
    "        ad = '\\\\node[above right, align=left, rotate=45] at (8.3,0) {' + ac + '}; \\n'\n",
    "    elif year == 2023:\n",
    "        ad = '\\\\node[above right, align=left, rotate=45] at (12.3,0) {' + ac + '}; \\n'\n",
    "    else:\n",
    "        ad = '\\\\node[above right, align=left, rotate=45] at (' + dict_year_point[year] + ',0) {' + ac + '}; \\n'\n",
    "    a += ad\n",
    "\n",
    "ba = ', '.join([f\"{v}/{k}\" for k, v in dict_year_point.items()])\n",
    "\n",
    "ba + '\\n'\n",
    "\n",
    "b = '\\\\begin{tikzpicture} \\n\\\n",
    "\\\\draw[->, thick] (0,0) -- (15,0); \\n\\\n",
    "\\\\foreach \\\\x/\\\\year in {' + ba + '} { \\n\\\n",
    "    \\\\draw[shift={(\\\\x,0)}, color=black] (0pt,2pt) -- (0pt,-2pt); \\n\\\n",
    "    \\\\node[below] at (\\\\x, 0) {\\\\year}; \\n\\\n",
    "    } \\n'\n",
    "\n",
    "c = '\\\\end{tikzpicture}'\n",
    "\n",
    "\n",
    "d = b + a + c\n",
    "\n",
    "print(d)\n",
    "\n",
    "with open(os.path.join('tables', 'NY-timeline.tex'), 'w') as f:\n",
    "    f.write(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb96438",
   "metadata": {},
   "source": [
    "## 3. YN studies\n",
    "\n",
    "### 3.1. Making the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e60c6d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "measurement :\n",
      "nb:  6\n",
      "\\paragraph{YN studies in the group ``measurement''} \n",
      " \\label{tab:YN-measurement} \n",
      "\\begin{longtable}{|>{\\raggedright\\arraybackslash}p{2.75cm}|p{11.75cm}|p{0.7cm}|}\n",
      "\\toprule\n",
      "\\bfseries study & \\bfseries detail & \\bfseries cites \\\\\n",
      "\\midrule \n",
      "\\endhead\n",
      "\\cite{hankel2016}, 2016, \\acrshort{nu} & sensors into motherboard power grid to measure power draw of components, for CPU, RAM, Network, Disk, Power Supply and System & 0 \\\\\n",
      "\\cite{enam2013}, 2013, \\acrshort{nu} & \\acrshort{epm} based on the Arduino board, for the whole system & 0 \\\\\n",
      "\\cite{ferreira2013}, 2013, SEFLab &  (Software Energy Footprint Lab) based on separate measurements of CPU, RAM, Fans, Disk and Motherboard, for execution of a software; \\textbf{available:} code \\href{https://github.com/SEFLab}{\\ref*{link-ferreira2013}} & 64 \\\\\n",
      "\\cite{piga2011}, 2011, \\acrshort{nu} & custom made board measures the computer power via current transducers, a data acquisition device, and a software that controls the framework, for the Disk, CPU and Motherboard & 8 \\\\\n",
      "\\cite{matsumoto2011}, 2011, \\acrshort{nu} & clamp meter, for GPU & 1 \\\\\n",
      "\\cite{chen2011}, 2011, \\acrshort{nu} & an analytical estimation model is proposed, but missing values for the model's weights. Work supported by observations from an \\acrshort{epm} (32A PDU gateway from Schleifenbauer), for VMs & 100 \\\\\n",
      "\\bottomrule\n",
      "\\end{longtable}\n",
      "\n",
      "\n",
      "\n",
      "False\n",
      "analytical estimation model :\n",
      "nb:  12\n",
      "\\paragraph{YN studies in the group ``analytical estimation model''} \n",
      " \\label{tab:YN-analytical-estimation-model} \n",
      "\\begin{longtable}{|>{\\raggedright\\arraybackslash}p{2.75cm}|p{11.75cm}|p{0.7cm}|}\n",
      "\\toprule\n",
      "\\bfseries study & \\bfseries detail & \\bfseries cites \\\\\n",
      "\\midrule \n",
      "\\endhead\n",
      "\\cite{liu2017}, 2017, \\acrshort{nu} & formula based on the amount of consumed energy represented by the static features of source codes and hardware specifications, for a computing task in the cloud, accounts for CPU, RAM and Disk & 13 \\\\\n",
      "\\cite{acar2016c}, 2016, TEEC &  (Tool to Estimate Energy Consumption) based on information from the Sigar library, for CPU, RAM and Disk & 2 \\\\\n",
      "\\cite{acar2016b}, 2016, TEEC & model based on hardware utilization, for CPU, RAM, Disk, Network & 12 \\\\\n",
      "\\cite{park2016}, 2016, \\acrshort{nu} & model with utilization rates as inputs, for CPU, RAM, Disk, Mainboard, CPU cooler, Case cooler, and Optical Disc Drive & 3 \\\\\n",
      "\\cite{acar2016a}, 2016, TEEC & model with hardware utilization as input, for processes, accounts for CPU, RAM and Disk & 29 \\\\\n",
      "\\cite{noureddine2015}, 2015, E-Surgeon & based on PowerAPI and Jalen, for java classes and methods, accounting for CPU and Network & 80 \\\\\n",
      "\\cite{noureddine2014a}, 2014, Jalen & based on hardware utilization and power estimation of PowerAPI, for java code on CPU and Network & 16 \\\\\n",
      "\\cite{peng2013}, 2013, \\acrshort{nu} & model based on specific \\acrshort{pmc}, for CPU, RAM, Disk, I/O Controller & 11 \\\\\n",
      "\\cite{bourdon2013}, 2013, PowerAPI & model with hardware utilization, frequency, voltage and specifications as inputs, for processes on CPU, RAM and Disk; \\textbf{available:} package, code \\href{https://github.com/powerapi-ng/powerapi}{\\ref*{link-bourdon2013}} & 94 \\\\\n",
      "\\cite{basmadjian2011}, 2011, \\acrshort{nu} & based on hardware utilization and specifications, for a server, accounting for CPU, RAM, Disk, Mainboard, Network, Fan and Power Supply & 130 \\\\\n",
      "\\cite{singh2009}, 2009, \\acrshort{nu} & model based on \\acrshort{pmc} and temperature, for CPU cores & 9 \\\\\n",
      "\\cite{spellmann2009}, 2009, \\acrshort{nu} & based on server power metrics, utilization rates and PUE, for servers & 15 \\\\\n",
      "\\bottomrule\n",
      "\\end{longtable}\n",
      "\n",
      "\n",
      "\n",
      "False\n",
      "data-based estimation model :\n",
      "nb:  28\n",
      "\\paragraph{YN studies in the group ``data-based estimation model''} \n",
      " \\label{tab:YN-data-based-estimation-model} \n",
      "\\begin{longtable}{|>{\\raggedright\\arraybackslash}p{2.75cm}|p{11.75cm}|p{0.7cm}|}\n",
      "\\toprule\n",
      "\\bfseries study & \\bfseries detail & \\bfseries cites \\\\\n",
      "\\midrule \n",
      "\\endhead\n",
      "\\cite{alavani2023}, 2023, \\acrshort{nu} & ML model (XGBoost) with code features and minimal runtime information as inputs, for CUDA program on GPU & 1 \\\\\n",
      "\\cite{pathania2023}, 2023, ESAVE &  (Estimating Server And Virtual machine Energy) ML model (XGBoost) with hardware specifications and CPU utlization as inputs, for bare-metal servers & 0 \\\\\n",
      "\\cite{wu2022}, 2022, MuMMI &  (Multiple Metrics Modeling Infrastructure) tree/rule-based, nonlinear and linear ML models with \\acrshort{pmc} as inputs, for CPU and RAM; \\textbf{available:} data are available on request from the authors & 2 \\\\\n",
      "\\cite{shim2022}, 2022, DeepPM &  (Deep Power Meter) ML model (Transformer) with compiled binary as inputs, for CPU & 1 \\\\\n",
      "\\cite{morlans2022}, 2022, \\acrshort{nu} & ML model (fully connected \\acrshort{nn}) with hardware specifications as inputs, for laptops & 0 \\\\\n",
      "\\cite{shahid2021b}, 2021, \\acrshort{nu} & linear models with \\acrshort{pmc} as inputs, for CPU and RAM or GPU & 8 \\\\\n",
      "\\cite{shahid2021a}, 2021, \\acrshort{nu} & linear models with \\acrshort{pmc} as inputs, for CPU and RAM or GPU & 12 \\\\\n",
      "\\cite{aboubakar2021}, 2021, \\acrshort{nu} & Statistical model ARIMA (Autoregressive Integrated Moving Average) based on hardware specifications and utilization, for CPU and RAM & 1 \\\\\n",
      "\\cite{lin2020}, 2020, \\acrshort{nu} & ML model (based on Elman \\acrshort{nn} and Long Short Term Memory \\acrshort{nn}) with \\acrshort{pmc} as inputs, for server & 40 \\\\\n",
      "\\cite{kloh2020}, 2020, \\acrshort{nu} & ML models (Linear Regression, Decision Tree, Support Vector Machine, \\acrshort{nn}) with \\acrshort{pmc} as inputs, for CPU, memory, cache, Jetson TX2; \\textbf{available:} trained models (no documentation) \\href{https://github.com/ViniciusPrataKloh/dissertacao-mestrado}{\\ref*{link-kloh2020}} & 4 \\\\\n",
      "\\cite{karantoumanis2020}, 2020, \\acrshort{nu} & ML models (Ordinary least squares linear regression, Lasso, Ridge, Epsilon-support vector, Decision tree, Random forest, k-nearest neighbors, Multi-layer Perceptron) with CPU utilization and RAM, Disk and Network information as input, for a scientific application running in a data center; \\textbf{available:} some models (in the article) & 1 \\\\\n",
      "\\cite{fu2018}, 2018, \\acrshort{nu} & ML model (Ridge regression) with information from the Perf tool as inputs, for CPU, RAM and Disk & 7 \\\\\n",
      "\\cite{dutta2018}, 2018, \\acrshort{nu} & ML models (ZeroR, Linear Regression, Sequential Minimal Optimization Regression, K-Nearest Neighbor, Reduced Error Pruning Tree, Bagging, Random Forest, \\acrshort{nn}) with GPU frequency, memory frequency and hardware resource utilization levels as inputs, for the GPU & 0 \\\\\n",
      "\\cite{jiang2017}, 2017, \\acrshort{nu} & ML model (Elman Neural Network) with information about the temperature, humidity and hardware utilization as inputs, for a server & 0 \\\\\n",
      "\\cite{li2016b}, 2016, \\acrshort{nu} & nonlinear relation between characteristics of a network representing the software and the power used, for CPU, RAM, Network and Disk & 1 \\\\\n",
      "\\cite{veni2016}, 2016, \\acrshort{nu} & ML model (Support Vector Regression) with \\acrshort{pmc} as inputs, for CPU, RAM, Cache and Disk of a virtual machine & 10 \\\\\n",
      "\\cite{gutierrez2015a}, 2015, \\acrshort{nu} & ML model (\\acrshort{nn}) with \\acrshort{pmc} as inputs, for the CPU & 1 \\\\\n",
      "\\cite{foo2015}, 2015, \\acrshort{nu} & ML model (Evolutionary \\acrshort{nn}) with inputs such as the number of Map and Reduce, CPU utilization and file size, for jobs in cloud data center & 29 \\\\\n",
      "\\cite{harton2015}, 2015, \\acrshort{nu} & ML model (Random Forest) with CPU utilization as input, for a whole server & 4 \\\\\n",
      "\\cite{gutierrez2015b}, 2015, \\acrshort{nu} & ML model (Feed-forward \\acrshort{nn}) with \\acrshort{pmc} as inputs, for CPU & 1 \\\\\n",
      "\\cite{colmant2014}, 2014, BitWatts & BitWatts - model with \\acrshort{pmc} as inputs, for processes, accounts for CPU; \\textbf{available:} package, code \\href{https://github.com/Spirals-Team/bitwatts}{\\ref*{link-colmant2014}} & 0 \\\\\n",
      "\\cite{storlie2014}, 2014, \\acrshort{nu} & hierarchical Bayesian modeling with hidden Markov and Dirichlet process models, for an HPC job & 18 \\\\\n",
      "\\cite{kim2014}, 2014, \\acrshort{nu} & model with operating frequency, number of active cores, number of cache accesses, and number of the last level cache misses as inputs, for CPU and RAM of a server & 26 \\\\\n",
      "\\cite{singh2013}, 2013, \\acrshort{nu} & model (Support Vector Machine) with hardware utilization as inputs, for processes on CPU, RAM, I/O and Network & 18 \\\\\n",
      "\\cite{chen2012}, 2012, ECAT &  (Energy-Consumption-Analysis-Tool) model based on task performance parameters such as CPU utilization, and hardware and software resources allocated, for data-, computation- or communication-intensive task in the cloud & 76 \\\\\n",
      "\\cite{schubert2012}, 2012, eprof & model with \\acrshort{pmc} as inputs, and stack trace used for attribution of used energy to code locations, on CPU, RAM, Disk and Network & 71 \\\\\n",
      "\\cite{zamani2010}, 2010, \\acrshort{nu} & Autoregressive moving average (ARMA) model with past and present \\acrshort{pmc} as intputs, for a server with CPUs & 13 \\\\\n",
      "\\cite{ma2009}, 2009, \\acrshort{nu} & Support Vector regression model with workload signals as inputs, for GPU & 152 \\\\\n",
      "\\bottomrule\n",
      "\\end{longtable}\n",
      "\n",
      "\n",
      "\n",
      "False\n",
      "on-chip sensors :\n",
      "nb:  4\n",
      "\\paragraph{YN studies in the group ``on-chip sensors''} \n",
      " \\label{tab:YN-on-chip-sensors} \n",
      "\\begin{longtable}{|>{\\raggedright\\arraybackslash}p{2.75cm}|p{11.75cm}|p{0.7cm}|}\n",
      "\\toprule\n",
      "\\bfseries study & \\bfseries detail & \\bfseries cites \\\\\n",
      "\\midrule \n",
      "\\endhead\n",
      "\\cite{corda2022}, 2022, PMT &  (Power Measurement Toolkit) based on \\acrshort{rapl} or LIKWID (CPU), \\acrshort{nvml} (Nvidia GPU) rocm-smi (AMD GPU), for CPU, RAM, GPU, Xilinx FPGAs, and interface to \\acrshort{epm}s; \\textbf{available:} package, code \\href{https://git.astron.nl/RD/pmt}{\\ref*{link-corda2022}} & 1 \\\\\n",
      "\\cite{montanana-aliaga2021}, 2021, Phantom & if not available: analytical estimation model -- based on computation load and hardware specifications, for application or whole system, accounting for CPU, RAM, I/O, GPU, and Network, FGPA, or Embedded Device with a power measurement kit; \\textbf{available:} code & 6 \\\\\n",
      "\\cite{becker2017}, 2017, Powerstat & based on the Power Supply Class of the Linux kernel (exposes information about the power supply to user space), for the whole computer; or \\acrshort{rapl}, for the CPU; \\textbf{available:} package, code \\href{https://github.com/ColinIanKing/powerstat}{\\ref*{link-becker2017}} & 5 \\\\\n",
      "\\cite{treibig2010}, 2010, LikwidPM &  (LIKWID-powermeter) based on \\acrshort{rapl}, for CPU and RAM; \\textbf{available:} package, code \\href{https://github.com/RRZE-HPC/likwid}{\\ref*{link-treibig2010}} & 706 \\\\\n",
      "\\bottomrule\n",
      "\\end{longtable}\n",
      "\n",
      "\n",
      "\n",
      "False\n",
      "hybrid :\n",
      "nb:  5\n",
      "\\paragraph{YN studies in a mixed group} \n",
      " \\label{tab:YN-hybrid} \n",
      "\\begin{longtable}{|>{\\raggedright\\arraybackslash}p{2.75cm}|p{11.75cm}|p{0.7cm}|}\n",
      "\\toprule\n",
      "\\bfseries study & \\bfseries detail & \\bfseries cites \\\\\n",
      "\\midrule \n",
      "\\endhead\n",
      "\\cite{noureddine2022}, 2022, PJ,JJX & data-based estimation model and on-chip sensors --  (PowerJoular, JoularJX) PJ: polynomial regression model with CPU cycles as inputs or on-chip sensors, for CPU and GPU on PCs, servers and single-board computer; JJX: based on PJ and a regression model with CPU utilization as inputs, for java methods; \\textbf{available:} package, code \\href{https://github.com/joular}{\\ref*{link-noureddine2022}} & 13 \\\\\n",
      "\\cite{naren2018}, 2018, TVAKSHAS & analytical estimation model and on-chip sensors: for the difference between actual power draw and utilized power draw -- based on Perf, \\acrshort{rapl} and measurement of power draw of CPU, for the CPU & 2 \\\\\n",
      "\\cite{alzamil2017}, 2017, \\acrshort{nu} & measurement and analytical estimation model -- \\acrshort{epm} and model with harware utilization as input, for VMs & 3 \\\\\n",
      "\\cite{suda2011}, 2011, \\acrshort{nu} & analytical and data-based estimation model -- based on measured hardware power parameters and on hardware specifications, for SIMD computing task running on CPU and GPU & 0 \\\\\n",
      "\\cite{lewis2008}, 2008, \\acrshort{nu} & analytical and data-based estimation model -- linear regression model with \\acrshort{pmc} as inputs, system-wide for servers & 178 \\\\\n",
      "\\bottomrule\n",
      "\\end{longtable}\n",
      "\n",
      "\n",
      "\n",
      "False\n",
      "other :\n",
      "nb:  2\n",
      "\\paragraph{YN studies without group} \n",
      " \\label{tab:YN-other} \n",
      "\\begin{longtable}{|>{\\raggedright\\arraybackslash}p{2.75cm}|p{11.75cm}|p{0.7cm}|}\n",
      "\\toprule\n",
      "\\bfseries study & \\bfseries detail & \\bfseries cites \\\\\n",
      "\\midrule \n",
      "\\endhead\n",
      "\\cite{noureddine2014}, 2014, JalenUnit & data-based estimation model for the energy variation of libraries based on their input parameters -- based on PowerAPI, Jalen, it is an estimation model for the energy variation of libraries based on their input parameters & 37 \\\\\n",
      "\\cite{gurumurthi2002}, 2002, SoftWatt & analytical estimation model built upon a computer architecture simulator -- system power simulator built on top of SimOS, for application and OS, on CPU, RAM and Caches & 295 \\\\\n",
      "\\bottomrule\n",
      "\\end{longtable}\n",
      "\n",
      "\n",
      "\n",
      "False\n",
      "\\ref{tab:YN-measurement} for the group `measurement', \\ref{tab:YN-analytical-estimation-model} for the group `analytical estimation model', \\ref{tab:YN-data-based-estimation-model} for the group `data-based estimation model', \\ref{tab:YN-on-chip-sensors} for the group `on-chip sensors', \\ref{tab:YN-hybrid} for studies in a mixed group, \\ref{tab:YN-other} for studies without group.%\n"
     ]
    }
   ],
   "source": [
    "_path = os.path.join('data_YN.xlsx')\n",
    "df_YN = pd.DataFrame(pd.read_excel(_path))\n",
    "\n",
    "\n",
    "# transpose the dataframe:\n",
    "df_YN = transpose_table(df_YN)\n",
    "\n",
    "df_YN = df_YN.sort_values('year', ascending=False)\n",
    "\n",
    "df_YN['name'] = df_YN['name'].replace(['none'], 'NU')\n",
    "\n",
    "df_YN['general'] = df_YN['fulltext_id'].apply(lambda x: '\\cite{' + str(x) + '}').astype(str) + ', ' + df_YN['year'].astype(str) + ', ' + df_YN['name'].astype(str)\n",
    "df_YN['for_timeline'] = df_YN['name'].astype(str) + ' ' + df_YN['fulltext_id'].apply(lambda x: '\\cite{' + str(x) + '}').astype(str)\n",
    "\n",
    "#  If want to add the note:\n",
    "df_YN.loc[df_YN['note_2'].notna(), 'detail'] = ' (' + df_YN['note_2'].astype(str) + ') ' + df_YN['detail'].astype(str)\n",
    "df_YN.loc[df_YN['note_1'].notna(), 'detail'] = df_YN['note_1'].astype(str) + ' -- ' + df_YN['detail'].astype(str)\n",
    "\n",
    "df_YN.loc[df_YN['available_link'].str.contains('http'), 'available'] = df_YN['available'].astype(str) + ' \\\\href{' + df_YN['available_link'].astype(str) + '}{\\\\ref*{link-' + df_YN['fulltext_id'].astype(str) + '}}'\n",
    "df_YN.loc[df_YN['available_link'].str.contains('http'), 'for_links_list'] = '\\\\item \\\\label{link-' + df_YN['fulltext_id'].astype(str) + '} \\\\url{' + df_YN['available_link'].astype(str) + '}'\n",
    "\n",
    "df_YN.loc[df_YN['available'].ne('none'), 'detail'] = df_YN['detail'].astype(str) + '; \\\\textbf{available:} ' + df_YN['available'].astype(str)\n",
    "\n",
    "columns_new_names = {'general': 'study', 'detail': 'detail', 'available':'available', 'citations':'cites'}\n",
    "for key in columns_new_names:\n",
    "    columns_new_names[key] = '\\\\bfseries ' + columns_new_names[key]\n",
    "\n",
    "def create_latex_YN(df_YN, category, disp, date=None, above=True):\n",
    "    list_cols = ['general', 'detail', 'citations']\n",
    "    a = '>{\\\\raggedright\\\\arraybackslash}'\n",
    "    latex_col_width = '|'+a+'p{2.75cm}|p{11.75cm}|p{0.7cm}|'\n",
    "\n",
    "    df_YN_category = df_YN[df_YN.category == category]\n",
    "\n",
    "    df_YN_category = df_YN_category[list_cols]\n",
    "\n",
    "    print('nb: ', df_YN_category.shape[0])\n",
    "\n",
    "    if disp:\n",
    "        display(df_YN_category)\n",
    "    df_YN_category = df_YN_category.rename(columns=columns_new_names)\n",
    "    latex_YN = df_YN_category.to_latex(index=False).replace('lll', latex_col_width)\n",
    "\n",
    "    if group == 'other':\n",
    "        caption='YN studies without group'\n",
    "    elif group == 'hybrid':\n",
    "        caption='YN studies in a mixed group'\n",
    "    else:\n",
    "        caption=\"YN studies in the group ``\" + category + \"''\"\n",
    "    lab='YN-'+'-'.join(category.split())\n",
    "\n",
    "    # for the version with paragraphs:\n",
    "    latex_YN = '\\\\paragraph{'+caption+ '} \\n \\\\label{tab:'+lab+'} \\n' + latex_YN\n",
    "    latex_YN = latex_YN.replace('{tabular}', '{longtable}')\n",
    "    latex_YN = latex_YN.replace('\\\\midrule', '\\\\midrule \\n\\\\endhead')\n",
    "\n",
    "    file_name = '-'.join(category.split())\n",
    "    if date:\n",
    "        if above:\n",
    "            file_name += '-' + str(date) + 'above'\n",
    "        else:\n",
    "            file_name += '-' + str(date) + 'below'\n",
    "\n",
    "    for w in abbrev_list_upper:\n",
    "        latex_YN = latex_YN.replace(w, '\\\\acrshort{'+w.lower()+'}')\n",
    "    \n",
    "    with open(os.path.join('tables', 'YN-' + file_name + '.tex'), 'w') as f:\n",
    "        f.write(latex_YN)\n",
    "\n",
    "    return(latex_YN)\n",
    "\n",
    "\n",
    "groups = ['measurement', 'analytical estimation model', 'data-based estimation model',\\\n",
    "        'on-chip sensors', 'hybrid', 'other']\n",
    "description=[]\n",
    "above=False\n",
    "for group in groups:\n",
    "    print(group, ':')\n",
    "    latex_YN = create_latex_YN(df_YN, group, disp=False)\n",
    "    print(latex_YN)\n",
    "    print('\\n')\n",
    "\n",
    "    print(above)\n",
    "\n",
    "    if group == 'other':\n",
    "        description.append(\"\\\\ref{tab:YN-\" + '-'.join(group.split()) + '} for studies without group')\n",
    "    elif group == 'hybrid':\n",
    "        description.append(\"\\\\ref{tab:YN-\" + '-'.join(group.split()) + '} for studies in a mixed group')\n",
    "    else:\n",
    "        description.append(\"\\\\ref{tab:YN-\" + '-'.join(group.split()) + \"} for the group `\" + group + \"'\")\n",
    "description = ', '.join(description)\n",
    "description += '.%'\n",
    "print(description)\n",
    "\n",
    "file_name = 'YN-description'\n",
    "with open(os.path.join('tables', file_name + '.tex'), 'w') as f:\n",
    "    f.write(description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9edf0fe",
   "metadata": {},
   "source": [
    "### 3.2. Timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d4f102c",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df_YN[df_YN['year'] <= 2015].groupby('year')['for_timeline'].apply(list)\n",
    "dict_timeline = grouped.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d73e8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2002: ['SoftWatt \\\\cite{gurumurthi2002}'], 2008: ['NU \\\\cite{lewis2008}'], 2009: ['NU \\\\cite{singh2009} \\\\cite{ma2009} \\\\cite{spellmann2009}'], 2010: ['LikwidPM \\\\cite{treibig2010}', 'NU \\\\cite{zamani2010}'], 2011: ['NU \\\\cite{suda2011} \\\\cite{basmadjian2011} \\\\cite{piga2011} \\\\cite{matsumoto2011} \\\\cite{chen2011}'], 2012: ['ECAT \\\\cite{chen2012}', 'eprof \\\\cite{schubert2012}'], 2013: ['NU \\\\cite{enam2013} \\\\cite{peng2013} \\\\cite{singh2013}', 'PowerAPI \\\\cite{bourdon2013}', 'SEFLab \\\\cite{ferreira2013}'], 2014: ['Jalen \\\\cite{noureddine2014a}', 'BitWatts \\\\cite{colmant2014}', 'NU \\\\cite{storlie2014} \\\\cite{kim2014}', 'JalenUnit \\\\cite{noureddine2014}'], 2015: ['NU \\\\cite{gutierrez2015a} \\\\cite{foo2015} \\\\cite{harton2015} \\\\cite{gutierrez2015b}', 'E-Surgeon \\\\cite{noureddine2015}']}\n"
     ]
    }
   ],
   "source": [
    "def post_process_timeline(dict_timeline):\n",
    "    d = {}\n",
    "    res = {}\n",
    "    for key,value in dict_timeline.items():\n",
    "        d[key]={}\n",
    "        \n",
    "        for v in value:\n",
    "            if v.split()[0] not in d[key]:\n",
    "                d[key][v.split()[0]] = []\n",
    "            d[key][v.split()[0]].append(v.split()[1])\n",
    "        \n",
    "            \n",
    "    for key,value in d.items():\n",
    "        res[key]=[]\n",
    "        \n",
    "        for k,v in value.items():\n",
    "\n",
    "            res[key].append(k + ' ' + ' '.join(v))\n",
    "    print(res)\n",
    "\n",
    "    return(res)\n",
    "\n",
    "dict_timeline = post_process_timeline(dict_timeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b52aa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_year_point = {}\n",
    "cpt = -1\n",
    "step = 1.75\n",
    "for key in dict_timeline.keys():\n",
    "    dict_year_point[key] = str(cpt)\n",
    "    cpt += step\n",
    "\n",
    "a = ''\n",
    "for year, studies in dict_timeline.items():\n",
    "    if len(studies)>=2:\n",
    "        aa = ', '.join(studies[:len(studies)//2])\n",
    "        ab = ', '.join(studies[len(studies)//2:])\n",
    "        ac = aa + '\\\\\\\\ ' + ab \n",
    "    else:\n",
    "        ac = ', '.join(studies)\n",
    "\n",
    "    # ac = ', '.join(studies)\n",
    "\n",
    "    ad = '\\\\node[above right, align=left, rotate=45] at (' + dict_year_point[year] + ',0) {' + ac + '}; \\n'\n",
    "    a += ad\n",
    "\n",
    "ba = ', '.join([f\"{v}/{k}\" for k, v in dict_year_point.items()])\n",
    "\n",
    "ba + '\\n'\n",
    "\n",
    "b = '\\\\begin{tikzpicture} \\n\\\n",
    "\\\\draw[dashed, thick] (-1.5,0) -- (1,0); \\n\\\n",
    "\\\\draw[thick] (1,0) -- (13.5,0); \\n\\\n",
    "\\\\draw[dashed, thick] (13.5,0) -- (14,0); \\n\\\n",
    "\\\\foreach \\\\x/\\\\year in {' + ba + '} { \\n\\\n",
    "    \\\\draw[shift={(\\\\x,0)}, color=black] (0pt,2pt) -- (0pt,-2pt); \\n\\\n",
    "    \\\\node[below] at (\\\\x, 0) {\\\\year}; \\n\\\n",
    "    } \\n'\n",
    "\n",
    "\n",
    "d = b + a\n",
    "\n",
    "d+= '\\n\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3c5a4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df_YN[df_YN['year'] > 2015].groupby('year')['for_timeline'].apply(list)\n",
    "dict_timeline = grouped.to_dict()\n",
    "dict_timeline['2019'] = []\n",
    "\n",
    "dict_timeline = {k: dict_timeline[k] for k in sorted(dict_timeline, key=int)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24c3b0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2016: ['TEEC \\\\cite{acar2016c} \\\\cite{acar2016b} \\\\cite{acar2016a}', 'NU \\\\cite{li2016b} \\\\cite{hankel2016} \\\\cite{veni2016} \\\\cite{park2016}'], 2017: ['NU \\\\cite{liu2017} \\\\cite{alzamil2017} \\\\cite{jiang2017}', 'Powerstat \\\\cite{becker2017}'], 2018: ['TVAKSHAS \\\\cite{naren2018}', 'NU \\\\cite{fu2018} \\\\cite{dutta2018}'], '2019': [], 2020: ['NU \\\\cite{lin2020} \\\\cite{kloh2020} \\\\cite{karantoumanis2020}'], 2021: ['NU \\\\cite{shahid2021b} \\\\cite{shahid2021a} \\\\cite{aboubakar2021}', 'Phantom \\\\cite{montanana-aliaga2021}'], 2022: ['PMT \\\\cite{corda2022}', 'MuMMI \\\\cite{wu2022}', 'DeepPM \\\\cite{shim2022}', 'NU \\\\cite{morlans2022}', 'PJ,JJX \\\\cite{noureddine2022}'], 2023: ['NU \\\\cite{alavani2023}', 'ESAVE \\\\cite{pathania2023}']}\n"
     ]
    }
   ],
   "source": [
    "def post_process_timeline(dict_timeline):\n",
    "    d = {}\n",
    "    res = {}\n",
    "    for key,value in dict_timeline.items():\n",
    "        d[key]={}\n",
    "        \n",
    "        for v in value:\n",
    "            if v.split()[0] not in d[key]:\n",
    "                d[key][v.split()[0]] = []\n",
    "            d[key][v.split()[0]].append(v.split()[1])\n",
    "        \n",
    "            \n",
    "    for key,value in d.items():\n",
    "        res[key]=[]\n",
    "        \n",
    "        for k,v in value.items():\n",
    "\n",
    "            res[key].append(k + ' ' + ' '.join(v))\n",
    "    print(res)\n",
    "\n",
    "    return(res)\n",
    "\n",
    "dict_timeline = post_process_timeline(dict_timeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7bc36696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tikzpicture} \n",
      "\\draw[dashed, thick] (-1.5,0) -- (1,0); \n",
      "\\draw[thick] (1,0) -- (13.5,0); \n",
      "\\draw[dashed, thick] (13.5,0) -- (14,0); \n",
      "\\foreach \\x/\\year in {-1/2002, 0.75/2008, 2.5/2009, 4.25/2010, 6.0/2011, 7.75/2012, 9.5/2013, 11.25/2014, 13.0/2015} { \n",
      "    \\draw[shift={(\\x,0)}, color=black] (0pt,2pt) -- (0pt,-2pt); \n",
      "    \\node[below] at (\\x, 0) {\\year}; \n",
      "    } \n",
      "\\node[above right, align=left, rotate=45] at (-1,0) {SoftWatt \\cite{gurumurthi2002}}; \n",
      "\\node[above right, align=left, rotate=45] at (0.75,0) {NU \\cite{lewis2008}}; \n",
      "\\node[above right, align=left, rotate=45] at (2.5,0) {NU \\cite{singh2009} \\cite{ma2009} \\cite{spellmann2009}}; \n",
      "\\node[above right, align=left, rotate=45] at (4.25,0) {LikwidPM \\cite{treibig2010}\\\\ NU \\cite{zamani2010}}; \n",
      "\\node[above right, align=left, rotate=45] at (6.0,0) {NU \\cite{suda2011} \\cite{basmadjian2011} \\cite{piga2011} \\cite{matsumoto2011} \\cite{chen2011}}; \n",
      "\\node[above right, align=left, rotate=45] at (7.75,0) {ECAT \\cite{chen2012}\\\\ eprof \\cite{schubert2012}}; \n",
      "\\node[above right, align=left, rotate=45] at (9.5,0) {NU \\cite{enam2013} \\cite{peng2013} \\cite{singh2013}\\\\ PowerAPI \\cite{bourdon2013}, SEFLab \\cite{ferreira2013}}; \n",
      "\\node[above right, align=left, rotate=45] at (11.25,0) {Jalen \\cite{noureddine2014a}, BitWatts \\cite{colmant2014}\\\\ NU \\cite{storlie2014} \\cite{kim2014}, JalenUnit \\cite{noureddine2014}}; \n",
      "\\node[above right, align=left, rotate=45] at (13.0,0) {NU \\cite{gutierrez2015a} \\cite{foo2015} \\cite{harton2015} \\cite{gutierrez2015b}\\\\ E-Surgeon \\cite{noureddine2015}}; \n",
      "\n",
      "\n",
      "\\draw[dashed, thick] (-1.5,-4.1) -- (0,-4.1); \n",
      "\\draw[->, thick] (0,-4.1) -- (14,-4.1); \n",
      "\\foreach \\x/\\year in {0/2016, 1.75/2017, 3.5/2018, 5.25/2019, 7.0/2020, 8.75/2021, 10.5/2022, 12.25/2023} { \n",
      "    \\draw[shift={(\\x,-4.1)}, color=black] (0pt,2pt) -- (0pt,-2pt); \n",
      "    \\node[below] at (\\x, -4.1) {\\year}; \n",
      "    } \n",
      "\\node[above right, align=left, rotate=45] at (0,-4.1) {TEEC \\cite{acar2016c} \\cite{acar2016b} \\cite{acar2016a}\\\\ NU \\cite{li2016b} \\cite{hankel2016} \\cite{veni2016} \\cite{park2016}}; \n",
      "\\node[above right, align=left, rotate=45] at (1.75,-4.1) {NU \\cite{liu2017} \\cite{alzamil2017} \\cite{jiang2017}\\\\ Powerstat \\cite{becker2017}}; \n",
      "\\node[above right, align=left, rotate=45] at (3.5,-4.1) {TVAKSHAS \\cite{naren2018}\\\\ NU \\cite{fu2018} \\cite{dutta2018}}; \n",
      "\\node[above right, align=left, rotate=45] at (5.25,-4.1) {}; \n",
      "\\node[above right, align=left, rotate=45] at (7.0,-4.1) {NU \\cite{lin2020} \\cite{kloh2020} \\cite{karantoumanis2020}}; \n",
      "\\node[above right, align=left, rotate=45] at (8.75,-4.1) {NU \\cite{shahid2021b} \\cite{shahid2021a} \\cite{aboubakar2021}\\\\ Phantom \\cite{montanana-aliaga2021}}; \n",
      "\\node[above right, align=left, rotate=45] at (11.15,-4.1) {PMT \\cite{corda2022}\\\\ MuMMI \\cite{wu2022}, DeepPM \\cite{shim2022}\\\\ NU \\cite{morlans2022}, PJ,JJX \\cite{noureddine2022}}; \n",
      "\\node[above right, align=left, rotate=45] at (12.25,-4.1) {NU \\cite{alavani2023}, ESAVE \\cite{pathania2023}}; \n",
      "\\end{tikzpicture}\n"
     ]
    }
   ],
   "source": [
    "dict_year_point = {}\n",
    "cpt = 0\n",
    "step = 1.75\n",
    "for key in dict_timeline.keys():\n",
    "    dict_year_point[key] = str(cpt)\n",
    "    cpt += step\n",
    "\n",
    "delta = str(-4.1)\n",
    "\n",
    "\n",
    "a = ''\n",
    "for year, studies in dict_timeline.items():\n",
    "    if len(studies)>=2 and year != 2023 and year!= 2022:\n",
    "        aa = ', '.join(studies[:len(studies)//2])\n",
    "        ab = ', '.join(studies[len(studies)//2:])\n",
    "        ac = aa + '\\\\\\\\ ' + ab \n",
    "    elif year == 2022:\n",
    "        aa = ', '.join(studies[:len(studies)//3])\n",
    "        ab = ', '.join(studies[len(studies)//3:2*len(studies)//3])\n",
    "        abb = ', '.join(studies[2*len(studies)//3:])\n",
    "        ac = aa + '\\\\\\\\ ' + ab + '\\\\\\\\ ' + abb\n",
    "    else:\n",
    "        ac = ', '.join(studies)\n",
    "\n",
    "    if year == 2022:\n",
    "        ad = '\\\\node[above right, align=left, rotate=45] at (11.15,'+delta+') {' + ac + '}; \\n'\n",
    "    else:\n",
    "        ad = '\\\\node[above right, align=left, rotate=45] at (' + dict_year_point[year] + ','+delta+') {' + ac + '}; \\n'\n",
    "    a += ad\n",
    "\n",
    "ba = ', '.join([f\"{v}/{k}\" for k, v in dict_year_point.items()])\n",
    "\n",
    "ba + '\\n'\n",
    "\n",
    "\n",
    "b = '\\\\draw[dashed, thick] (-1.5,'+delta+') -- (0,'+delta+'); \\n\\\n",
    "\\\\draw[->, thick] (0,'+delta+') -- (14,'+delta+'); \\n\\\n",
    "\\\\foreach \\\\x/\\\\year in {' + ba + '} { \\n\\\n",
    "    \\\\draw[shift={(\\\\x,'+delta+')}, color=black] (0pt,2pt) -- (0pt,-2pt); \\n\\\n",
    "    \\\\node[below] at (\\\\x, '+delta+') {\\\\year}; \\n\\\n",
    "    } \\n'\n",
    "\n",
    "c = '\\\\end{tikzpicture}'\n",
    "\n",
    "\n",
    "d += b + a + c\n",
    "\n",
    "print(d)\n",
    "\n",
    "with open(os.path.join('tables', 'YN-timeline.tex'), 'w') as f:\n",
    "    f.write(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dce470f",
   "metadata": {},
   "source": [
    "## 4. Links list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4c2fe11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{enumerate}[label={\\normalfont \\textbf{(L\\arabic*)}}] \n",
      " \\item \\label{link-getzner2023} \\url{https://github.com/JohannesGetzner/dl-energy-estimator}\n",
      " \\item \\label{link-you2023} \\url{https://github.com/SymbioticLab/Zeus }\n",
      " \\item \\label{link-budennyy2022} \\url{https://github.com/sb-ai-lab/Eco2AI }\n",
      " \\item \\label{link-carastan-santos2022} \\url{https://github.com/phamthi1812/Benchmark-Tracker}\n",
      " \\item \\label{link-igescu2021} \\url{https://github.com/epfl-iglobalhealth/CS433-2021-ecoML}\n",
      " \\item \\label{link-montgomerie-corcoran2021} \\url{https://github.com/AlexMontgomerie/pommel}\n",
      " \\item \\label{link-lannelongue2021} \\url{https://github.com/GreenAlgorithms/green-algorithms-tool}\n",
      " \\item \\label{link-wang2021} \\url{https://github.com/Accelergy-Project/accelergy}\n",
      " \\item \\label{link-trebaol2020} \\url{https://github.com/epfl-iglobalhealth/cumulator}\n",
      " \\item \\label{link-anthony2020} \\url{https://github.com/lfwa/carbontracker }\n",
      " \\item \\label{link-henderson2020} \\url{https://github.com/Breakend/experiment-impact-tracker}\n",
      " \\item \\label{link-lottick2019} \\url{https://github.com/responsibleproblemsolving/energy-usage}\n",
      " \\item \\label{link-lacoste2019} \\url{https://github.com/mlco2/impact}\n",
      " \\item \\label{link-rodrigues2018} \\url{https://github.com/Crefeda/SyNERGY }\n",
      " \\item \\label{link-cai2017} \\url{https://github.com/enyac-group/NeuralPower }\n",
      " \\item \\label{link-yang2017} \\url{https://energyestimation.mit.edu/}\n",
      " \\item \\label{link-lottick2019-bis} \\url{https://github.com/mlco2/codecarbon}\n",
      " \\item \\label{link-corda2022} \\url{https://git.astron.nl/RD/pmt}\n",
      " \\item \\label{link-noureddine2022} \\url{https://github.com/joular}\n",
      " \\item \\label{link-kloh2020} \\url{https://github.com/ViniciusPrataKloh/dissertacao-mestrado}\n",
      " \\item \\label{link-becker2017} \\url{https://github.com/ColinIanKing/powerstat}\n",
      " \\item \\label{link-colmant2014} \\url{https://github.com/Spirals-Team/bitwatts}\n",
      " \\item \\label{link-bourdon2013} \\url{https://github.com/powerapi-ng/powerapi}\n",
      " \\item \\label{link-ferreira2013} \\url{https://github.com/SEFLab}\n",
      " \\item \\label{link-treibig2010} \\url{https://github.com/RRZE-HPC/likwid}\n",
      "\\end{enumerate}\n"
     ]
    }
   ],
   "source": [
    "links_list_latex = '\\n '.join(df_YY.loc[df_YY['available_link'].str.contains('http'), 'for_links_list'].values) ## links from YY\n",
    "links_list_latex += '\\n '\n",
    "links_list_latex += '\\n '.join(df_YY.loc[df_YY['available_link_bis'].str.contains('http'), 'for_links_list_bis'].values) ## links from YY\n",
    "links_list_latex += '\\n '\n",
    "links_list_latex += '\\n '.join(df_YN.loc[df_YN['available_link'].str.contains('http'), 'for_links_list'].values) ## links from YN\n",
    "\n",
    "# Final itemize for latex\n",
    "links_list_latex = '\\\\begin{enumerate}[label={\\\\normalfont \\\\textbf{(L\\\\arabic*)}}] \\n ' + links_list_latex + '\\n\\\\end{enumerate}'\n",
    "print(links_list_latex)\n",
    "\n",
    "with open(os.path.join('tables', 'linklist.tex'), 'w') as f:\n",
    "    f.write(links_list_latex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c290108",
   "metadata": {},
   "source": [
    "## 5. Subsidiary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4a03ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb:  5\n",
      "nb:  7\n",
      "nb:  26\n",
      "\\begin{tabular}{|>{\\raggedright\\arraybackslash}p{3.2cm}|p{6.5cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|l}\n",
      "\\toprule\n",
      "name & detail & code & doc & blog \\\\\n",
      "\\midrule\n",
      "Kepler (Kubernetes Efficient Power Level Exporter) & for Kubernetes systems, at process, container, or Kubernetes pod level, on Intel CPU, RAM and Nvidia GPU, or whole system, or whole network of systems &  \\href{https://github.com/sustainable-computing-io/kepler}{\\ref*{link-c-Kepler}} &  &  \\href{https://sustainable-computing.io/}{\\ref*{link-b-Kepler}} \\\\\n",
      "Tracarbon & for Intel CPU and RAM &  \\href{https://github.com/fvaleye/tracarbon}{\\ref*{link-c-Tracarbon}} &  \\href{https://fvaleye.github.io/tracarbon/documentation/}{\\ref*{link-d-Tracarbon}} &  \\href{https://medium.com/@florian.valeye/tracarbon-track-your-devices-carbon-footprint-fb051fcc9009}{\\ref*{link-b-Tracarbon}} \\\\\n",
      "PyJoules & for Intel CPU, RAM, and Nvidia GPU &  \\href{https://github.com/powerapi-ng/pyJoules}{\\ref*{link-c-PyJoules}} &  \\href{https://pyjoules.readthedocs.io/en/latest/}{\\ref*{link-d-PyJoules}} &  \\\\\n",
      "Powerstat & Intel CPU, RAM, or laptop on battery &  \\href{https://github.com/ColinIanKing/powerstat}{\\ref*{link-c-Powerstat}} &  \\href{https://manpages.ubuntu.com/manpages/bionic/man8/powerstat.8.html}{\\ref*{link-d-Powerstat}} &  \\\\\n",
      "PowerTOP & notably process- and system-level &  \\href{https://github.com/fenrus75/powertop}{\\ref*{link-c-PowerTOP}} &  \\href{https://manpages.ubuntu.com/manpages/mantic/en/man8/powertop.8.html}{\\ref*{link-d-PowerTOP}} &  \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{enumerate}[label={\\normalfont \\textbf{(L\\arabic*)}}] \n",
      "\\setcounter{enumi}{25} \n",
      " \\item \\label{link-c-Kepler} \\url{https://github.com/sustainable-computing-io/kepler}\n",
      " \\item \\label{link-c-Tracarbon} \\url{https://github.com/fvaleye/tracarbon}\n",
      " \\item \\label{link-c-PyJoules} \\url{https://github.com/powerapi-ng/pyJoules}\n",
      " \\item \\label{link-c-Powerstat} \\url{https://github.com/ColinIanKing/powerstat}\n",
      " \\item \\label{link-c-PowerTOP} \\url{https://github.com/fenrus75/powertop}\n",
      " \\item \\label{link-d-Tracarbon} \\url{https://fvaleye.github.io/tracarbon/documentation/}\n",
      " \\item \\label{link-d-PyJoules} \\url{https://pyjoules.readthedocs.io/en/latest/}\n",
      " \\item \\label{link-d-Powerstat} \\url{https://manpages.ubuntu.com/manpages/bionic/man8/powerstat.8.html}\n",
      " \\item \\label{link-d-PowerTOP} \\url{https://manpages.ubuntu.com/manpages/mantic/en/man8/powertop.8.html}\n",
      " \\item \\label{link-b-Kepler} \\url{https://sustainable-computing.io/}\n",
      " \\item \\label{link-b-Tracarbon} \\url{https://medium.com/@florian.valeye/tracarbon-track-your-devices-carbon-footprint-fb051fcc9009}\n",
      "\\end{enumerate}\n",
      "\n",
      "\n",
      "\\begin{tabular}{|>{\\raggedright\\arraybackslash}p{3.25cm}|p{1.25cm}|p{6.7cm}|p{1.5cm}|p{1.5cm}|}\n",
      "\\toprule\n",
      "name & location & detail & code & doc \\\\\n",
      "\\midrule\n",
      "Energy-Scopium & \\cite{jay2023} & for CPU, RAM and GPU &  &  \\href{https://www.denergium.fr/pages/the-energyscopium-software-suite.html}{\\ref*{link-d-Energy-Scopium}} \\\\\n",
      "Perf & \\cite{jay2023} & Performance analysis tool. Notably provides CPU performance counters, and energy consumption (RAPL based) &  \\href{https://www.man7.org/linux/man-pages/man1/perf.1.html}{\\ref*{link-c-Perf}} &  \\\\\n",
      "Scaphandre & \\cite{jay2023} & process-level, for Intel CPU, RAM &  \\href{https://github.com/hubblo-org/scaphandre}{\\ref*{link-c-Scaphandre}} &  \\href{https://hubblo-org.github.io/scaphandre-documentation/}{\\ref*{link-d-Scaphandre}} \\\\\n",
      "Silicon-Labs & \\cite{rieger2017} & for methods calls in embedded software &  &  \\\\\n",
      "NVML & \\cite{fahad2019} & for Nvidia GPU &  &  \\href{https://developer.nvidia.com/nvidia-system-management-interface}{\\ref*{link-d-NVML}} \\\\\n",
      "Intel-SMC & \\cite{fahad2019} & (Intel System Management Controller) for Intel Xeon Phi co-processors &  &  \\href{https://www.intel.com/content/dam/develop/external/us/en/documents/xeon-phi-coprocessor-system-software-developers-guide.pdf}{\\ref*{link-d-Intel-SMC}} \\\\\n",
      "Intel-Power-Gadget & \\cite{garcia-martin2019a} & based on RAPL, for Intel CPU and RAM &  &  \\href{https://www.intel.com/content/www/us/en/developer/articles/tool/power-gadget.html}{\\ref*{link-d-Intel-Power-Gadget}} \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{enumerate}[label={\\normalfont \\textbf{(L\\arabic*)}}] \n",
      "\\setcounter{enumi}{36} \n",
      " \\item \\label{link-c-Perf} \\url{https://www.man7.org/linux/man-pages/man1/perf.1.html}\n",
      " \\item \\label{link-c-Scaphandre} \\url{https://github.com/hubblo-org/scaphandre}\n",
      " \\item \\label{link-d-Energy-Scopium} \\url{https://www.denergium.fr/pages/the-energyscopium-software-suite.html}\n",
      " \\item \\label{link-d-Scaphandre} \\url{https://hubblo-org.github.io/scaphandre-documentation/}\n",
      " \\item \\label{link-d-NVML} \\url{https://developer.nvidia.com/nvidia-system-management-interface}\n",
      " \\item \\label{link-d-Intel-SMC} \\url{https://www.intel.com/content/dam/develop/external/us/en/documents/xeon-phi-coprocessor-system-software-developers-guide.pdf}\n",
      " \\item \\label{link-d-Intel-Power-Gadget} \\url{https://www.intel.com/content/www/us/en/developer/articles/tool/power-gadget.html}\n",
      "\\end{enumerate}\n",
      "\n",
      "\n",
      "\\begin{tabular}{|>{\\raggedright\\arraybackslash}p{5cm}|p{1.6cm}|p{8.55cm}|}\n",
      "\\toprule\n",
      "general & location & detail \\\\\n",
      "\\midrule\n",
      "\\cite{marantos2022}, 2022, Energy-Toolbox & \\cite{pijnacker2023} & for application or class, on CPU and GPU \\\\\n",
      "\\cite{marantos2021}, 2021, SKD4ED & \\cite{pijnacker2023} & for application \\\\\n",
      "\\cite{mancebo2018}, 2018, FEETINGS/EET & \\cite{pijnacker2023} & for hardware components of a computer \\\\\n",
      "\\cite{walker2017}, 2017, Powmon & \\cite{garcia-martin2019a} & for mobile CPU \\\\\n",
      "\\cite{rodrigues2017}, 2017, ARM-Streamline & \\cite{garcia-martin2019a} & for ARM mobile CPU \\\\\n",
      "\\cite{nucci2017}, 2017, PETrA & \\cite{ergasheva2020} & for method, for mobile applications \\\\\n",
      "\\cite{pereira2017}, 2017, SPELL & \\cite{pijnacker2023} & for method or program on computer \\\\\n",
      "\\cite{rouhani2016}, 2016, DeLight & \\cite{garcia-martin2019a} & for training of feed-forward NN \\\\\n",
      "\\cite{chowdhury2016}, 2016, GreenOracle & \\cite{pijnacker2023} & for application on Android \\\\\n",
      "\\cite{westfield2016}, 2016, Orka & \\cite{pijnacker2023} & for method on Android \\\\\n",
      "\\cite{couto2015}, 2015, Greendroid & \\cite{rieger2017} & for Android programs during unit tests \\\\\n",
      "\\cite{aggarwal2015}, 2015, Green-Advisor & \\cite{ergasheva2020} & for changes of energy profile of an application \\\\\n",
      "\\cite{cordero2015}, 2015, GreenSoM & \\cite{pijnacker2023} & for Java-class \\\\\n",
      "\\cite{godboley2015}, 2015, Green-JEXT & \\cite{pijnacker2023} & for application \\\\\n",
      "\\cite{wilke2013}, 2013, JouleUnit & \\cite{rieger2017} & for unit testing of application on any platform \\\\\n",
      "\\cite{weaver2012}, 2012, PAPI & \\cite{garcia-martin2019a} & based on RAPL, for Intel CPU and RAM \\\\\n",
      "\\cite{hao2012}, 2012, eCalc & \\cite{pijnacker2023} & for method or program on android \\\\\n",
      "\\cite{ge2012}, 2012, eTune & \\cite{pijnacker2023} & for application or class, on CPU in data center/cloud \\\\\n",
      "\\cite{david2010, rotem2012, hackenberg2015}, 2012, RAPL & \\cite{fahad2019} & for Intel CPU and RAM \\\\\n",
      "\\cite{hindle2012}, 2012, Green-Mining & \\cite{rieger2017} & for software with different versions, on whole system \\\\\n",
      "\\cite{chung2011}, 2011, ANEPROF & \\cite{ergasheva2020} & for Java application on Android \\\\\n",
      "\\cite{zhang2010}, 2010, PowerTutor & \\cite{rieger2017} & for android smartphones \\\\\n",
      "\\cite{do2009}, 2009, pTop & \\cite{noureddine2013} & process-level model based on hardware specifications (TDP) and utilization, for CPU, RAM, Network and Disk \\\\\n",
      "\\cite{wonil2009}, 2009, ePRO-MP & \\cite{ergasheva2020} & for multi-threaded application \\\\\n",
      "\\cite{li2009}, 2009, McPAT & \\cite{garcia-martin2019a} & for C application \\\\\n",
      "\\cite{flinn1999}, 1999, PowerScope & \\cite{noureddine2013, pijnacker2023} & process-level or method level \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_path = os.path.join('subsidiary.xlsx')\n",
    "df_s = pd.DataFrame(pd.read_excel(_path))\n",
    "df_s\n",
    "\n",
    "df_1 = df_s[df_s['in_secondary']=='no']\n",
    "df_1.loc[df_1['link'].notna(), 'code'] = ' \\\\href{' + df_1['link'].astype(str) + '}{\\\\ref*{link-c-' + df_1['name'].astype(str) + '}}'\n",
    "df_1.loc[df_1['link_2'].notna(), 'doc'] = ' \\\\href{' + df_1['link_2'].astype(str) + '}{\\\\ref*{link-d-' + df_1['name'].astype(str) + '}}'\n",
    "df_1.loc[df_1['link_3'].notna(), 'blog'] = ' \\\\href{' + df_1['link_3'].astype(str) + '}{\\\\ref*{link-b-' + df_1['name'].astype(str) + '}}'\n",
    "df_1.loc[df_1['link'].notna(), 'for_link_list_1'] = '\\\\item \\\\label{link-c-' + df_1['name'].astype(str) + '} \\\\url{' + df_1['link'].astype(str) + '}'\n",
    "df_1.loc[df_1['link_2'].notna(), 'for_link_list_2'] = '\\\\item \\\\label{link-d-' + df_1['name'].astype(str) + '} \\\\url{' + df_1['link_2'].astype(str) + '}'\n",
    "df_1.loc[df_1['link_3'].notna(), 'for_link_list_3'] = '\\\\item \\\\label{link-b-' + df_1['name'].astype(str) + '} \\\\url{' + df_1['link_3'].astype(str) + '}'\n",
    "# df_1.loc[df_1['full_name'].notna(), 'detail'] = ' (' + df_1['full_name'].astype(str) + ') ' + df_1['detail'].astype(str)\n",
    "df_1.loc[df_1['full_name'].notna(), 'name'] =  df_1['name'].astype(str) + ' (' + df_1['full_name'].astype(str) + ')'\n",
    "\n",
    "\n",
    "df_2 = df_s[(df_s['in_secondary']=='yes') & (df_s['tool_alone']=='yes')]\n",
    "df_2.loc[df_2['link'].notna(), 'code'] = ' \\\\href{' + df_2['link'].astype(str) + '}{\\\\ref*{link-c-' + df_2['name'].astype(str) + '}}'\n",
    "df_2.loc[df_2['link_2'].notna(), 'doc'] = ' \\\\href{' + df_2['link_2'].astype(str) + '}{\\\\ref*{link-d-' + df_2['name'].astype(str) + '}}'\n",
    "df_2.loc[df_2['link'].notna(), 'for_link_list_1'] = '\\\\item \\\\label{link-c-' + df_2['name'].astype(str) + '} \\\\url{' + df_2['link'].astype(str) + '}'\n",
    "df_2.loc[df_2['link_2'].notna(), 'for_link_list_2'] = '\\\\item \\\\label{link-d-' + df_2['name'].astype(str) + '} \\\\url{' + df_2['link_2'].astype(str) + '}'\n",
    "df_2.loc[:, 'location'] = df_2['location'].apply(lambda x: '\\cite{' + str(x) + '}').astype(str)\n",
    "df_2.loc[df_2['full_name'].notna(), 'detail'] =  '(' + df_2['full_name'].astype(str) + ') ' + df_2['detail'].astype(str)\n",
    "\n",
    "df_3 = df_s[(df_s['in_secondary']=='yes') & (df_s['tool_alone']=='no')]\n",
    "df_3 = df_3.sort_values('year', ascending=False)\n",
    "df_3['year'] = df_3['year'].astype(int)\n",
    "df_3['general'] = df_3['fulltext_id'].apply(lambda x: '\\cite{' + str(x) + '}').astype(str) + ', ' + df_3['year'].astype(str) + ', ' + df_3['name'].astype(str)\n",
    "df_3['location'] = df_3['location'].apply(lambda x: '\\cite{' + str(x) + '}').astype(str)\n",
    "# df_3.loc[df_3['full_name'].notna(), 'detail'] = ' (' + df_3['full_name'].astype(str) + ') ' + df_3['detail'].astype(str)\n",
    "df_3.loc[df_3['full_name'].notna(), 'name'] =  df_3['name'].astype(str) + ' (' + df_3['full_name'].astype(str) + ')'\n",
    "\n",
    "def create_latex_1(df):\n",
    "\n",
    "    links_list_latex = '\\n '.join(df.loc[df['link'].notna(), 'for_link_list_1'].values)\n",
    "    links_list_latex += '\\n '\n",
    "    links_list_latex += '\\n '.join(df.loc[df['link_2'].notna(), 'for_link_list_2'].values)\n",
    "    links_list_latex += '\\n '\n",
    "    links_list_latex += '\\n '.join(df.loc[df['link_3'].notna(), 'for_link_list_3'].values)\n",
    "    links_list_latex = '\\\\begin{enumerate}[label={\\\\normalfont \\\\textbf{(L\\\\arabic*)}}] \\n\\\\setcounter{enumi}{25} \\n ' + links_list_latex + '\\n\\\\end{enumerate}'\n",
    "    with open(os.path.join('tables', 'subsidiary_linklist_df1.tex'), 'w') as f:\n",
    "        f.write(links_list_latex)\n",
    "\n",
    "    list_cols = ['name', 'detail', 'code', 'doc', 'blog']\n",
    "    a = '>{\\\\raggedright\\\\arraybackslash}'\n",
    "    latex_col_width = '|'+a+'p{3.2cm}|p{6.5cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|'\n",
    "    df = df[list_cols]\n",
    "    print('nb: ', df.shape[0])\n",
    "    df = df.fillna('')\n",
    "    latex = df.to_latex(index=False).replace('llll', latex_col_width)\n",
    "    with open(os.path.join('tables', 'subsidiary_1.tex'), 'w') as f:\n",
    "        f.write(latex)\n",
    "\n",
    "    return(latex + '\\n' + links_list_latex)\n",
    "\n",
    "\n",
    "def create_latex_2(df):\n",
    "\n",
    "    links_list_latex = '\\n '.join(df.loc[df['link'].notna(), 'for_link_list_1'].values)\n",
    "    links_list_latex += '\\n '\n",
    "    links_list_latex += '\\n '.join(df.loc[df['link_2'].notna(), 'for_link_list_2'].values)\n",
    "    links_list_latex = '\\\\begin{enumerate}[label={\\\\normalfont \\\\textbf{(L\\\\arabic*)}}] \\n\\\\setcounter{enumi}{36} \\n ' + links_list_latex + '\\n\\\\end{enumerate}'\n",
    "    with open(os.path.join('tables', 'subsidiary_linklist_df2.tex'), 'w') as f:\n",
    "        f.write(links_list_latex)\n",
    "\n",
    "    list_cols = ['name', 'location', 'detail', 'code', 'doc']\n",
    "    a = '>{\\\\raggedright\\\\arraybackslash}'\n",
    "    latex_col_width = '|'+a+'p{3.25cm}|p{1.25cm}|p{6.7cm}|p{1.5cm}|p{1.5cm}|'\n",
    "    df = df[list_cols]\n",
    "    print('nb: ', df.shape[0])\n",
    "    df = df.fillna('')\n",
    "    latex = df.to_latex(index=False).replace('lllll', latex_col_width)\n",
    "    with open(os.path.join('tables', 'subsidiary_2.tex'), 'w') as f:\n",
    "        f.write(latex)\n",
    "\n",
    "    return(latex + '\\n' + links_list_latex)\n",
    "\n",
    "def create_latex_3(df):\n",
    "    list_cols = ['general', 'location', 'detail']\n",
    "    a = '>{\\\\raggedright\\\\arraybackslash}'\n",
    "    latex_col_width = '|'+a+'p{5cm}|p{1.6cm}|p{8.55cm}|'\n",
    "\n",
    "    df = df[list_cols]\n",
    "\n",
    "    print('nb: ', df.shape[0])\n",
    "\n",
    "    df = df[list_cols]\n",
    "\n",
    "    latex = df.to_latex(index=False).replace('lll', latex_col_width)\n",
    "\n",
    "    with open(os.path.join('tables', 'subsidiary_3.tex'), 'w') as f:\n",
    "        f.write(latex)\n",
    "    return(latex)\n",
    "\n",
    "l1 = create_latex_1(df_1)\n",
    "l2 = create_latex_2(df_2)\n",
    "l3 = create_latex_3(df_3)\n",
    "print(l1)\n",
    "print('\\n')\n",
    "print(l2)\n",
    "print('\\n')\n",
    "print(l3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_SLR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
